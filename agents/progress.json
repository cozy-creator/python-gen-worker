{
  "issues": [
    {
      "id": 50,
      "name": "Untrusted tenant code: per-run file capability tokens (no static FILE_SERVICE_TOKEN in pods)",
      "description": "Assume tenant functions can read any env var/file in the container. Remove reliance on long-lived file service tokens inside worker pods. Instead, accept a short-lived, run-scoped file capability JWT from gen-orchestrator for each task/session and use it for all cozy-hub file store reads/writes (inputs/outputs).",
      "tasks": [
        "[ ] Extend worker task envelope to include `file_token` (JWT) + `file_base_url` (cozy-hub file API base) and thread it into ActionContext / CozyHubFileClient.",
        "[ ] Update CozyHubDownloader/ctx.save_* helpers to use the per-run `file_token` (Authorization: Bearer ...) and avoid using env FILE_API_TOKEN for production flows.",
        "[ ] Ensure the token is tenant-scoped and prefix-scoped (runs/<run_id>/...) and expires quickly; treat auth failures as non-retriable unless token is refreshable.",
        "[ ] Add tests: task-scoped file token is required when running in \"untrusted\" mode; uploads/downloads succeed for allowed prefixes and fail for disallowed paths."
      ],
      "completed": false
    },
    {
      "id": 51,
      "name": "LRU model cache with live VRAM reporting to gen-orchestrator",
      "description": "Implement an LRU model cache in python-worker that tracks loaded models in VRAM and reports availability to gen-orchestrator in real-time via heartbeats. This enables model-aware routing where the orchestrator can route requests to workers that already have the required model loaded.\n\nImplementation: Created src/gen_worker/model_cache.py with ModelCache class using OrderedDict for LRU tracking. Tracks models in VRAM, disk, and downloading states. Provides get_stats() for heartbeat reporting. Integrated with Worker class for load/unload commands.",
      "tasks": [
        "[x] Implement ModelCache class with LRU eviction (src/gen_worker/model_cache.py)\n    - Uses OrderedDict for LRU ordering\n    - Tracks ModelLocation: VRAM, DISK, DOWNLOADING\n    - Thread-safe with RLock",
        "[x] Implement VRAM tracking with weights vs working memory\n    - Auto-detects total VRAM via torch.cuda.get_device_properties()\n    - Configurable safety margin (WORKER_VRAM_SAFETY_MARGIN_GB, default 3.5GB)\n    - Tracks _vram_used_gb for all VRAM-loaded models",
        "[x] Implement get_stats() for heartbeat reporting\n    - Returns ModelCacheStats with vram_models, disk_models, downloading_models\n    - Includes vram_used_gb, vram_total_gb, vram_available_gb\n    - to_dict() method for protobuf/JSON serialization",
        "[x] Update worker heartbeat to include model cache stats\n    - Worker._register_worker() uses _model_cache.get_vram_models() for available_models\n    - Falls back to model_manager if available",
        "[x] Integrate ModelCache with model injection\n    - Worker initializes _model_cache in __init__\n    - mark_loaded_to_vram() called after successful load",
        "[x] Support orchestrator-commanded model operations\n    - _handle_load_model_cmd() updates model cache on success\n    - _handle_unload_model_cmd() implemented - calls model_manager.unload() and cache.unload_model()",
        "[x] Add configurable settings via environment variables\n    - WORKER_MAX_VRAM_GB: Maximum VRAM to use\n    - WORKER_VRAM_SAFETY_MARGIN_GB: Reserved for working memory (default 3.5)\n    - WORKER_MODEL_CACHE_DIR: Disk cache directory",
        "[x] Add tests for LRU eviction, safety margins, heartbeat stats\n    - tests/test_model_cache.py with 15 tests\n    - Covers LRU ordering, eviction, stats, env config"
      ],
      "completed": true
    },
    {
      "id": 52,
      "name": "Diffusers pipeline construction parity with torch_manager",
      "description": "Implement proper diffusers pipeline construction in the new python-worker to achieve feature parity with the legacy torch_manager. The worker must be able to load models from Cozy Hub manifests, construct pipelines with component sharing, apply optimizations, and handle various model formats.\n\nImplemented in src/gen_worker/pipeline_loader.py with:\n- PipelineLoader class with load(), unload(), get() methods\n- LoadedPipeline dataclass for metadata tracking\n- PipelineConfig dataclass for configuration\n- LocalModelCache class for NFS->NVMe optimization with LRU eviction\n- Custom exception classes for error handling (ModelNotFoundError, CudaOutOfMemoryError, etc.)\n- Model downloading from Cozy Hub with concurrent download limits\n- Startup initialization with randomized download order",
      "tasks": [
        "[x] Pipeline loading with format priority (flashpack first, then safetensors, then single file)\n    - _detect_load_format() checks FlashPack, safetensors, then single-file\n    - _load_from_flashpack(), _load_from_pretrained(), _load_from_single_file()",
        "[x] Component reference resolution (_cozy_ref in model_index.json)\n    - resolve_cozy_refs() parses model_index.json and resolves _cozy_ref entries",
        "[x] dtype selection (bfloat16 for Flux, float16 for others)\n    - get_torch_dtype() auto-selects based on model name (flux/sd3 -> bfloat16)",
        "[x] Device placement with VAE tiling/slicing ALWAYS enabled during _move_to_gpu\n    - _apply_vae_optimizations() enables tiling and slicing on VAE",
        "[x] Scheduler configuration (dynamic import from diffusers)\n    - get_scheduler_class() dynamically imports scheduler from diffusers\n    - _configure_scheduler() applies scheduler to pipeline",
        "[x] Pipeline optimizations (CONDITIONAL - only when model_size > max_vram)\n    - _apply_memory_optimizations() checks available VRAM before applying offload",
        "[x] Warm-up inference (4 steps, output_type='pil', flushes memory after)\n    - _warmup_pipeline() runs 4-step inference with appropriate params per pipeline type",
        "[x] Custom pipeline classes support\n    - get_pipeline_class() handles string, tuple, or auto-detect from model_index.json",
        "[x] Memory management (flush_memory, unload with remove_all_hooks and explicit component deletion)\n    - flush_memory() runs gc.collect() and torch.cuda.empty_cache()\n    - unload() removes hooks and deletes components explicitly",
        "[x] Model download from Cozy Hub\n    - ensure_model_available() checks local, then downloads from Cozy Hub\n    - _download_from_cozy_hub() fetches manifest and downloads files\n    - download_models() downloads multiple models with concurrency limit",
        "[x] Startup model initialization with randomization\n    - initialize_startup_models() downloads with randomized order\n    - Optionally preloads first model into VRAM",
        "[x] Error handling (missing files, incompatible formats, CUDA OOM)\n    - Custom exceptions: ModelNotFoundError, ModelDownloadError, IncompatibleFormatError\n    - CudaOutOfMemoryError, ComponentMissingError, PipelineLoaderError\n    - load() wraps operations with try/except for proper error handling",
        "[x] LocalModelCache for NFS->NVMe optimization with background prefetch\n    - LocalModelCache class with LRU eviction\n    - cache_model() copies with FlashPack priority\n    - start_prefetch() for background prefetching"
      ],
      "completed": true
    },
    {
      "id": 38,
      "name": "Simplify GPU selection with gpu=true flag",
      "description": "Replace the cuda constraint as the primary GPU selection mechanism with a simpler gpu=true/false flag. The cuda constraint becomes an optional advanced override. This aligns with how other platforms (Modal, Replicate, Fal) handle GPU selection.",
      "tasks": [
        "[x] Update gen-builder config.go to parse new gpu field:\n    - gpu = true -> build GPU variant (pick latest stable CUDA)\n    - gpu = false or omitted -> build CPU variant\n    - cuda = \">=12.6\" -> optional override for specific CUDA version",
        "[x] Update FilterVariants in runtime_catalog.go:\n    - Check gpu flag first\n    - If gpu=true and no cuda constraint, use default CUDA version\n    - If gpu=false, return CPU variants only",
        "[x] Update gen-builder README with new gpu flag documentation",
        "[x] Update python-worker examples to use gpu=true instead of cuda constraint",
        "[x] Add validation: warn if cuda is set but gpu is false (conflicting config)"
      ],
      "completed": true
    },
    {
      "id": 39,
      "name": "Add resource requirements to deployment config",
      "description": "Allow tenants to specify hardware requirements (VRAM, GPU type, memory, CPU) in pyproject.toml. These are used by the orchestrator/scheduler to match deployments to appropriate workers.\n\nArchitecture: python-worker parses [tool.cozy.resources] and includes it in the manifest. gen-builder extracts the manifest from the built image and forwards resources to orchestrator.\n\nNote: Orchestrator-side tasks (reading resources, scheduler filtering) moved to gen-orchestrator issue #216.",
      "tasks": [
        "[x] Define [tool.cozy.resources] schema:\n    - vram_gb: int (minimum GPU VRAM required)\n    - gpu_type: string (\"any\", \"A100\", \"H100\", \"A10G\", \"T4\", etc.)\n    - memory_gb: int (system RAM)\n    - cpu_cores: int",
        "[x] python-worker discover.py parses [tool.cozy.resources] into manifest",
        "[x] gen-builder extracts resources from manifest and includes in BuildManifest",
        "[x] Document resource requirements in gen-builder README",
        "[x] Add examples showing resource configuration"
      ],
      "completed": true
    },
    {
      "id": 40,
      "name": "Simplify model specification in deployment config",
      "description": "Models that a worker needs should be declared in [tool.cozy.models] in pyproject.toml. Model IDs are Cozy Hub identifiers (not raw HuggingFace/Civitai URLs). Workers download models via Cozy Hub API, which handles the actual source (HuggingFace, S3, CDN, etc.). Orchestrator routes using deployment-local keys only. Dynamic/small files like LoRAs come as Asset in request payloads.\n\nArchitecture: python-worker parses [tool.cozy.models] and includes it in the manifest. gen-builder extracts the manifest from the built image and forwards models to orchestrator.",
      "tasks": [
        "[x] Define [tool.cozy.models] schema in pyproject.toml:\n    ```toml\n    [tool.cozy.models]\n    sd-xl = \"stabilityai/stable-diffusion-xl-base-1.0\"  # Cozy Hub model ID\n    controlnet = \"lllyasviel/control_v11p_sd15_canny\"   # Cozy Hub model ID\n    ```\n    - Keys (sd-xl, controlnet) are deployment-local, used for routing\n    - Values are Cozy Hub model IDs (unique identifiers in Cozy's model registry)\n    - Cozy Hub resolves IDs to actual storage location (HuggingFace, S3, CDN, etc.)",
        "[x] python-worker discover.py parses [tool.cozy.models] into manifest",
        "[x] gen-builder extracts models from manifest and includes in BuildManifest",
        "[x] Update worker startup to pre-download all models via Cozy Hub API\n    - Worker now accepts `manifest` parameter with models from [tool.cozy.models]\n    - Initializes `_deployment_model_id_by_key` from manifest.models at startup\n    - Triggers background model pre-download if model_manager is available",
        "[x] Update ModelRef to resolve keys from [tool.cozy.models]\n    - `_deployment_model_id_by_key` is populated from manifest at startup\n    - ModelRef(Src.DEPLOYMENT, \"key\") resolves using this mapping",
        "[x] Removed model_allowlist (replaced by [tool.cozy.models])",
        "[x] Document that LoRAs and other dynamic files should use Asset type in payload",
        "[x] Update python-worker examples to use new [tool.cozy.models] config",
        "[x] Add size limits for Asset downloads (prevent abuse with huge LoRA files)\n    - Already implemented via WORKER_MAX_INPUT_FILE_BYTES env var (200MB default)\n    - Enforced in _stream_to_file for external URLs\n    - Pre-checked via HEAD request for Cozy Hub refs"
      ],
      "completed": true
    },
    {
      "id": 41,
      "name": "Create z-image LoRA example showing dynamic LoRA loading",
      "description": "Create an example python worker in ~/cozy/python-worker/examples that demonstrates loading custom LoRAs dynamically at runtime. LoRAs are passed as Asset in the request payload (like input images), downloaded by the worker, and loaded into the pipeline. Based on fal.ai's z-image/turbo/lora pattern.",
      "tasks": [
        "[ ] Create ~/cozy/python-worker/examples/z-image-lora/ directory structure",
        "[ ] Define input schema with LoRA as Asset (LoraSpec struct with file, weight, adapter_name)",
        "[ ] Implement generate function with dynamic LoRA loading:\n    - Load base model via ModelRef(Src.DEPLOYMENT, \"sdxl\")\n    - For each LoRA: load_lora_weights(), set_adapters()\n    - Unload LoRAs with unload_lora_weights() for next request",
        "[ ] Add pyproject.toml with gpu=true and [tool.cozy.models]",
        "[ ] Add example request payloads in README",
        "[ ] Test that LoRA Assets are properly materialized before function runs",
        "[ ] Document the pattern: LoRAs as Assets, not as model config"
      ],
      "completed": false
    },
    {
      "id": 42,
      "name": "Improve manifest to include top-level models list and per-function model requirements",
      "description": "The manifest generated by gen_worker.discover should have a clear top-level list of all required models, plus per-function model requirements. This makes it easy for workers to know what to pre-download, and for schedulers to understand deployment requirements.",
      "tasks": [
        "[x] Add top-level 'models' field to manifest output (keys are deployment-local, values are Cozy Hub IDs)\n    - discover_manifest() includes config[\"models\"] from [tool.cozy.models]",
        "[x] Extract models from all function injection_json and deduplicate\n    - discover_manifest() collects all required model keys from functions",
        "[x] Add 'required_models' field to each function in manifest\n    - _extract_function_metadata() adds required_models list from deployment-source injections",
        "[x] Only include models with source='deployment' (not 'payload' which are dynamic)\n    - required_models only includes keys where source == 'deployment'",
        "[x] Update gen-builder to read top-level models list from manifest\n    - Already done in previous issue #40 work",
        "[x] Update worker startup to use manifest models list for pre-download\n    - Already done in previous issue #40 work",
        "[x] Validate manifest models against [tool.cozy.models] config if present\n    - discover_manifest() prints warning if function requires keys not in config",
        "[x] Update manifest JSON schema documentation\n    - Updated docstring in discover_manifest()",
        "[x] Add tests for models extraction in discover.py\n    - tests/test_discover_models.py with 4 tests covering extraction, validation, warnings"
      ],
      "completed": true
    },
    {
      "id": 43,
      "name": "Create example showing payload-based model selection with multiple fine-tunes",
      "description": "Create an example python worker demonstrating how to support multiple model fine-tunes (e.g., 10 SDXL variants) efficiently. The request payload specifies which model to use via a key, and the scheduler routes to workers that already have that model loaded in VRAM.",
      "tasks": [
        "[ ] Create ~/cozy/python-worker/examples/multi-checkpoint/ directory",
        "[ ] Define multiple models in pyproject.toml [tool.cozy.models]",
        "[ ] Implement function with ModelRef(Src.PAYLOAD, \"model_key\")",
        "[ ] Document how scheduler routing works with deployment keys",
        "[ ] Add example requests showing different model_key values",
        "[ ] Test that model switching works correctly",
        "[ ] Document VRAM considerations (LRU eviction, cold start latency)",
        "[ ] Clarify model specification rules in docs"
      ],
      "completed": false
    },
    {
      "id": 44,
      "name": "Progressive model availability and disk vs VRAM tracking",
      "description": "Distinguish between disk-cached vs VRAM-loaded models for cache-aware routing. Orchestrator only needs to know: (1) is model in VRAM? (2) is model on disk? If neither, it's 'cold'.\n\nStatus: COMPLETE. Python-worker sends both available_models (VRAM) and cached_models (disk) to orchestrator. Gen-orchestrator #217 implements cache-aware scheduling.",
      "tasks": [
        "[x] Update python-worker heartbeat to collect and send model states\n    - Worker._register_worker() collects vram_models and cached_models\n    - Uses ModelCache.get_stats() for model state tracking\n    - Sends available_models (VRAM) and cached_models (disk) in heartbeat",
        "[x] Implement progressive startup in python-worker\n    - ModelCache.are_models_available(model_ids) checks if required models are ready\n    - ModelCache.get_available_models() returns VRAM + disk models\n    - Worker can check model availability before accepting tasks",
        "[x] Handle multi-model functions correctly (check all required_models)\n    - ModelCache.are_models_available(model_ids) checks list of models\n    - Functions have required_models field from issue #42",
        "[x] Add config for download concurrency limit (WORKER_MAX_CONCURRENT_DOWNLOADS)\n    - DEFAULT_MAX_CONCURRENT_DOWNLOADS = 2\n    - ModelCache.get_max_concurrent_downloads() reads env var",
        "[x] Add tests for progressive model availability\n    - TestProgressiveAvailability with 7 tests in test_model_cache.py",
        "[x] Send cached_models in heartbeat (proto updated in gen-orchestrator #217)",
        "[x] Document model availability behavior in python-worker README"
      ],
      "completed": true
    },
    {
      "id": 45,
      "name": "Clean up python-worker: remove torch_manager, fix dependencies, implement new model loader",
      "description": "The python-worker codebase has accumulated legacy code (torch_manager) that should be removed. The new architecture: worker downloads models from Cozy Hub, constructs pipelines using diffusers/transformers, and injects ready-to-use pipelines into tenant functions. Support both safetensors and flashpack formats.",
      "tasks": [
        "[ ] Remove legacy torch_manager module (delete src/gen_worker/torch_manager/ and default_model_manager/)",
        "[ ] Clean up pyproject.toml (safetensors/flashpack in core, torch as optional, remove onnxruntime/tensorrt)",
        "[ ] Implement new model_loader.py module with flashpack-first loading",
        "[ ] Implement ModelCache with LRU eviction and VRAM tracking",
        "[ ] Update _resolve_injected_value in worker.py to use new ModelLoader",
        "[ ] Update worker startup to pre-download models from manifest",
        "[ ] Update base image Dockerfiles (python:3.12-slim + torch + gen-worker core ONLY)",
        "[ ] Update base-images.json to reflect new architecture",
        "[ ] Remove private devpi references from Dockerfiles",
        "[ ] Test clean import without optional deps",
        "[ ] Update __init__.py exports",
        "[ ] Update entrypoint.py to use new ModelLoader",
        "[ ] Update examples to use new model loading pattern",
        "[ ] Document the new architecture in README"
      ],
      "completed": false
    },
    {
      "id": 46,
      "name": "Switch base images from nvidia/cuda to python:3.12-slim",
      "description": "Replace the large nvidia/cuda base images with minimal python:3.12-slim. PyTorch CUDA wheels bundle their own CUDA runtime, so the nvidia base is unnecessary. This reduces image size by ~1-2GB and simplifies the build.",
      "tasks": [
        "[x] Update runtime/Dockerfile.base (GPU) to use python:3.12-slim + PyTorch wheels",
        "[x] Update runtime/Dockerfile.base.cpu",
        "[x] Update runtime/base-images.json",
        "[x] Remove deadsnakes PPA setup",
        "[x] Remove clang (gcc from build-essential is sufficient)",
        "[x] Test image builds and verify smaller sizes",
        "[x] Test PyTorch CUDA functionality (torch.cuda.is_available())",
        "[x] Update build-local-base-images.sh script",
        "[x] Document the change in README"
      ],
      "completed": true
    },
    {
      "id": 47,
      "name": "Update python-worker examples to use correct dependency strategy",
      "description": "Review and update all examples in ~/cozy/python-worker/examples to ensure they follow the new dependency strategy: gen-worker provides core SDK + safetensors + flashpack; torch is a peer dependency (provided by base image); tenant's pyproject.toml adds diffusers/transformers/accelerate as needed.",
      "tasks": [
        "[ ] Audit all example pyproject.toml files in ~/cozy/python-worker/examples/",
        "[ ] Ensure examples do NOT list safetensors or flashpack (gen-worker provides these)",
        "[ ] Ensure examples do NOT list torch (peer dependency from base image)",
        "[ ] Ensure examples that use diffusers pipelines list diffusers/transformers/accelerate",
        "[ ] Verify gen-worker dependency version is consistent across all examples",
        "[ ] Remove any unnecessary dependencies from examples",
        "[ ] Test that examples still build and run with the updated dependencies",
        "[ ] Update example READMEs if they mention dependency installation"
      ],
      "completed": false
    },
    {
      "id": 48,
      "name": "Rename [tool.cozy.runtime] to [tool.cozy.build]",
      "description": "Renamed the config section from [tool.cozy.runtime] to [tool.cozy.build] for clarity.\n\nArchitecture (IMPORTANT - python-worker is the smart library):\n- python-worker parses ALL [tool.cozy.*] config (build, models, resources) into manifest\n- python-worker is the source of truth for config parsing\n- gen-builder is a dumb pipeline that extracts manifest and forwards to orchestrator\n- gen-builder ONLY parses [tool.cozy.build] settings that are needed BEFORE building (gpu, cuda, torch for base image selection) - this is unavoidable chicken-and-egg\n- gen-builder should prefer manifest values whenever possible",
      "tasks": [
        "[x] Rename Runtime struct to Build in gen-builder config.go",
        "[x] Update all cfg.Runtime references to cfg.Build in gen-builder",
        "[x] python-worker parses [tool.cozy.build] into manifest (source of truth)",
        "[x] Update all example pyproject.toml files to use [tool.cozy.build]",
        "[x] Update gen-builder README documentation",
        "[x] Update hello-world example README",
        "[x] Fix gen-builder error messages (runtime -> build)",
        "[x] Fix gen-builder server.go API struct (Runtime -> Build)",
        "[x] Verify both repos compile/run correctly"
      ],
      "completed": true
    },
    {
      "id": 49,
      "name": "Add deployment ID to pyproject.toml config",
      "description": "Allow tenants to specify a default deployment ID in pyproject.toml. This makes projects self-describing and enables CLI usage without flags. The deployment ID from the build request takes precedence if specified.\n\nArchitecture:\n- python-worker parses [tool.cozy].deployment and includes it in the manifest\n- gen-builder extracts deployment from manifest and uses it as default if not in request\n- Build request deployment_id takes precedence over pyproject.toml value",
      "tasks": [
        "[ ] Add deployment field to [tool.cozy] in pyproject.toml schema:\n    ```toml\n    [tool.cozy]\n    deployment = \"my-worker\"  # default deployment ID\n    ```",
        "[ ] python-worker discover.py: parse deployment from pyproject.toml into manifest",
        "[ ] gen-builder: extract deployment from manifest as fallback",
        "[ ] gen-builder: use request.deployment if specified, else manifest.deployment, else error",
        "[ ] Update python-worker examples to include deployment field",
        "[ ] Document deployment ID precedence (request > pyproject.toml)",
        "[ ] Add validation: deployment ID must be valid (lowercase, alphanumeric, hyphens)"
      ],
      "completed": false
    }
  ]
}
