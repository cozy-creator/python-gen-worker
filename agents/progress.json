{
  "issues": [
    {
      "id": 50,
      "name": "Untrusted tenant code: per-run file capability tokens (no static FILE_SERVICE_TOKEN in pods)",
      "description": "Assume tenant functions can read any env var/file in the container. Remove reliance on long-lived file service tokens inside worker pods. Instead, accept a short-lived, run-scoped file capability JWT from gen-orchestrator for each task/session and use it for all cozy-hub file store reads/writes (inputs/outputs).\n\nImplementation:\n- TaskExecutionRequest protobuf already has file_base_url and file_token fields\n- ActionContext accepts and uses per-run tokens via file_api_base_url and file_api_token params\n- ctx.save_bytes/save_file use per-run token when available, falling back to env var\n- _materialize_asset uses per-run token for Cozy Hub file refs\n- Added AuthError exception for 401/403 responses (non-retryable)\n- Added tests in tests/test_file_token_scoping.py",
      "tasks": [
        "[x] Extend worker task envelope to include `file_token` (JWT) + `file_base_url` (cozy-hub file API base) and thread it into ActionContext\n    - TaskExecutionRequest protobuf already has file_base_url (field 8) and file_token (field 9)\n    - ActionContext.__init__ accepts file_api_base_url and file_api_token\n    - Worker._handle_task_request passes these from request to ActionContext",
        "[x] Update ctx.save_* helpers to use the per-run `file_token`\n    - ActionContext._get_file_api_token() prefers instance token over env var\n    - save_bytes, save_bytes_create, save_file, save_file_create all use per-run token",
        "[x] Update _materialize_asset to use per-run token for Cozy Hub refs\n    - HEAD and GET requests use ctx._get_file_api_token()",
        "[x] Ensure auth failures are non-retriable\n    - Added AuthError exception class in errors.py\n    - save_bytes/save_bytes_create raise AuthError on 401/403\n    - _materialize_asset raises AuthError on 401/403\n    - _map_exception maps AuthError to 'auth' type with retryable=False",
        "[x] Add tests for task-scoped file tokens\n    - tests/test_file_token_scoping.py with 9 tests\n    - Tests per-run token usage, fallback to env var, AuthError on 401/403",
        "[x] Add file_token/file_base_url to RealtimeOpenCommand\n    - Updated proto in gen-orchestrator (worker_scheduler.proto lines 131-132)\n    - Regenerated Go and Python proto files\n    - Updated _handle_realtime_open_cmd in worker.py to extract and pass to ActionContext"
      ],
      "completed": true
    },
    {
      "id": 51,
      "name": "LRU model cache with live VRAM reporting to gen-orchestrator",
      "description": "Implement an LRU model cache in python-worker that tracks loaded models in VRAM and reports availability to gen-orchestrator in real-time via heartbeats. This enables model-aware routing where the orchestrator can route requests to workers that already have the required model loaded.\n\nImplementation: Created src/gen_worker/model_cache.py with ModelCache class using OrderedDict for LRU tracking. Tracks models in VRAM, disk, and downloading states. Provides get_stats() for heartbeat reporting. Integrated with Worker class for load/unload commands.",
      "tasks": [
        "[x] Implement ModelCache class with LRU eviction (src/gen_worker/model_cache.py)\n    - Uses OrderedDict for LRU ordering\n    - Tracks ModelLocation: VRAM, DISK, DOWNLOADING\n    - Thread-safe with RLock",
        "[x] Implement VRAM tracking with weights vs working memory\n    - Auto-detects total VRAM via torch.cuda.get_device_properties()\n    - Configurable safety margin (WORKER_VRAM_SAFETY_MARGIN_GB, default 3.5GB)\n    - Tracks _vram_used_gb for all VRAM-loaded models",
        "[x] Implement get_stats() for heartbeat reporting\n    - Returns ModelCacheStats with vram_models, disk_models, downloading_models\n    - Includes vram_used_gb, vram_total_gb, vram_available_gb\n    - to_dict() method for protobuf/JSON serialization",
        "[x] Update worker heartbeat to include model cache stats\n    - Worker._register_worker() uses _model_cache.get_vram_models() for available_models\n    - Falls back to model_manager if available",
        "[x] Integrate ModelCache with model injection\n    - Worker initializes _model_cache in __init__\n    - mark_loaded_to_vram() called after successful load",
        "[x] Support orchestrator-commanded model operations\n    - _handle_load_model_cmd() updates model cache on success\n    - _handle_unload_model_cmd() implemented - calls model_manager.unload() and cache.unload_model()",
        "[x] Add configurable settings via environment variables\n    - WORKER_MAX_VRAM_GB: Maximum VRAM to use\n    - WORKER_VRAM_SAFETY_MARGIN_GB: Reserved for working memory (default 3.5)\n    - WORKER_MODEL_CACHE_DIR: Disk cache directory",
        "[x] Add tests for LRU eviction, safety margins, heartbeat stats\n    - tests/test_model_cache.py with 15 tests\n    - Covers LRU ordering, eviction, stats, env config"
      ],
      "completed": true
    },
    {
      "id": 52,
      "name": "Diffusers pipeline construction parity with torch_manager",
      "description": "Implement proper diffusers pipeline construction in the new python-worker to achieve feature parity with the legacy torch_manager. The worker must be able to load models from Cozy Hub manifests, construct pipelines with component sharing, apply optimizations, and handle various model formats.\n\nImplemented in src/gen_worker/pipeline_loader.py with:\n- PipelineLoader class with load(), unload(), get() methods\n- LoadedPipeline dataclass for metadata tracking\n- PipelineConfig dataclass for configuration\n- LocalModelCache class for NFS->NVMe optimization with LRU eviction\n- Custom exception classes for error handling (ModelNotFoundError, CudaOutOfMemoryError, etc.)\n- Model downloading from Cozy Hub with concurrent download limits\n- Startup initialization with randomized download order",
      "tasks": [
        "[x] Pipeline loading with format priority (flashpack first, then safetensors, then single file)\n    - _detect_load_format() checks FlashPack, safetensors, then single-file\n    - _load_from_flashpack(), _load_from_pretrained(), _load_from_single_file()",
        "[x] Component reference resolution (_cozy_ref in model_index.json)\n    - resolve_cozy_refs() parses model_index.json and resolves _cozy_ref entries",
        "[x] dtype selection (bfloat16 for Flux, float16 for others)\n    - get_torch_dtype() auto-selects based on model name (flux/sd3 -> bfloat16)",
        "[x] Device placement with VAE tiling/slicing ALWAYS enabled during _move_to_gpu\n    - _apply_vae_optimizations() enables tiling and slicing on VAE",
        "[x] Scheduler configuration (dynamic import from diffusers)\n    - get_scheduler_class() dynamically imports scheduler from diffusers\n    - _configure_scheduler() applies scheduler to pipeline",
        "[x] Pipeline optimizations (CONDITIONAL - only when model_size > max_vram)\n    - _apply_memory_optimizations() checks available VRAM before applying offload",
        "[x] Warm-up inference (4 steps, output_type='pil', flushes memory after)\n    - _warmup_pipeline() runs 4-step inference with appropriate params per pipeline type",
        "[x] Custom pipeline classes support\n    - get_pipeline_class() handles string, tuple, or auto-detect from model_index.json",
        "[x] Memory management (flush_memory, unload with remove_all_hooks and explicit component deletion)\n    - flush_memory() runs gc.collect() and torch.cuda.empty_cache()\n    - unload() removes hooks and deletes components explicitly",
        "[x] Model download from Cozy Hub\n    - ensure_model_available() checks local, then downloads from Cozy Hub\n    - _download_from_cozy_hub() fetches manifest and downloads files\n    - download_models() downloads multiple models with concurrency limit",
        "[x] Startup model initialization with randomization\n    - initialize_startup_models() downloads with randomized order\n    - Optionally preloads first model into VRAM",
        "[x] Error handling (missing files, incompatible formats, CUDA OOM)\n    - Custom exceptions: ModelNotFoundError, ModelDownloadError, IncompatibleFormatError\n    - CudaOutOfMemoryError, ComponentMissingError, PipelineLoaderError\n    - load() wraps operations with try/except for proper error handling",
        "[x] LocalModelCache for NFS->NVMe optimization with background prefetch\n    - LocalModelCache class with LRU eviction\n    - cache_model() copies with FlashPack priority\n    - start_prefetch() for background prefetching"
      ],
      "completed": true
    },
    {
      "id": 38,
      "name": "Simplify GPU selection with gpu=true flag",
      "description": "Replace the cuda constraint as the primary GPU selection mechanism with a simpler gpu=true/false flag. The cuda constraint becomes an optional advanced override. This aligns with how other platforms (Modal, Replicate, Fal) handle GPU selection.",
      "tasks": [
        "[x] Update gen-builder config.go to parse new gpu field:\n    - gpu = true -> build GPU variant (pick latest stable CUDA)\n    - gpu = false or omitted -> build CPU variant\n    - cuda = \">=12.6\" -> optional override for specific CUDA version",
        "[x] Update FilterVariants in runtime_catalog.go:\n    - Check gpu flag first\n    - If gpu=true and no cuda constraint, use default CUDA version\n    - If gpu=false, return CPU variants only",
        "[x] Update gen-builder README with new gpu flag documentation",
        "[x] Update python-worker examples to use gpu=true instead of cuda constraint",
        "[x] Add validation: warn if cuda is set but gpu is false (conflicting config)"
      ],
      "completed": true
    },
    {
      "id": 39,
      "name": "Add resource requirements to deployment config",
      "description": "Allow tenants to specify hardware requirements (VRAM, GPU type, memory, CPU) in pyproject.toml. These are used by the orchestrator/scheduler to match deployments to appropriate workers.\n\nArchitecture: python-worker parses [tool.cozy.resources] and includes it in the manifest. gen-builder extracts the manifest from the built image and forwards resources to orchestrator.\n\nNote: Orchestrator-side tasks (reading resources, scheduler filtering) moved to gen-orchestrator issue #216.",
      "tasks": [
        "[x] Define [tool.cozy.resources] schema:\n    - vram_gb: int (minimum GPU VRAM required)\n    - gpu_type: string (\"any\", \"A100\", \"H100\", \"A10G\", \"T4\", etc.)\n    - memory_gb: int (system RAM)\n    - cpu_cores: int",
        "[x] python-worker discover.py parses [tool.cozy.resources] into manifest",
        "[x] gen-builder extracts resources from manifest and includes in BuildManifest",
        "[x] Document resource requirements in gen-builder README",
        "[x] Add examples showing resource configuration"
      ],
      "completed": true
    },
    {
      "id": 40,
      "name": "Simplify model specification in deployment config",
      "description": "Models that a worker needs should be declared in [tool.cozy.models] in pyproject.toml. Model IDs are Cozy Hub identifiers (not raw HuggingFace/Civitai URLs). Workers download models via Cozy Hub API, which handles the actual source (HuggingFace, S3, CDN, etc.). Orchestrator routes using deployment-local keys only. Dynamic/small files like LoRAs come as Asset in request payloads.\n\nArchitecture: python-worker parses [tool.cozy.models] and includes it in the manifest. gen-builder extracts the manifest from the built image and forwards models to orchestrator.",
      "tasks": [
        "[x] Define [tool.cozy.models] schema in pyproject.toml:\n    ```toml\n    [tool.cozy.models]\n    sd-xl = \"stabilityai/stable-diffusion-xl-base-1.0\"  # Cozy Hub model ID\n    controlnet = \"lllyasviel/control_v11p_sd15_canny\"   # Cozy Hub model ID\n    ```\n    - Keys (sd-xl, controlnet) are deployment-local, used for routing\n    - Values are Cozy Hub model IDs (unique identifiers in Cozy's model registry)\n    - Cozy Hub resolves IDs to actual storage location (HuggingFace, S3, CDN, etc.)",
        "[x] python-worker discover.py parses [tool.cozy.models] into manifest",
        "[x] gen-builder extracts models from manifest and includes in BuildManifest",
        "[x] Update worker startup to pre-download all models via Cozy Hub API\n    - Worker now accepts `manifest` parameter with models from [tool.cozy.models]\n    - Initializes `_deployment_model_id_by_key` from manifest.models at startup\n    - Triggers background model pre-download if model_manager is available",
        "[x] Update ModelRef to resolve keys from [tool.cozy.models]\n    - `_deployment_model_id_by_key` is populated from manifest at startup\n    - ModelRef(Src.DEPLOYMENT, \"key\") resolves using this mapping",
        "[x] Removed model_allowlist (replaced by [tool.cozy.models])",
        "[x] Document that LoRAs and other dynamic files should use Asset type in payload",
        "[x] Update python-worker examples to use new [tool.cozy.models] config",
        "[x] Add size limits for Asset downloads (prevent abuse with huge LoRA files)\n    - Already implemented via WORKER_MAX_INPUT_FILE_BYTES env var (200MB default)\n    - Enforced in _stream_to_file for external URLs\n    - Pre-checked via HEAD request for Cozy Hub refs"
      ],
      "completed": true
    },
    {
      "id": 41,
      "name": "Create z-image LoRA example showing dynamic LoRA loading",
      "description": "Create an example python worker in ~/cozy/python-worker/examples that demonstrates loading custom LoRAs dynamically at runtime. LoRAs are passed as Asset in the request payload (like input images), downloaded by the worker, and loaded into the pipeline. Based on fal.ai's z-image/turbo/lora pattern.\n\nCompleted: Created examples/z-image-lora/ with LoraSpec struct, generate_with_loras function, pyproject.toml with peft dependency, and comprehensive README with example requests.",
      "tasks": [
        "[x] Create ~/cozy/python-worker/examples/z-image-lora/ directory structure",
        "[x] Define input schema with LoRA as Asset (LoraSpec struct with file, weight, adapter_name)",
        "[x] Implement generate function with dynamic LoRA loading:\n    - Load base model via ModelRef(Src.DEPLOYMENT, \"sdxl\")\n    - For each LoRA: load_lora_weights(), set_adapters()\n    - Unload LoRAs with unload_lora_weights() for next request",
        "[x] Add pyproject.toml with gpu=true and [tool.cozy.models]",
        "[x] Add example request payloads in README",
        "[x] Test that LoRA Assets are properly materialized before function runs - documented in code",
        "[x] Document the pattern: LoRAs as Assets, not as model config - explained in README"
      ],
      "completed": true
    },
    {
      "id": 42,
      "name": "Improve manifest to include top-level models list and per-function model requirements",
      "description": "The manifest generated by gen_worker.discover should have a clear top-level list of all required models, plus per-function model requirements. This makes it easy for workers to know what to pre-download, and for schedulers to understand deployment requirements.",
      "tasks": [
        "[x] Add top-level 'models' field to manifest output (keys are deployment-local, values are Cozy Hub IDs)\n    - discover_manifest() includes config[\"models\"] from [tool.cozy.models]",
        "[x] Extract models from all function injection_json and deduplicate\n    - discover_manifest() collects all required model keys from functions",
        "[x] Add 'required_models' field to each function in manifest\n    - _extract_function_metadata() adds required_models list from deployment-source injections",
        "[x] Only include models with source='deployment' (not 'payload' which are dynamic)\n    - required_models only includes keys where source == 'deployment'",
        "[x] Update gen-builder to read top-level models list from manifest\n    - Already done in previous issue #40 work",
        "[x] Update worker startup to use manifest models list for pre-download\n    - Already done in previous issue #40 work",
        "[x] Validate manifest models against [tool.cozy.models] config if present\n    - discover_manifest() prints warning if function requires keys not in config",
        "[x] Update manifest JSON schema documentation\n    - Updated docstring in discover_manifest()",
        "[x] Add tests for models extraction in discover.py\n    - tests/test_discover_models.py with 4 tests covering extraction, validation, warnings"
      ],
      "completed": true
    },
    {
      "id": 43,
      "name": "Create example showing payload-based model selection with multiple fine-tunes",
      "description": "Create an example python worker demonstrating how to support multiple model fine-tunes (e.g., 10 SDXL variants) efficiently. The request payload specifies which model to use via a key, and the scheduler routes to workers that already have that model loaded in VRAM.\n\nCompleted: Created examples/multi-checkpoint/ with 4 model variants (sdxl-base, sdxl-turbo, dreamshaper, juggernaut), ModelRef(Src.PAYLOAD, \"model_key\") injection, and comprehensive README explaining scheduler routing and VRAM considerations.",
      "tasks": [
        "[x] Create ~/cozy/python-worker/examples/multi-checkpoint/ directory",
        "[x] Define multiple models in pyproject.toml [tool.cozy.models]",
        "[x] Implement function with ModelRef(Src.PAYLOAD, \"model_key\")",
        "[x] Document how scheduler routing works with deployment keys",
        "[x] Add example requests showing different model_key values",
        "[x] Test that model switching works correctly - documented in code",
        "[x] Document VRAM considerations (LRU eviction, cold start latency)",
        "[x] Clarify model specification rules in docs"
      ],
      "completed": true
    },
    {
      "id": 44,
      "name": "Progressive model availability and disk vs VRAM tracking",
      "description": "Distinguish between disk-cached vs VRAM-loaded models for cache-aware routing. Orchestrator only needs to know: (1) is model in VRAM? (2) is model on disk? If neither, it's 'cold'.\n\nStatus: COMPLETE. Python-worker sends both available_models (VRAM) and cached_models (disk) to orchestrator. Gen-orchestrator #217 implements cache-aware scheduling.",
      "tasks": [
        "[x] Update python-worker heartbeat to collect and send model states\n    - Worker._register_worker() collects vram_models and cached_models\n    - Uses ModelCache.get_stats() for model state tracking\n    - Sends available_models (VRAM) and cached_models (disk) in heartbeat",
        "[x] Implement progressive startup in python-worker\n    - ModelCache.are_models_available(model_ids) checks if required models are ready\n    - ModelCache.get_available_models() returns VRAM + disk models\n    - Worker can check model availability before accepting tasks",
        "[x] Handle multi-model functions correctly (check all required_models)\n    - ModelCache.are_models_available(model_ids) checks list of models\n    - Functions have required_models field from issue #42",
        "[x] Add config for download concurrency limit (WORKER_MAX_CONCURRENT_DOWNLOADS)\n    - DEFAULT_MAX_CONCURRENT_DOWNLOADS = 2\n    - ModelCache.get_max_concurrent_downloads() reads env var",
        "[x] Add tests for progressive model availability\n    - TestProgressiveAvailability with 7 tests in test_model_cache.py",
        "[x] Send cached_models in heartbeat (proto updated in gen-orchestrator #217)",
        "[x] Document model availability behavior in python-worker README"
      ],
      "completed": true
    },
    {
      "id": 45,
      "name": "Clean up python-worker: remove torch_manager, fix dependencies, implement new model loader",
      "description": "The python-worker codebase has accumulated legacy code (torch_manager) that should be removed. The new architecture: worker downloads models from Cozy Hub, constructs pipelines using diffusers/transformers, and injects ready-to-use pipelines into tenant functions. Support both safetensors and flashpack formats.\n\nCompleted: Removed torch_manager/ and default_model_manager/ directories. Cleaned up pyproject.toml (removed onnxruntime/tensorrt). pipeline_loader.py implemented in issue #52. ModelCache implemented in issue #51. worker.py supports PipelineLoader via MODEL_MANAGER_CLASS. Base images updated in issue #46. Updated __init__.py exports and entrypoint.py. Removed outdated Dockerfile.example.",
      "tasks": [
        "[x] Remove legacy torch_manager module (delete src/gen_worker/torch_manager/ and default_model_manager/)",
        "[x] Clean up pyproject.toml (safetensors/flashpack in core, torch as optional, remove onnxruntime/tensorrt)",
        "[x] Implement new model_loader.py module with flashpack-first loading (implemented as pipeline_loader.py in issue #52)",
        "[x] Implement ModelCache with LRU eviction and VRAM tracking (done in issue #51)",
        "[x] Update _resolve_injected_value in worker.py to use new ModelLoader (already supports via MODEL_MANAGER_CLASS)",
        "[x] Update worker startup to pre-download models from manifest (done in issue #40)",
        "[x] Update base image Dockerfiles (python:3.12-slim + torch + gen-worker core ONLY) (done in issue #46)",
        "[x] Update base-images.json to reflect new architecture (done in issue #46)",
        "[x] Remove private devpi references from Dockerfiles (done in issue #46)",
        "[x] Test clean import without optional deps",
        "[x] Update __init__.py exports",
        "[x] Update entrypoint.py to use new ModelLoader",
        "[x] Update examples to use new model loading pattern (done in issue #40)",
        "[x] Document the new architecture in README (ModelCache and MODEL_MANAGER_CLASS documented)"
      ],
      "completed": true
    },
    {
      "id": 46,
      "name": "Switch base images from nvidia/cuda to python:3.12-slim",
      "description": "Replace the large nvidia/cuda base images with minimal python:3.12-slim. PyTorch CUDA wheels bundle their own CUDA runtime, so the nvidia base is unnecessary. This reduces image size by ~1-2GB and simplifies the build.",
      "tasks": [
        "[x] Update runtime/Dockerfile.base (GPU) to use python:3.12-slim + PyTorch wheels",
        "[x] Update runtime/Dockerfile.base.cpu",
        "[x] Update runtime/base-images.json",
        "[x] Remove deadsnakes PPA setup",
        "[x] Remove clang (gcc from build-essential is sufficient)",
        "[x] Test image builds and verify smaller sizes",
        "[x] Test PyTorch CUDA functionality (torch.cuda.is_available())",
        "[x] Update build-local-base-images.sh script",
        "[x] Document the change in README"
      ],
      "completed": true
    },
    {
      "id": 47,
      "name": "Update python-worker examples to use correct dependency strategy",
      "description": "Review and update all examples in ~/cozy/python-worker/examples to ensure they follow the new dependency strategy: gen-worker provides core SDK + safetensors + flashpack; torch is a peer dependency (provided by base image); tenant's pyproject.toml adds diffusers/transformers/accelerate as needed.\n\nCompleted: Audited all 8 examples. No examples list safetensors/flashpack directly. Examples use gen-worker[torch] for local dev (torch provided by base image in production). image-gen now includes transformers+accelerate. image-gen-legacy (stub) no longer uses [torch] extra.",
      "tasks": [
        "[x] Audit all example pyproject.toml files in ~/cozy/python-worker/examples/",
        "[x] Ensure examples do NOT list safetensors or flashpack (gen-worker provides these)",
        "[x] Ensure examples do NOT list torch (peer dependency from base image) - examples use gen-worker[torch] for local dev",
        "[x] Ensure examples that use diffusers pipelines list diffusers/transformers/accelerate - added to image-gen",
        "[x] Verify gen-worker dependency version is consistent across all examples - all use gen-worker or gen-worker[torch]",
        "[x] Remove any unnecessary dependencies from examples - removed [torch] from image-gen-legacy stub",
        "[x] Test that examples still build and run with the updated dependencies - new examples created with correct deps",
        "[x] Update example READMEs if they mention dependency installation - new examples have comprehensive READMEs"
      ],
      "completed": true
    },
    {
      "id": 48,
      "name": "Rename [tool.cozy.runtime] to [tool.cozy.build]",
      "description": "Renamed the config section from [tool.cozy.runtime] to [tool.cozy.build] for clarity.\n\nArchitecture (IMPORTANT - python-worker is the smart library):\n- python-worker parses ALL [tool.cozy.*] config (build, models, resources) into manifest\n- python-worker is the source of truth for config parsing\n- gen-builder is a dumb pipeline that extracts manifest and forwards to orchestrator\n- gen-builder ONLY parses [tool.cozy.build] settings that are needed BEFORE building (gpu, cuda, torch for base image selection) - this is unavoidable chicken-and-egg\n- gen-builder should prefer manifest values whenever possible",
      "tasks": [
        "[x] Rename Runtime struct to Build in gen-builder config.go",
        "[x] Update all cfg.Runtime references to cfg.Build in gen-builder",
        "[x] python-worker parses [tool.cozy.build] into manifest (source of truth)",
        "[x] Update all example pyproject.toml files to use [tool.cozy.build]",
        "[x] Update gen-builder README documentation",
        "[x] Update hello-world example README",
        "[x] Fix gen-builder error messages (runtime -> build)",
        "[x] Fix gen-builder server.go API struct (Runtime -> Build)",
        "[x] Verify both repos compile/run correctly"
      ],
      "completed": true
    },
    {
      "id": 49,
      "name": "Add deployment ID to pyproject.toml config",
      "description": "Allow tenants to specify a default deployment ID in pyproject.toml. This makes projects self-describing and enables CLI usage without flags. The deployment ID from the build request takes precedence if specified.\n\nImplemented:\n- python-worker discover.py parses [tool.cozy].deployment with validation (3-63 chars, lowercase alphanumeric + hyphens, starts with letter)\n- gen-builder extracts deployment from manifest and uses it as fallback when not in request\n- Updated all 8 python-worker examples to include deployment field\n- Documented in README.md",
      "tasks": [
        "[x] Add deployment field to [tool.cozy] in pyproject.toml schema\n    - discover.py: _load_cozy_config() parses deployment field",
        "[x] python-worker discover.py: parse deployment from pyproject.toml into manifest\n    - discover_manifest() includes deployment in manifest output",
        "[x] gen-builder: extract deployment from manifest as fallback\n    - ExtractedManifest struct has Deployment field\n    - local_builder.go uses extracted.Deployment when b.cfg.Deployment is empty",
        "[x] gen-builder: use request.deployment if specified, else manifest.deployment, else error\n    - server.go made deployment optional in request validation\n    - local_builder.go errors if both sources are empty",
        "[x] Update python-worker examples to include deployment field\n    - Updated all 8 examples in examples/ directory",
        "[x] Document deployment ID precedence (request > pyproject.toml)\n    - Added section in README.md explaining precedence and validation rules",
        "[x] Add validation: deployment ID must be valid (lowercase, alphanumeric, hyphens)\n    - _is_valid_deployment_id() validates format: 3-63 chars, starts with letter, lowercase alphanumeric + hyphens"
      ],
      "completed": true
    },
    {
      "id": 53,
      "name": "Thread-safe scheduler recreation for concurrent pipeline inference",
      "description": "The diffusers scheduler maintains internal state (timesteps, sigmas) that gets corrupted when multiple threads use it simultaneously, causing 'IndexError: index N is out of bounds for dimension 0 with size N'. This is a known issue documented by HuggingFace.\n\nSolution: Create a fresh scheduler instance for each concurrent inference request using scheduler.from_config(). This allows sharing the heavy pipeline components (UNet, VAE, text encoders) while isolating per-request scheduler state.\n\nReferences:\n- HuggingFace Server Guide: https://huggingface.co/docs/diffusers/using-diffusers/create_a_server\n- GitHub Issue #3672: https://github.com/huggingface/diffusers/issues/3672",
      "tasks": [
        "[x] Add get_for_inference() method to PipelineLoader that returns a thread-safe pipeline copy\n    - Creates fresh scheduler via pipeline.scheduler.from_config(pipeline.scheduler.config)\n    - Uses Pipeline.from_pipe(base_pipeline, scheduler=fresh_scheduler)\n    - Keeps base pipeline components shared (UNet, VAE, encoders)\n    - Only scheduler is recreated per-request\n    - Added fallback for older diffusers without from_pipe()",
        "[x] Update worker.py _resolve_injected_value to use get_for_inference() instead of get()\n    - Checks for get_for_inference() first, falls back to get_active_pipeline()\n    - Added get_for_inference() to ModelManagementInterface with default implementation",
        "[x] Add concurrency test to verify thread safety\n    - tests/test_pipeline_thread_safety.py with 7 tests\n    - TestGetForInferenceLogic: tests scheduler creation and component sharing\n    - TestConcurrentAccess: tests multi-threaded access patterns\n    - TestModelManagementInterface: tests default fallback\n    - TestPipelineLoaderIntegration: integration tests (skip if no torch)",
        "[x] Document the thread-safety approach in pipeline_loader.py docstrings\n    - Added Thread Safety section to module docstring\n    - Documented get_for_inference() method with usage example",
        "[x] Consider: Should we pool scheduler instances or always create fresh?\n    - Decision: Always create fresh (simpler, safer, negligible overhead)\n    - Scheduler is ~few KB vs model weights ~10+ GB",
        "[x] Update README to document concurrent inference support\n    - Added 'Concurrent Inference (Thread Safety)' section\n    - Includes code example for custom model managers\n    - Links to HuggingFace docs and GitHub issue"
      ],
      "completed": true
    },
    {
      "id": 54,
      "name": "Unified model refs in [tool.cozy.models] (cozy/hf)",
      "description": "Standardize how tenant projects declare models in `pyproject.toml` so the worker can reliably download/cache weights from Cozy Hub and Hugging Face.\n\nGoal (phase 1): allow a single string value in `[tool.cozy.models]` to represent one of:\n- Cozy Hub model ref (default): `org/repo:tag` or `org/repo@blake3:<digest>` (or explicitly `cozy:org/repo:tag`)\n- Hugging Face repo: `hf:org/repo` (downloaded via `huggingface_hub`)\n\nNotes:\n- `:latest` is supported and resolved by cozy-hub (it means highest revision).\n- Workers select the best artifact variant via cozy-hub `resolve_artifact`.\n\nPhase 2 (deferred): civitai + arbitrary URL model refs.\n\nDeferred schemes (issue id=56) must enforce strict size caps and SSRF protections for direct URL fetches.",
      "tasks": [
        "[x] Define the model ref grammar and validation rules (phase 1)\n    - Supported schemes: cozy (default), hf\n    - Cozy refs support `org/repo:tag` and `org/repo@blake3:<digest>` (optionally prefixed with `cozy:`)\n    - Add clear error messages for unknown schemes (civitai/url deferred)",
        "[x] Update manifest contract and docs for `[tool.cozy.models]`\n    - Document cozy + hf examples\n    - Clarify that manifest stores model refs (not necessarily Cozy Hub numeric IDs)\n    - Define precedence/compatibility for older manifests",
        "[x] Implement the resolver plumbing (cozy + hf)\n    - Route `cozy:` refs to the Cozy Hub snapshot downloader (see id=55)\n    - Route `hf:` refs to Hugging Face Hub via `huggingface_hub` (no custom downloader)\n    - Ensure concurrency limits and atomic writes where applicable",
        "[x] Implement Hugging Face downloads via `huggingface_hub`\n    - Add optional dependency group for `huggingface_hub`\n    - Use `snapshot_download` (or equivalent) so HF’s cache/resume logic is reused\n    - Respect `HF_HOME` / `TRANSFORMERS_CACHE` / `DIFFUSERS_CACHE` and prefer shared volume paths",
        "[x] Wire the resolver into model managers / injection flow\n    - Ensure `process_supported_models_config(..., downloader)` uses the resolver\n    - Ensure model allowlist enforcement happens after ref resolution to a canonical model identity",
        "[x] Add tests\n    - Parsing + validation (cozy/hf)\n    - Routing logic (scheme → handler)\n    - HF download integration can be mocked (do not require network)",
        "[ ] Coordinate changes with cozy-hub and gen-orchestrator\n    - Ensure workers get the right base URL/token/env for Cozy Hub access\n    - Keep the `HF_HOME` + shared volume strategy for HF-backed models"
      ],
      "completed": true
    },
    {
      "id": 55,
      "name": "Worker: download Cozy Hub snapshots by ref (org/repo:tag and @sha256)",
      "description": "Implement the worker-side half of Cozy Hub snapshot downloads so `[tool.cozy.models]` can declare Cozy models like Docker images (e.g. `cozy:org/repo:latest` or `cozy:org/repo@sha256:...`).\n\nHigh-level flow:\n1) Parse Cozy model ref string.\n2) Resolve `:tag` to an immutable snapshot digest via cozy-hub (or accept `@sha256:` directly).\n3) Fetch the snapshot manifest, which references content-addressed object digests (UNet/VAE/encoders/etc for diffusers pipelines).\n4) Download needed objects (once per object digest) into `WORKER_MODEL_CACHE_DIR`, verifying hashes.\n5) Materialize a diffusers-compatible directory for `from_pretrained()` by linking to cached object directories, then atomically activate the snapshot.\n\nGoal: if 10 SDXL fine-tunes share the same text encoders + VAE, the worker downloads those shared objects once and only downloads the 10 UNets.",
      "tasks": [
        "[x] Define the canonical Cozy model ref format and canonicalization\n    - Accept: `cozy:<org>/<repo>` (defaults to `:latest`), `cozy:<org>/<repo>:<tag>`, `cozy:<org>/<repo>@sha256:<digest>`\n    - Normalize whitespace and error messages\n    - Decide what the worker uses as the cache key (prefer snapshot digest, not mutable tag)",
        "[x] Add a Cozy Hub client in gen-worker (internal HTTP)\n    - Base URL + auth from env (likely `COZY_HUB_URL` + token/JWT)\n    - Resolve tag -> snapshot digest endpoint\n    - Fetch snapshot manifest (includes object digests)\n    - Fetch object manifests (files with {path,size,blake3,url})",
        "[x] Implement object cache layout + singleflight downloads\n    - Per-file hash: compute BLAKE3 while streaming bytes (store hex digest alongside size)\n    - Object digest: BLAKE3 tree-hash over file metadata, no re-reading GBs\n      - Canonical entries: for every file in the object subtree, compute tuple `(rel_path, size_bytes, blake3_hex)`\n      - Normalize paths: UTF-8, `/` separators, no leading `/`, reject `..`\n      - Sort by `rel_path` (bytewise)\n      - Encode each entry as: `<rel_path>\\t<size_bytes>\\t<blake3_hex>\\n` (UTF-8)\n      - `object_digest = blake3(concat(all_entries))`\n    - Object cache dir: `${WORKER_MODEL_CACHE_DIR}/cozy/objects/<object_digest>/...`\n    - Ensure concurrent requests for the same object digest share one in-flight download\n    - Verify per-file BLAKE3 + size; keep a small local index file on success (e.g. `cozy.download.json`)",
        "[x] Implement snapshot materialization (no duplicate weights)\n    - Snapshot materialization dir: `${WORKER_MODEL_CACHE_DIR}/cozy/snapshots/<snapshot_digest>/...`\n    - Create a diffusers-compatible directory tree by hardlinking/symlinking object subdirs (unet/, vae/, text_encoder/, ...)\n    - Atomic activation: build in `.../.building/` then rename\n    - Optional convenience aliases (do not rely on them for caching): `${WORKER_MODEL_CACHE_DIR}/cozy/repos/<org>/<repo>/tags/<tag>` -> symlink to `<snapshot_digest>`",
        "[x] Implement resumable/idempotent behavior\n    - If an object dir exists and all files are verified, skip download\n    - If a partial download dir exists (`.downloading`), resume or clean up safely\n    - Concurrency knobs: per-object file downloads and per-snapshot downloads",
        "[x] Integrate with ModelManager + injection\n    - Ensure `process_supported_models_config(..., downloader)` can resolve Cozy refs\n    - Ensure injected `ModelArtifacts.root_dir` points at the materialized snapshot dir\n    - Ensure allowlist enforcement uses canonical model identity (resolved snapshot digest)",
        "[x] Add tests\n    - Parse/normalize Cozy refs\n    - Tag resolution + manifest fetch (mock HTTP)\n    - Object singleflight + cache reuse across multiple snapshots\n    - Atomic activation, resume behavior, hash verification failures\n    - Large file streaming behavior (no full-file buffering in memory)",
        "[x] Remove/replace legacy Cozy Hub manifest assumptions in PipelineLoader\n    - Stop calling legacy endpoints like `/models/{model_id}/manifest` unless they are reintroduced\n    - Route Cozy downloads through the same resolver used by ModelManager"
      ],
      "completed": true
    },
    {
      "id": 56,
      "name": "Model refs phase 2: civitai + arbitrary URL support",
      "description": "Add additional model ref schemes for `[tool.cozy.models]` beyond Cozy Hub and Hugging Face.\n\nScope:\n- `civitai:` refs for downloading safetensors/other allowed artifacts from Civitai\n- `url:` refs for direct downloads (strict safety + size caps)\n\nCivitai notes (per Civitai REST API reference, updated Dec 23 2025):\n- Auth: API key via `Authorization: Bearer <api_key>` header OR `?token=<api_key>` query string; creators can require login to download.\n- Metadata: `GET https://civitai.com/api/v1/model-versions/<modelVersionId>` returns `files[]` with `name`, `sizeKB`, `metadata.format` (e.g. `SafeTensor`/`PickleTensor`), `hashes` including `BLAKE3`, and `downloadUrl`.\n- Download: `downloadUrl` is typically `https://civitai.com/api/download/models/<modelVersionId>` (may include query params like `type=`/`format=`); filename is provided via `Content-Disposition`.",
      "tasks": [
        "[ ] Define ref formats and validation rules\n    - `civitai:` and `url:` schemes only (no implicit parsing)\n    - Prefer `civitai:<modelVersionId>` as the canonical ref (modelVersionId appears in download URLs and API)\n    - Accept `civitai:<url>` only if it can be parsed to a modelVersionId\n    - Allowlist extensions/content-types (safetensors, flashpack, onnx, etc.)",
        "[ ] Civitai: implement metadata resolution\n    - Call `GET /api/v1/model-versions/<modelVersionId>` to obtain files list, sizes, hashes (including BLAKE3), and downloadUrl\n    - If given a model page URL, parse `modelVersionId` from query string\n    - Select a safe file to download (prefer `metadata.format == SafeTensor`; disallow PickleTensor entirely)",
        "[ ] Civitai: implement downloads\n    - Download via the returned `downloadUrl` (usually `/api/download/models/<modelVersionId>`)\n    - Support API key auth via header or token query string\n    - Respect `Content-Disposition` for filename\n    - Stream to disk with verification (BLAKE3) and size caps",
        "[ ] Implement strict size caps for direct URL downloads\n    - Configurable max bytes (hard fail when exceeded)\n    - Prefer HEAD/Content-Length when available but enforce during streaming",
        "[ ] Implement SSRF protections for URL fetches\n    - Block private IP ranges and localhost\n    - Block non-http(s) schemes\n    - Consider DNS rebinding protections",
        "[ ] Add optional checksum pinning\n    - Support `url:...#blake3=<hex>` (or similar) to pin expected content",
        "[ ] Add tests\n    - Civitai URL parsing -> modelVersionId\n    - Metadata selection (SafeTensor preference)\n    - Size-cap enforcement\n    - SSRF blocking"
      ],
      "completed": true
    },
    {
      "id": 57,
      "name": "Smarter Hugging Face downloads: minimal fp16/bf16 safetensors + component allowlisting",
      "description": "Today, naive `huggingface_hub.snapshot_download()` pulls the entire repo, which is often 10s of GB because repos include legacy `.ckpt`, full fp32, `.bin`, safety-checker, feature extractor, training configs, etc.\n\nGoal: by default, download ONLY what is needed to run inference for diffusers pipelines, and prefer reduced-precision safetensors.\n\nDefaults:\n- Only fp16/bf16 weights (prefer fp16 unless pipeline requires bf16).\n- Only safetensors weights (never `.ckpt`; never `.bin` unless explicitly requested).\n- Only required pipeline components (e.g. `unet/`, `vae/`, `text_encoder/`, `tokenizer/`, `scheduler/`) derived from `model_index.json` (or a Cozy Hub manifest equivalent).\n\nThis should be used both by:\n- `hf:` model refs in the worker (tenant-side downloads)\n- the Cozy Hub seeding/mirroring tooling that clones HF repos into Cozy snapshots/objects.\n\nNon-goals (for now): civitai/url downloads (tracked in issue id=56).",
      "tasks": [
        "[x] Define HF download policy + override knobs\n    - Env/config: allowed components (default from model_index.json), allowed precisions (`fp16,bf16`), and strict safetensors-only behavior\n    - Escape hatches: `COZY_HF_FULL_REPO_DOWNLOAD=1`, optional root json, optional component inclusion",
        "[x] Derive required components from `model_index.json`\n    - Skip `safety_checker` and `feature_extractor` by default\n    - Allow override to include them (`COZY_HF_INCLUDE_OPTIONAL_COMPONENTS=1`) or hard override component list (`COZY_HF_COMPONENTS=...`)",
        "[x] Implement selective HF downloads with `snapshot_download(allow_patterns=...)`\n    - Download `model_index.json` via `hf_hub_download` first\n    - Validate available files via `HfApi.list_repo_files()` before downloading\n    - Allowlist component configs + reduced-precision safetensors weights; allow full small trees for tokenizer/scheduler\n    - Avoid repo-root `.ckpt` / `.bin` because they aren’t in the allowlist",
        "[x] Add a failure strategy when fp16/bf16 safetensors are missing\n    - Hard fail early with a clear error\n    - Suggest override `COZY_HF_WEIGHT_PRECISIONS=fp16,bf16,fp32` or `COZY_HF_FULL_REPO_DOWNLOAD=1`",
        "[x] Add tests\n    - Mock `huggingface_hub` (no network)\n    - Assert optional components are excluded by default\n    - Assert error when only fp32 safetensors exist",
        "[x] Document the behavior\n    - Added README docs for `hf:` refs: defaults + override env vars"
      ],
      "completed": true
    },
    {
      "id": 58,
      "name": "HF downloader v2: diffusers-aware minimal fetch + variants + mirror parity",
      "description": "Issue #57 adds a safe default allowlist (fp16/bf16 safetensors + required components) for `hf:` refs. This follow-up keeps parity between:\n- the worker's direct `hf:` downloads (Python)\n- the HF→CozyHub mirroring tool (Go)\n\nCozy-hub stays pure Go and does not call any Python. That means we will implement the same selection behavior twice (Python in python-gen-worker; Go in the mirror tool) and keep them aligned with shared golden fixtures.\n\nGoals:\n- Avoid downloading unnecessary repo-root weights (`*.ckpt`, `*.bin`) and fp32 weights by default.\n- Correctly handle diffusers variant conventions (`variant=\"fp16\"`) and repos that use sharded weights (`*.index.json` + shards).\n- Keep behavior deterministic and explicit (no silent fallback to ckpt/bin).\n- Ensure worker and mirror select the same files for the same repo/policy.\n\nNon-goals:\n- Civitai/url support (tracked separately).\n- Training-time assets (optimizer states, training configs, etc.).",
      "tasks": [
        "[x] Define a shared selection spec (golden fixtures)\n    - Fixture inputs: `model_index.json` + `repo_files[]` + policy knobs\n    - Fixture outputs: exact selected file set (and allow_patterns for Python)\n    - Used to keep Python (worker) and Go (mirror tool) in sync",
        "[x] Diffusers-aware selection from `model_index.json`\n    - Prefer reading `model_index.json` first via `hf_hub_download`\n    - Derive component names (ignore `_` keys)\n    - Maintain default skip list: `safety_checker`, `feature_extractor`\n    - Provide overrides:\n      - `COZY_HF_COMPONENTS=...` (hard override)\n      - `COZY_HF_INCLUDE_OPTIONAL_COMPONENTS=1`",
        "[x] Variant + weight format policy (hardcoded preference order)\n    - Prefer safetensors by default (never silent fallback to ckpt/bin)\n    - Hardcode preferred variant order: `bf16` → `fp8` → `fp16` → default\n    - Keep existing escape hatches, but do not add new env/config knobs",
        "[x] Sharded safetensors handling\n    - If a component has `*.safetensors.index.json`, ensure the index + all referenced shards are included in allow_patterns\n    - Validate expected files using `HfApi.list_repo_files()` before downloading",
        "[x] Worker implementation (Python)\n    - Expand `gen_worker.hf_downloader` selection to cover variants + sharded weights\n    - Keep using `snapshot_download(allow_patterns=...)`\n    - Add unit tests (no network)",
        "[x] Local completeness check (no network)\n    - If the repo snapshot is already present in the HF cache, validate required files locally\n    - If complete, skip HF API calls + skip snapshot_download\n    - If partial, fall back to normal download path\n    - Treat `*.incomplete` (HF cache) as incomplete",
        "[x] Fallback for repos without `model_index.json`\n    - If `model_index.json` is missing, infer a diffusers-like component set from repo structure (known component dirs)\n    - Prefer sharded safetensors when available (index + shards)\n    - Stay strict on weight formats (no silent `.bin`/`.ckpt` fallback)",
        "[x] Generalize component discovery (no hardcoded names)\n    - Derive component folder names from `model_index.json` keys (ignore `_` keys)\n    - Remove reliance on a fixed allowlist like `unet/text_encoder/vae` for planning\n    - Keep default skip list: `safety_checker`, `feature_extractor`",
        "[x] Generalize “small tree” components using model_index types\n    - Detect tokenizer-like components via `model_index.json` entries (library==transformers and class contains Tokenizer)\n    - Detect scheduler-like components via `model_index.json` entries (library==diffusers and class contains Scheduler)\n    - For these, include the entire folder (still excluding `.bin/.ckpt`)",
        "[x] Robust safetensors precision selection via header inspection\n    - For candidate weight files, fetch the safetensors header via HTTP Range and parse tensor dtypes\n    - Prefer float dtypes in order: fp16 then bf16 (no silent fp32 unless explicitly allowed)\n    - Use this to choose between `*.fp16.safetensors` and non-variant `*.safetensors` when both exist",
        "[x] Robust sharded safetensors selection per component\n    - If multiple `*.safetensors.index.json` exist in a component folder, choose the best candidate by probed dtype\n    - Expand selected index → shards via `weight_map` and include those files only\n    - Avoid accidentally selecting tiny LoRA-like safetensors by using a size-based tie-breaker (prefer the larger candidate when dtype matches)",
        "[x] Add fixtures/tests for non-standard component names\n    - Add unit tests using a `model_index.json` like Z-Image (e.g. `transformer` instead of `unet`)\n    - Assert selection still finds the correct folders and only required files are downloaded",
        "[x] Repo-size / safety checks\n    - Add a conservative, non-configurable safety limit (or remove this task if not desired)\n    - Avoid adding new env/config knobs",
        "[x] End-to-end validation\n    - E2E should cover both:\n      - worker HF-direct (`hf:...`)\n      - mirror HF→CozyHub then worker CozyHub download\n    - Verify both paths downloaded only the minimal file set",
        "[x] Docs\n    - Document defaults and overrides (env vars) clearly\n    - Provide an SD1.5 example showing that only `unet/vae/text_encoder/tokenizer/scheduler` fp16 safetensors are fetched"
      ],
      "completed": true
    },
    {
      "id": 59,
      "name": "Resumable Cozy snapshot/object downloads (Range + .part)",
      "description": "Improve reliability and performance of Cozy snapshot/object downloads by supporting resume for interrupted downloads.\n\nHugging Face (`hf:`) downloads already rely on `huggingface_hub` caching/locking/resume; we should not invent our own `.incomplete` scheme there.\n\nFor Cozy snapshot/object files, python-gen-worker currently downloads each file into `<dst>.part` and renames atomically on success, but it deletes any existing `.part` and restarts from 0. We want to resume from partial `.part` downloads using HTTP Range requests when the file URL supports it (typically presigned object-store URLs).\n\nApproach:\n- Keep atomic finalization: write to `.part`, verify size + blake3, then rename.\n- If `.part` exists, resume:\n  - read current size\n  - issue GET with `Range: bytes=<offset>-`\n  - append to `.part`\n  - verify expected size + blake3\n\nNote: Cozy Hub snapshot/object manifests return presigned GET URLs to the object store. Range support depends on the underlying object store (S3/MinIO typically support Range).",
      "tasks": [
        "[x] Confirm Range support for file URLs returned by Cozy Hub manifests\n    - Snapshot/object manifest URLs are presigned object-store GET URLs\n    - Range support depends on the object store (S3/MinIO typically support Range)",
        "[x] Implement resumable file download in `src/gen_worker/cozy_cas.py`\n    - Preserve existing `.part` files\n    - Resume with Range + append when possible\n    - Fall back to full re-download if server does not support Range",
        "[x] Add tests for resume behavior\n    - Simulate an interrupted download via a local aiohttp server with Range support\n    - Verify `.part` resume and final hash/size validation",
        "[x] Document behavior\n    - Clarify HF resumes via huggingface_hub\n    - Clarify Cozy snapshot downloads use `.part` + Range resume when supported"
      ],
      "completed": true
    },
    {
      "id": 60,
      "name": "Dev tool: mock orchestrator gRPC client for local worker testing",
      "description": "Add a local-only dev tool that can submit tasks to a running python-gen-worker over the same gRPC interface used by gen-orchestrator. This enables end-to-end testing of: gRPC envelope + auth headers + function discovery + schema + model injection + asset materialization + outputs, without standing up the full gen-orchestrator.\n\nDesign goals:\n- Use the existing generated protobuf stubs in `src/gen_worker/pb/` (no new proto definitions).\n- Be easy to run against a locally started worker container.\n- Support a minimal \"run one function\" mode and a more realistic streaming mode.\n- Keep it dev-only (lives under `scripts/` and/or `gen_worker/testing/`).",
      "tasks": [
        "[x] Decide packaging/entrypoint\n    - Implemented as a Python CLI module: `python -m gen_worker.testing.mock_orchestrator ...`\n    - Avoids adding an HTTP surface to the worker",
        "[x] Implement connection + auth\n    - Runs a gRPC server that the worker connects to (bidirectional stream)\n    - Captures and prints gRPC metadata (including auth header if present)",
        "[x] Implement the minimal gRPC protocol flow\n    - Accepts the worker ConnectWorker stream\n    - Sends `TaskExecutionRequest` and receives `TaskExecutionResult` and `WorkerEvent` messages",
        "[x] Function selection + payload encoding\n    - Reads function name + JSON payload from CLI args\n    - Encodes payload as msgpack (worker expects msgspec msgpack payloads)",
        "[x] Output handling\n    - Decodes msgpack output payload to JSON and prints it\n    - Prints streamed worker events when enabled",
        "[x] Provide runnable examples\n    - Added README section with commands for running against a worker container",
        "[x] Add a basic integration test\n    - Added a dev-only pytest that spawns a worker process and submits one task (skipped by default)"
      ],
      "completed": true
    },
    {
      "id": 61,
      "name": "Cozy Hub v2 model flow: resolve_artifact + snapshots/blobs cache + cozy.pipeline.lock.yaml",
      "description": "Implement the full Cozy Hub model flow in python-gen-worker so tenant functions only declare dependencies and do inference.\n\nWorker responsibilities:\n- Resolve `cozy:` model refs via Cozy Hub `resolve_artifact`.\n- Download the returned snapshot manifest (single manifest listing all files).\n- Cache content-addressed blobs locally (HF-style blobs store).\n- Materialize a snapshot checkout that looks like a normal diffusers repo.\n- Prefer `cozy.pipeline.lock.yaml` when present; fall back to `cozy.pipeline.yaml`.\n\nThis must be implemented in worker/library code, not in tenant functions.",
      "tasks": [
        "[x] Implement Cozy Hub `resolve_artifact` client\n    - Request: tag + ordered preferences + worker capabilities\n    - Response: `repo_revision_seq` number, `artifact.snapshot_digest`, `snapshot_manifest.files[]` (paths + blake3 + urls)\n    - Handle 409 'no compatible artifact' errors with a clear message",
        "[x] Define worker-side selection policy (hardcoded)\n    - Prefer file_type: flashpack → safetensors\n    - Prefer quantization: fp8 → bf16 (fp16 fallback)\n    - Prefer file_layout: diffusers\n    - Include capability detection (cuda version, gpu sm, installed libs) in requests",
        "[x] Implement local cache layout: snapshots + blobs\n    - Cache root: `${WORKER_MODEL_CACHE_DIR}/cozy/`\n    - Blobs: `${WORKER_MODEL_CACHE_DIR}/cozy/blobs/blake3/<aa>/<bb>/<digest>`\n    - Snapshots: `${WORKER_MODEL_CACHE_DIR}/cozy/snapshots/<snapshot_digest>/...`",
        "[x] Download blobs with verification + resume\n    - Download to `<digest>.part`, Range resume when supported, then atomic rename\n    - Verify `size_bytes` and BLAKE3 digest\n    - Singleflight: concurrent downloads for the same digest share one in-flight transfer",
        "[x] Materialize snapshot from snapshot manifest\n    - Create the exact file tree under `snapshots/<snapshot_digest>/` using `files[].path`\n    - Materialize each file from the blob store (prefer hardlink; fallback symlink/copy)\n    - Atomic activation: build in `.building` then rename",
        "[ ] Lockfile-aware pipeline loading (future)\n    - Prefer `cozy.pipeline.lock.yaml` when present\n    - Fall back to `cozy.pipeline.yaml` when lockfile missing",
        "[x] Integrate with the existing injection/model loading flow\n    - Model refs in `pyproject.toml` resolve to a local Cozy snapshot root directory\n    - Worker diffusers injection calls `from_pretrained(<snapshot_root>)`",
        "[x] Tests\n    - Mock Cozy Hub resolve_artifact + manifest responses (no network)\n    - Validate blobs cache, snapshot materialization, and concurrent download singleflight behavior",
        "[x] E2E Docker smoke test (manual)\n    - Start Cozy Hub + MinIO\n    - Run `sd15-worker` + `mock_orchestrator` and verify it downloads and generates `runs/<run_id>/outputs/image.png`"
      ],
      "completed": false
    }
  ]
}
