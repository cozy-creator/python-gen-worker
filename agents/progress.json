{
  "issues": [
    {
      "name": "Untrusted tenant code: per-run file capability tokens (no static FILE_SERVICE_TOKEN in pods)",
      "description": "Assume tenant functions can read any env var/file in the container. Remove reliance on long-lived file service tokens inside worker pods. Instead, accept a short-lived, run-scoped file capability JWT from gen-orchestrator for each task/session and use it for all cozy-hub file store reads/writes (inputs/outputs).",
      "tasks": [
        "[ ] Extend worker task envelope to include `file_token` (JWT) + `file_base_url` (cozy-hub file API base) and thread it into ActionContext / CozyHubFileClient.",
        "[ ] Update CozyHubDownloader/ctx.save_* helpers to use the per-run `file_token` (Authorization: Bearer ...) and avoid using env FILE_API_TOKEN for production flows.",
        "[ ] Ensure the token is tenant-scoped and prefix-scoped (runs/<run_id>/...) and expires quickly; treat auth failures as non-retriable unless token is refreshable.",
        "[ ] Add tests: task-scoped file token is required when running in \"untrusted\" mode; uploads/downloads succeed for allowed prefixes and fail for disallowed paths."
      ],
      "completed": false
    },
    {
      "name": "LRU model cache with live VRAM reporting to gen-orchestrator",
      "description": "Implement an LRU model cache in python-worker that tracks loaded models in VRAM and reports availability to gen-orchestrator in real-time via heartbeats. This enables model-aware routing where the orchestrator can route requests to workers that already have the required model loaded.\n\nStudied torch_manager LRUCache (manager.py:126-177). Key patterns:\n- Uses OrderedDict with timestamps for efficient LRU tracking\n- Separate gpu_cache and cpu_cache for different memory locations\n- access() moves item to end (most recently used)\n- get_lru_models() returns list ordered by least recently used first\n- Constants: VRAM_SAFETY_MARGIN_GB = 3.5, DEFAULT_MAX_VRAM_BUFFER_GB = 2.0, RAM_SAFETY_MARGIN_GB = 10.0",
      "tasks": [
        "[ ] Implement ModelCache class with LRU eviction (from torch_manager LRUCache pattern)",
        "[ ] Implement VRAM tracking with weights vs working memory",
        "[ ] Implement get_stats() for heartbeat reporting",
        "[ ] Update worker heartbeat to include model cache stats",
        "[ ] Integrate ModelCache with model injection",
        "[ ] Support orchestrator-commanded model operations (LoadModelCommand, UnloadModelCommand)",
        "[ ] Add configurable settings via environment variables",
        "[ ] Add tests for LRU eviction, safety margins, heartbeat stats"
      ],
      "completed": false
    },
    {
      "name": "Diffusers pipeline construction parity with torch_manager",
      "description": "Implement proper diffusers pipeline construction in the new python-worker to achieve feature parity with the legacy torch_manager. The worker must be able to load models from Cozy Hub manifests, construct pipelines with component sharing, apply optimizations, and handle various model formats.\n\nStudied torch_manager implementation (manager.py, flashpack_loader.py). Key patterns to carry forward:\n- MODEL_COMPONENTS dict defines components per pipeline type (vae, unet/transformer, text_encoder, scheduler, tokenizer)\n- VRAM_SAFETY_MARGIN_GB = 3.5, DEFAULT_MAX_VRAM_BUFFER_GB = 2.0\n- FlashPackLoader loads from pipeline/ subdirectory with config files\n- Optimizations are CONDITIONAL: only applied when model_size > max_vram\n- LocalModelCache copies from NFS to local NVMe for faster loading (prioritizes flashpack)",
      "tasks": [
        "[ ] Pipeline loading with format priority (flashpack first, then safetensors, then single file)",
        "[ ] Component reference resolution (_cozy_ref in model_index.json)",
        "[ ] dtype selection (bfloat16 for Flux, float16 for others)",
        "[ ] Device placement with VAE tiling/slicing ALWAYS enabled during _move_to_gpu",
        "[ ] Scheduler configuration (dynamic import from diffusers)",
        "[ ] Pipeline optimizations (CONDITIONAL - only when model_size > max_vram)",
        "[ ] Warm-up inference (4 steps, output_type='pil', flushes memory after)",
        "[ ] Custom pipeline classes support",
        "[ ] Memory management (flush_memory, unload with remove_all_hooks and explicit component deletion, _free_space_for_model LRU eviction)",
        "[ ] Model download from Cozy Hub",
        "[ ] Startup model initialization with randomization",
        "[ ] Error handling (missing files, incompatible formats, CUDA OOM)",
        "[ ] LocalModelCache for NFS->NVMe optimization with background prefetch"
      ],
      "completed": false
    },
    {
      "id": 38,
      "name": "Simplify GPU selection with gpu=true flag",
      "description": "Replace the cuda constraint as the primary GPU selection mechanism with a simpler gpu=true/false flag. The cuda constraint becomes an optional advanced override. This aligns with how other platforms (Modal, Replicate, Fal) handle GPU selection.",
      "tasks": [
        "[ ] Update gen-builder config.go to parse new gpu field:\n    - gpu = true -> build GPU variant (pick latest stable CUDA)\n    - gpu = false or omitted -> build CPU variant\n    - cuda = \">=12.6\" -> optional override for specific CUDA version",
        "[ ] Update FilterVariants in runtime_catalog.go:\n    - Check gpu flag first\n    - If gpu=true and no cuda constraint, use default CUDA version\n    - If gpu=false, return CPU variants only",
        "[ ] Update gen-builder README with new gpu flag documentation",
        "[ ] Update python-worker examples to use gpu=true instead of cuda constraint",
        "[ ] Add validation: warn if cuda is set but gpu is false (conflicting config)"
      ],
      "completed": false
    },
    {
      "id": 39,
      "name": "Add resource requirements to deployment config",
      "description": "Allow tenants to specify hardware requirements (VRAM, GPU type, memory, CPU) in pyproject.toml. These are used by the orchestrator/scheduler to match deployments to appropriate workers.",
      "tasks": [
        "[ ] Define [tool.cozy.resources] schema:\n    - vram_gb: int (minimum GPU VRAM required)\n    - gpu_type: string (\"any\", \"A100\", \"H100\", \"A10G\", \"T4\", etc.)\n    - memory_gb: int (system RAM)\n    - cpu_cores: int",
        "[ ] Update gen-builder config.go to parse resources section",
        "[ ] Include resources in build manifest output",
        "[ ] Update gen-orchestrator to read resources from deployment registration",
        "[ ] Update scheduler to filter workers by resource requirements",
        "[ ] Document resource requirements in gen-builder README",
        "[ ] Add examples showing resource configuration"
      ],
      "completed": false
    },
    {
      "id": 40,
      "name": "Simplify model specification in deployment config",
      "description": "Models that a worker needs should be declared in [tool.cozy.models] in pyproject.toml. Model IDs are Cozy Hub identifiers (not raw HuggingFace/Civitai URLs). Workers download models via Cozy Hub API, which handles the actual source (HuggingFace, S3, CDN, etc.). Orchestrator routes using deployment-local keys only. Dynamic/small files like LoRAs come as Asset in request payloads.",
      "tasks": [
        "[ ] Define [tool.cozy.models] schema in pyproject.toml:\n    ```toml\n    [tool.cozy.models]\n    sd-xl = \"stabilityai/stable-diffusion-xl-base-1.0\"  # Cozy Hub model ID\n    controlnet = \"lllyasviel/control_v11p_sd15_canny\"   # Cozy Hub model ID\n    ```\n    - Keys (sd-xl, controlnet) are deployment-local, used for routing\n    - Values are Cozy Hub model IDs (unique identifiers in Cozy's model registry)\n    - Cozy Hub resolves IDs to actual storage location (HuggingFace, S3, CDN, etc.)",
        "[ ] Update gen-builder config.go to parse [tool.cozy.models] section",
        "[ ] Include models in build manifest (list of model IDs to pre-download)",
        "[ ] Update worker startup to pre-download all models via Cozy Hub API",
        "[ ] Update ModelRef to resolve keys from [tool.cozy.models]",
        "[ ] Deprecate/remove runtime.model_allowlist (replaced by [tool.cozy.models])",
        "[ ] Document that LoRAs and other dynamic files should use Asset type in payload",
        "[ ] Update python-worker examples to use new [tool.cozy.models] config",
        "[ ] Add size limits for Asset downloads (prevent abuse with huge LoRA files)"
      ],
      "completed": false
    },
    {
      "id": 41,
      "name": "Create z-image LoRA example showing dynamic LoRA loading",
      "description": "Create an example python worker in ~/cozy/python-worker/examples that demonstrates loading custom LoRAs dynamically at runtime. LoRAs are passed as Asset in the request payload (like input images), downloaded by the worker, and loaded into the pipeline. Based on fal.ai's z-image/turbo/lora pattern.",
      "tasks": [
        "[ ] Create ~/cozy/python-worker/examples/z-image-lora/ directory structure",
        "[ ] Define input schema with LoRA as Asset (LoraSpec struct with file, weight, adapter_name)",
        "[ ] Implement generate function with dynamic LoRA loading:\n    - Load base model via ModelRef(Src.DEPLOYMENT, \"sdxl\")\n    - For each LoRA: load_lora_weights(), set_adapters()\n    - Unload LoRAs with unload_lora_weights() for next request",
        "[ ] Add pyproject.toml with gpu=true and [tool.cozy.models]",
        "[ ] Add example request payloads in README",
        "[ ] Test that LoRA Assets are properly materialized before function runs",
        "[ ] Document the pattern: LoRAs as Assets, not as model config"
      ],
      "completed": false
    },
    {
      "id": 42,
      "name": "Improve manifest to include top-level models list and per-function model requirements",
      "description": "The manifest generated by gen_worker.discover should have a clear top-level list of all required models, plus per-function model requirements. This makes it easy for workers to know what to pre-download, and for schedulers to understand deployment requirements.",
      "tasks": [
        "[ ] Add top-level 'models' field to manifest output (keys are deployment-local, values are Cozy Hub IDs)",
        "[ ] Extract models from all function injection_json and deduplicate",
        "[ ] Add 'required_models' field to each function in manifest",
        "[ ] Only include models with source='deployment' (not 'payload' which are dynamic)",
        "[ ] Update gen-builder to read top-level models list from manifest",
        "[ ] Update worker startup to use manifest models list for pre-download",
        "[ ] Validate manifest models against [tool.cozy.models] config if present",
        "[ ] Update manifest JSON schema documentation",
        "[ ] Add tests for models extraction in discover.py"
      ],
      "completed": false
    },
    {
      "id": 43,
      "name": "Create example showing payload-based model selection with multiple fine-tunes",
      "description": "Create an example python worker demonstrating how to support multiple model fine-tunes (e.g., 10 SDXL variants) efficiently. The request payload specifies which model to use via a key, and the scheduler routes to workers that already have that model loaded in VRAM.",
      "tasks": [
        "[ ] Create ~/cozy/python-worker/examples/multi-checkpoint/ directory",
        "[ ] Define multiple models in pyproject.toml [tool.cozy.models]",
        "[ ] Implement function with ModelRef(Src.PAYLOAD, \"model_key\")",
        "[ ] Document how scheduler routing works with deployment keys",
        "[ ] Add example requests showing different model_key values",
        "[ ] Test that model switching works correctly",
        "[ ] Document VRAM considerations (LRU eviction, cold start latency)",
        "[ ] Clarify model specification rules in docs"
      ],
      "completed": false
    },
    {
      "id": 44,
      "name": "Progressive model availability and disk vs VRAM tracking",
      "description": "Refine model-aware routing to support progressive model availability (worker can start serving inference as soon as first model downloads, while continuing to download others) and distinguish between disk-cached vs VRAM-loaded models.",
      "tasks": [
        "[ ] Add cached_models and downloading_models fields to WorkerResources proto",
        "[ ] Update python-worker heartbeat to report all three model states (available, cached, downloading)",
        "[ ] Update gen-orchestrator scheduler routing logic for three-tier preference",
        "[ ] Implement progressive startup in python-worker (accept jobs as models become available)",
        "[ ] Add model download progress tracking in heartbeat metadata",
        "[ ] Update scheduler to avoid sending job for model still downloading",
        "[ ] Handle multi-model functions correctly (check all required_models)",
        "[ ] Add config for download concurrency limit (WORKER_MAX_CONCURRENT_DOWNLOADS)",
        "[ ] Document progressive availability behavior in python-worker README",
        "[ ] Add tests for progressive model availability"
      ],
      "completed": false
    },
    {
      "id": 45,
      "name": "Clean up python-worker: remove torch_manager, fix dependencies, implement new model loader",
      "description": "The python-worker codebase has accumulated legacy code (torch_manager) that should be removed. The new architecture: worker downloads models from Cozy Hub, constructs pipelines using diffusers/transformers, and injects ready-to-use pipelines into tenant functions. Support both safetensors and flashpack formats.",
      "tasks": [
        "[ ] Remove legacy torch_manager module (delete src/gen_worker/torch_manager/ and default_model_manager/)",
        "[ ] Clean up pyproject.toml (safetensors/flashpack in core, torch as optional, remove onnxruntime/tensorrt)",
        "[ ] Implement new model_loader.py module with flashpack-first loading",
        "[ ] Implement ModelCache with LRU eviction and VRAM tracking",
        "[ ] Update _resolve_injected_value in worker.py to use new ModelLoader",
        "[ ] Update worker startup to pre-download models from manifest",
        "[ ] Update base image Dockerfiles (python:3.12-slim + torch + gen-worker core ONLY)",
        "[ ] Update base-images.json to reflect new architecture",
        "[ ] Remove private devpi references from Dockerfiles",
        "[ ] Test clean import without optional deps",
        "[ ] Update __init__.py exports",
        "[ ] Update entrypoint.py to use new ModelLoader",
        "[ ] Update examples to use new model loading pattern",
        "[ ] Document the new architecture in README"
      ],
      "completed": false
    },
    {
      "id": 46,
      "name": "Switch base images from nvidia/cuda to python:3.12-slim",
      "description": "Replace the large nvidia/cuda base images with minimal python:3.12-slim. PyTorch CUDA wheels bundle their own CUDA runtime, so the nvidia base is unnecessary. This reduces image size by ~1-2GB and simplifies the build.",
      "tasks": [
        "[x] Update runtime/Dockerfile.base (GPU) to use python:3.12-slim + PyTorch wheels",
        "[x] Update runtime/Dockerfile.base.cpu",
        "[x] Update runtime/base-images.json",
        "[x] Remove deadsnakes PPA setup",
        "[x] Remove clang (gcc from build-essential is sufficient)",
        "[x] Test image builds and verify smaller sizes",
        "[x] Test PyTorch CUDA functionality (torch.cuda.is_available())",
        "[x] Update build-local-base-images.sh script",
        "[x] Document the change in README"
      ],
      "completed": true
    },
    {
      "id": 47,
      "name": "Update python-worker examples to use correct dependency strategy",
      "description": "Review and update all examples in ~/cozy/python-worker/examples to ensure they follow the new dependency strategy: gen-worker provides core SDK + safetensors + flashpack; torch is a peer dependency (provided by base image); tenant's pyproject.toml adds diffusers/transformers/accelerate as needed.",
      "tasks": [
        "[ ] Audit all example pyproject.toml files in ~/cozy/python-worker/examples/",
        "[ ] Ensure examples do NOT list safetensors or flashpack (gen-worker provides these)",
        "[ ] Ensure examples do NOT list torch (peer dependency from base image)",
        "[ ] Ensure examples that use diffusers pipelines list diffusers/transformers/accelerate",
        "[ ] Verify gen-worker dependency version is consistent across all examples",
        "[ ] Remove any unnecessary dependencies from examples",
        "[ ] Test that examples still build and run with the updated dependencies",
        "[ ] Update example READMEs if they mention dependency installation"
      ],
      "completed": false
    }
  ]
}
