{
  "issues": [
    {
      "name": "Signature-based dependency injection (typing.Annotated + ModelRef)",
      "description": "Adopt the new tenant function contract where models/processors/pipelines are injected via `typing.Annotated[..., ModelRef(...)]` instead of decorator flags like expects_pipeline_arg. The worker owns downloads + VRAM lifecycle; tenant code receives ready-to-use objects. Keep `@worker_function` minimal (marker + requires_gpu) and rely on signature inspection for injection and validation.",
      "tasks": [
        "[x] Add `gen_worker.injection` module defining ModelRefSource enum, ModelRef annotation helper",
        "[x] Define injection resolution rules (DEPLOYMENT, PAYLOAD sources)",
        "[x] Implement injection in the worker call wrapper",
        "[x] Implement a minimal injection catalog (torch backend first)",
        "[x] Tighten `@worker_function` usage (marker + requires_gpu only)",
        "[x] Schema reporting: include injection metadata per function",
        "[x] Tests for Annotated injection and allowlist rejection"
      ],
      "completed": true
    },
    {
      "name": "Strict typed tenant contract (msgspec input/output inferred from signature)",
      "description": "Enforce that tenant functions use strongly-typed `msgspec.Struct` for inputs and outputs, inferred from the function signature. This makes function invocation safer and enables orchestrator-side JSON validation against the exact schema the function expects.",
      "tasks": [
        "[x] Define canonical function signature rules and enforce at discovery time",
        "[x] Implement signature inspection utilities",
        "[x] Update function discovery to fail fast with clear errors",
        "[x] Update invocation wrapper to decode payload strictly as inferred msgspec.Struct",
        "[x] Update output handling for non-incremental and incremental functions",
        "[x] Enforce that dict-based payload handlers are not supported",
        "[x] Tests for contract enforcement"
      ],
      "completed": true
    },
    {
      "name": "Incremental output: generator functions + typed output events",
      "description": "Add first-class support for incremental output from tenant functions. The worker supports functions that yield typed deltas and forwards those incrementals to the scheduler as ordered job events.",
      "tasks": [
        "[x] Infer incremental output from return annotation (Iterator/Iterable)",
        "[x] Extend function discovery to support incremental output mode",
        "[x] Define minimal stable event taxonomy (output.delta, output.completed, output.error)",
        "[x] Implement iterator execution path with rate limiting and cancellation",
        "[x] Ensure emitted deltas are JSON-serializable with size limits",
        "[x] Preserve existing RunResult semantics",
        "[x] Extend schema reporting for incremental output metadata",
        "[x] Unit tests for iterator yields, cancellation, oversize handling",
        "[x] Add example incremental-output function"
      ],
      "completed": true
    },
    {
      "name": "WebSocket realtime functions (worker-side runtime)",
      "description": "Support true WebSocket realtime endpoints (bidirectional) without exposing workers publicly. Tenant code registers an async handler and uses a gen-worker-owned socket interface.",
      "tasks": [
        "[x] Define worker-owned RealtimeSocket interface",
        "[x] Add @worker_websocket decorator and discovery",
        "[x] Add internal realtime session loop over worker transport",
        "[x] Injection support for realtime handlers",
        "[x] Cancellation + cleanup handling",
        "[x] Safety limits (max frame bytes)",
        "[x] Integration tests for echo handler"
      ],
      "completed": true
    },
    {
      "name": "Alternative backends: ONNX Runtime + TensorRT runtime handles",
      "description": "Add first-class support for non-torch execution backends by injecting backend-specific runtime handles into tenant functions.",
      "tasks": [
        "[x] Packaging extras for onnxruntime and tensorrt",
        "[x] Backend runtime handle interfaces",
        "[x] Injection support with loader hooks",
        "[x] Compatibility gating for TensorRT",
        "[x] Tests for loader-hook caching and metadata mismatch"
      ],
      "completed": true
    },
    {
      "name": "Support orchestrator queue/long-poll API (bytes + multipart inputs)",
      "description": "Ensure python-worker cleanly supports the orchestrator's simplified API surface: queue + long-poll, raw-bytes output, and multipart-uploaded inputs proxied through cozy-hub.",
      "tasks": [
        "[x] Confirm function discovery reports Asset-based file inputs",
        "[x] Ensure input materialization supports cozy-hub refs and URLs with size caps and SSRF protections",
        "[x] Ensure output contract for file outputs is Asset",
        "[x] Ensure ctx.save_bytes/ctx.save_file persist to cozy-hub and return Asset metadata",
        "[x] Unit tests for asset materialization with size caps",
        "[x] Document output_format only affects orchestrator HTTP responses"
      ],
      "completed": true
    },
    {
      "name": "Expanded structured error taxonomy + sanitization",
      "description": "Expand beyond retryable/fatal into a richer, typed error taxonomy for tenant functions with strict sanitization for client-facing messages.",
      "tasks": [
        "[x] Add tenant-facing error classes (ValidationError, ResourceError, CanceledError)",
        "[x] Define canonical error payload shape",
        "[x] Implement exception->error mapping layer",
        "[x] Implement sanitization policy for safe_message",
        "[x] Tests for error mapping and sanitization"
      ],
      "completed": true
    },
    {
      "name": "Worker leader discovery + failover",
      "description": "Add multi-seed scheduler discovery and automatic leader redirect handling for Raft deployments.",
      "tasks": [
        "[x] Add SCHEDULER_ADDRS env (comma-separated)",
        "[x] Try seed addresses on startup until connection succeeds",
        "[x] On stream failure, retry across seed list with backoff",
        "[x] Parse FailedPrecondition not_leader:<addr> and reconnect",
        "[x] Add test coverage for redirect handling"
      ],
      "completed": true
    },
    {
      "name": "Worker runtime enhancements (outputs, progress, limits)",
      "description": "Improve worker ergonomics for serverless inference: direct output uploads, progress events, and concurrency caps.",
      "tasks": [
        "[x] Add direct output upload support",
        "[x] Return output metadata instead of raw bytes",
        "[x] Deprecate raw output byte streaming for large outputs",
        "[x] Add ctx.emit() for progress/events",
        "[x] Add per-worker/per-function concurrency limits",
        "[x] Add basic validation for input/output payloads",
        "[x] Add structured error taxonomy",
        "[x] Report richer resource info (GPU, VRAM, driver version)",
        "[x] Add heartbeat backoff + jitter",
        "[x] Add graceful shutdown to drain in-flight jobs",
        "[x] Enforce configurable max payload sizes",
        "[x] Verify scheduler JWT on connect",
        "[x] Fetch and cache scheduler JWKS",
        "[x] Handle JWKS rotation",
        "[x] Fail closed when JWT verification fails"
      ],
      "completed": true
    },
    {
      "name": "Example worker functions (python-worker/examples)",
      "description": "Maintain example tenant projects inside the python-worker repo under `python-worker/examples/` so they evolve with the SDK and stay runnable by gen-builder.",
      "tasks": [
        "[x] Decide on example project layout",
        "[x] Create multiple example subfolders",
        "[x] Ensure each example imports gen_worker and uses @worker_function",
        "[x] Enforce dependency policy (no requirements.txt)",
        "[x] Add [tool.cozy] entries to examples",
        "[x] Align dependencies to python-worker SDK version",
        "[x] Add README per example"
      ],
      "completed": true
    },
    {
      "name": "Python worker SDK cleanup",
      "description": "Position python-worker as a pure pip package and keep example projects in python-worker/examples.",
      "tasks": [
        "[x] Remove or relocate python-worker/example-functions and image_gen_example",
        "[x] Update python-worker README",
        "[x] Ensure packaging stays intact",
        "[x] Add or update minimal docs",
        "[x] Document recommended contract for dynamic checkpoint selection",
        "[x] Document the build contract",
        "[x] Verify examples install gen-worker as dependency"
      ],
      "completed": true
    },
    {
      "name": "JWT RSA import safety",
      "description": "Avoid import-time failures when PyJWT lacks RSA support; ensure JWKS verification is available when configured.",
      "tasks": [
        "[x] Guard RSAAlgorithm import",
        "[x] Require PyJWT crypto extra",
        "[x] Reinstall and smoke test example imports"
      ],
      "completed": true
    },
    {
      "name": "gen-worker core + torch add-on split",
      "description": "Separate lightweight orchestration SDK from torch-specific model memory management, with a clean interface between them.",
      "tasks": [
        "[x] Define core responsibilities",
        "[x] Keep model downloading from Cozy hub in core",
        "[x] Core must integrate downloader -> local model path -> ModelManager calls",
        "[x] Define ModelManager interface in core with no torch imports",
        "[x] Move default_model_manager behind optional add-on",
        "[x] Remove torch/diffusers/transformers from core dependencies",
        "[x] Add optional dependency group for torch runtime",
        "[x] Update worker to accept ModelManager instance or plugin path",
        "[x] Document the split and install modes",
        "[x] Audit imports to ensure torch-only code is isolated"
      ],
      "completed": true
    },
    {
      "name": "Standardize release config in pyproject.toml ([tool.cozy])",
      "description": "Remove the separate cozy.toml manifest and standardize all tenant release configuration under `pyproject.toml` in `[tool.cozy]`.",
      "tasks": [
        "[x] Update docs to use [tool.cozy] only",
        "[x] Update python-worker/examples to use [tool.cozy]",
        "[x] Update worker-side config parsing",
        "[x] Add validation helper for [tool.cozy], uv.lock presence",
        "[x] Tests for cozy.toml rejection"
      ],
      "completed": true
    },
    {
      "id": 50,
      "name": "Untrusted tenant code: per-run file capability tokens (no static FILE_SERVICE_TOKEN in pods)",
      "description": "Assume tenant functions can read any env var/file in the container. Remove reliance on long-lived file service tokens inside worker pods. Instead, accept a short-lived, run-scoped file capability JWT from gen-orchestrator for each task/session and use it for all cozy-hub file store reads/writes (inputs/outputs).\n\nImplementation:\n- TaskExecutionRequest protobuf already has file_base_url and file_token fields\n- ActionContext accepts and uses per-run tokens via file_api_base_url and file_api_token params\n- ctx.save_bytes/save_file use per-run token when available, falling back to env var\n- _materialize_asset uses per-run token for Cozy Hub file refs\n- Added AuthError exception for 401/403 responses (non-retryable)\n- Added tests in tests/test_file_token_scoping.py",
      "tasks": [
        "[x] Extend worker task envelope to include `file_token` (JWT) + `file_base_url` (cozy-hub file API base) and thread it into ActionContext\n    - TaskExecutionRequest protobuf already has file_base_url (field 8) and file_token (field 9)\n    - ActionContext.__init__ accepts file_api_base_url and file_api_token\n    - Worker._handle_task_request passes these from request to ActionContext",
        "[x] Update ctx.save_* helpers to use the per-run `file_token`\n    - ActionContext._get_file_api_token() prefers instance token over env var\n    - save_bytes, save_bytes_create, save_file, save_file_create all use per-run token",
        "[x] Update _materialize_asset to use per-run token for Cozy Hub refs\n    - HEAD and GET requests use ctx._get_file_api_token()",
        "[x] Ensure auth failures are non-retriable\n    - Added AuthError exception class in errors.py\n    - save_bytes/save_bytes_create raise AuthError on 401/403\n    - _materialize_asset raises AuthError on 401/403\n    - _map_exception maps AuthError to 'auth' type with retryable=False",
        "[x] Add tests for task-scoped file tokens\n    - tests/test_file_token_scoping.py with 9 tests\n    - Tests per-run token usage, fallback to env var, AuthError on 401/403",
        "[x] Add file_token/file_base_url to RealtimeOpenCommand\n    - Updated proto in gen-orchestrator (worker_scheduler.proto lines 131-132)\n    - Regenerated Go and Python proto files\n    - Updated _handle_realtime_open_cmd in worker.py to extract and pass to ActionContext"
      ],
      "completed": true
    },
    {
      "id": 65,
      "name": "Worker Bootstrap Contract v2: JWT-claims identity + minimal env injection",
      "description": "gen-orchestrator now treats worker containers as untrusted and no longer injects identity or secrets via env. Worker identity is carried in the scheduler-issued WORKER_JWT claims, and the scheduler treats those claims as authoritative.\n\nStartup contract (pod env injected by orchestrator):\n- Required: SCHEDULER_ADDR, WORKER_JWT\n- Optional: SCHEDULER_ADDRS (seed list), HF_HOME, WORKER_MODEL_CACHE_DIR, RUNPOD_POD_ID (provider), and tenant-provided runtime env overlay (from Cozy Hub DB; sanitized denylist)\n- Forbidden: DB creds, S3/AWS creds, CIVITAI_API_KEY, Runpod secret templating, and any long-lived Cozy Hub token\n\nJWT claims contract (required on every worker connection):\n- `iss`: orchestrator issuer URL (required)\n- `aud`: includes orchestrator audience (required)\n- `sub`: worker_id\n- `release_id`: release/build id (hash of the built image)\n- `deployment`: owner/project (or equivalent stable identifier)\n- Dev-only: signature verification may be skipped when ENV!=prod and the scheduler has no keys configured, but the token + required claims are still required.\n\nHandshake contract (gRPC stream):\n- Worker connects with authorization: Bearer <WORKER_JWT>\n- WorkerRegistration is treated as untrusted; scheduler derives worker identity from JWT claims (registration worker_id/release_id may be empty)\n- Scheduler replies with ReleaseModelConfig (supported_model_ids + model_id_by_key + optional resolved_cozy_models_by_id for startup prefetch)\n\nPer-run contract:\n- TaskExecutionRequest/RealtimeOpenCommand carries owner + per-run Cozy Hub file capability (file_base_url + file_token) so the worker never needs OWNER/COZY_HUB_TOKEN env fallbacks.",
      "tasks": [
        "[x] Stop depending on RELEASE_ID/OWNER env vars inside the worker runtime\n    - Remove reading RELEASE_ID/OWNER in worker init\n    - Remove OWNER fallback in file HTTP helper that sets X-Cozy-Owner",
        "[x] Make WorkerRegistration not rely on env identity\n    - Do not require RELEASE_ID/OWNER/WORKER_ID in env\n    - Send empty worker_id/release_id in registration; rely on scheduler overriding with JWT claims",
        "[x] Support WORKER_JWT rotation\n    - Worker-connect JWTs expire every 7 days (`exp=iat+7d`)\n    - When the scheduler pushes a rotated WORKER_JWT over the gRPC stream, store it and use it for the next reconnect\n    - Ensure rotated token replaces the original WORKER_JWT in memory (do not write into env)\n    - If no rotated token is available and the original expires, the worker must restart (or be replaced) to get a fresh injected token",
        "[x] Update README bootstrap docs\n    - Document the exact env vars orchestrator injects vs what is delivered per-run\n    - Remove mentions of RELEASE_ID/OWNER/COZY_HUB_TOKEN/USE_TLS as orchestrator-provided inputs",
        "[x] Add regression tests for the new bootstrap semantics\n    - Worker can operate without RELEASE_ID/OWNER env vars\n    - X-Cozy-Owner comes only from per-run envelope (TaskExecutionRequest.owner), not env",
        "[x] Review entrypoint env surface\n    - Treat WORKER_ID and USE_TLS as local-dev-only knobs (not part of orchestrator contract)\n    - Fail fast if WORKER_JWT is missing (the scheduler requires JWTs; unauthenticated workers should not run)",
        "[x] Update docker/examples\n    - Ensure examples only require SCHEDULER_ADDR + WORKER_JWT to start\n    - Avoid examples that rely on COZY_HUB_TOKEN or other long-lived secrets"
      ],
      "completed": true
    },
    {
      "id": 66,
      "name": "Startup Model Prefetch + Incremental Readiness Reporting",
      "description": "The worker should start downloading/validating the models it needs for its release immediately after connecting, without waiting for a run.\n\nTarget behavior:\n- Scheduler sends `ReleaseModelConfig` that includes the release's required model list plus `resolved_cozy_models_by_id` (snapshot digests + presigned URLs).\n- Worker prefetches models concurrently into `WORKER_MODEL_CACHE_DIR` (ideally on `/workspace/...`), and updates readiness incrementally.\n- Worker can serve jobs that only require models already ready while other models are still downloading.\n\nNon-goals (v1):\n- Downloading arbitrary dynamic model refs without orchestrator involvement.\n",
      "tasks": [
        "[x] Consume resolved manifests in ReleaseModelConfig\n    - When `ReleaseModelConfig.resolved_cozy_models_by_id` is present, store it as the baseline resolved mapping for this worker\n    - Use it as the default resolver for Cozy model downloads (no Cozy Hub API resolve calls)",
        "[x] Background prefetch\n    - After initial registration + ReleaseModelConfig, start background downloads for required models\n    - Respect a concurrency limit to avoid saturating disk/network\n    - If a model is already present on disk (snapshot dir exists), mark it ready without downloading",
        "[x] Incremental readiness reporting\n    - As each model becomes ready on disk, ensure it appears in `WorkerResources.disk_models` on the next heartbeat\n    - Optionally emit a `WorkerEvent` like `model.ready` for faster scheduler convergence (but heartbeats alone are acceptable)",
        "[x] URL refresh integration\n    - If a presigned URL expires mid-download, request refreshed URLs from the scheduler\n    - Retry download with refreshed URLs (scheduler must re-send ReleaseModelConfig with fresh URLs)",
        "[x] Tests\n    - Worker prefetches models from ReleaseModelConfig without waiting for TaskExecutionRequest\n    - disk_models grows over time as downloads complete\n    - A job requiring a ready model can run while other models are still downloading"
      ],
      "completed": true
    },
    {
      "id": 67,
      "name": "Worker telemetry for scheduling + VRAM rebalancing (availability + model ops)",
      "description": "Gen-orchestrator needs cheap, reliable worker signals to (1) route to the most available worker and (2) rebalance models into/out of VRAM without thrashing (see gen-orchestrator issue #239).\n\nThis issue focuses on worker-side reporting and behavior only. It must not add new tenant-facing configuration or require tenants to report anything.\n\nConstraints:\n- Worker code is untrusted tenant code: no secrets or orchestrator configs are exposed.\n- Keep signals low-cardinality and stable (metrics-friendly).\n",
      "tasks": [
        "[x] Tighten `gpu_is_busy` semantics\n    - Ensure `WorkerResources.gpu_is_busy` is true while:\n      - an inference is actively using the GPU\n      - a LoadModel/UnloadModel command is running (VRAM churn)\n    - Ensure it flips back to false promptly when idle (avoid sticky-busy that blocks routing)",
        "[x] Report concurrency caps consistently\n    - Ensure `WorkerResources.max_concurrency` and `function_concurrency` are populated (when known)\n    - Scheduler uses this alongside gpu_is_busy to avoid over-assigning work to a worker",
        "[x] Expose model operation progress to the scheduler\n    - Option A (preferred): add explicit WorkerEvent messages like:\n      - `model.download.started|completed|failed`\n      - `model.load.started|completed|failed`\n      - include {model_id, snapshot_digest?, duration_ms}\n    - Option B: only via heartbeats (acceptable v1), but ensure heartbeats reflect disk_models/vram_models changes quickly",
        "[x] Defensive behavior during rebalancing\n    - If a model is currently in use for an in-flight run, reject/defers UnloadModelCommand for that model\n    - If VRAM is insufficient, fail LoadModelCommand fast with a structured error so scheduler can pick another worker/model",
        "[x] Tests\n    - gpu_is_busy toggles correctly across inference and model ops\n    - disk_models/vram_models updates are reflected in heartbeats after downloads/loads complete\n    - unload is rejected/ignored for in-use models (if we can simulate in tests)"
      ],
      "completed": true
    },
    {
      "id": 51,
      "name": "LRU model cache with live VRAM reporting to gen-orchestrator",
      "description": "Implement an LRU model cache in python-worker that tracks loaded models in VRAM and reports availability to gen-orchestrator in real-time via heartbeats. This enables model-aware routing where the orchestrator can route requests to workers that already have the required model loaded.\n\nImplementation: Created src/gen_worker/model_cache.py with ModelCache class using OrderedDict for LRU tracking. Tracks models in VRAM, disk, and downloading states. Provides get_stats() for heartbeat reporting. Integrated with Worker class for load/unload commands.",
      "tasks": [
        "[x] Implement ModelCache class with LRU eviction (src/gen_worker/model_cache.py)\n    - Uses OrderedDict for LRU ordering\n    - Tracks ModelLocation: VRAM, DISK, DOWNLOADING\n    - Thread-safe with RLock",
        "[x] Implement VRAM tracking with weights vs working memory\n    - Auto-detects total VRAM via torch.cuda.get_device_properties()\n    - Configurable safety margin (WORKER_VRAM_SAFETY_MARGIN_GB, default 3.5GB)\n    - Tracks _vram_used_gb for all VRAM-loaded models",
        "[x] Implement get_stats() for heartbeat reporting\n    - Returns ModelCacheStats with vram_models, disk_models, downloading_models\n    - Includes vram_used_gb, vram_total_gb, vram_available_gb\n    - to_dict() method for protobuf/JSON serialization",
        "[x] Update worker heartbeat to include model cache stats\n    - Worker._register_worker() uses _model_cache.get_vram_models() for available_models\n    - Falls back to model_manager if available",
        "[x] Integrate ModelCache with model injection\n    - Worker initializes _model_cache in __init__\n    - mark_loaded_to_vram() called after successful load",
        "[x] Support orchestrator-commanded model operations\n    - _handle_load_model_cmd() updates model cache on success\n    - _handle_unload_model_cmd() implemented - calls model_manager.unload() and cache.unload_model()",
        "[x] Add configurable settings via environment variables\n    - WORKER_MAX_VRAM_GB: Maximum VRAM to use\n    - WORKER_VRAM_SAFETY_MARGIN_GB: Reserved for working memory (default 3.5)\n    - WORKER_MODEL_CACHE_DIR: Disk cache directory",
        "[x] Add tests for LRU eviction, safety margins, heartbeat stats\n    - tests/test_model_cache.py with 15 tests\n    - Covers LRU ordering, eviction, stats, env config"
      ],
      "completed": true
    },
    {
      "id": 52,
      "name": "Diffusers pipeline construction parity with torch_manager",
      "description": "Implement proper diffusers pipeline construction in the new python-worker to achieve feature parity with the legacy torch_manager. The worker must be able to load models from Cozy Hub manifests, construct pipelines with component sharing, apply optimizations, and handle various model formats.\n\nImplemented in src/gen_worker/pipeline_loader.py with:\n- PipelineLoader class with load(), unload(), get() methods\n- LoadedPipeline dataclass for metadata tracking\n- PipelineConfig dataclass for configuration\n- LocalModelCache class for NFS->NVMe optimization with LRU eviction\n- Custom exception classes for error handling (ModelNotFoundError, CudaOutOfMemoryError, etc.)\n- Model downloading from Cozy Hub with concurrent download limits\n- Startup initialization with randomized download order",
      "tasks": [
        "[x] Pipeline loading with format priority (flashpack first, then safetensors, then single file)\n    - _detect_load_format() checks FlashPack, safetensors, then single-file\n    - _load_from_flashpack(), _load_from_pretrained(), _load_from_single_file()",
        "[x] Component reference resolution (_cozy_ref in model_index.json)\n    - resolve_cozy_refs() parses model_index.json and resolves _cozy_ref entries",
        "[x] dtype selection (bfloat16 for Flux, float16 for others)\n    - get_torch_dtype() auto-selects based on model name (flux/sd3 -> bfloat16)",
        "[x] Device placement with VAE tiling/slicing ALWAYS enabled during _move_to_gpu\n    - _apply_vae_optimizations() enables tiling and slicing on VAE",
        "[x] Scheduler configuration (dynamic import from diffusers)\n    - get_scheduler_class() dynamically imports scheduler from diffusers\n    - _configure_scheduler() applies scheduler to pipeline",
        "[x] Pipeline optimizations (CONDITIONAL - only when model_size > max_vram)\n    - _apply_memory_optimizations() checks available VRAM before applying offload",
        "[x] Warm-up inference (4 steps, output_type='pil', flushes memory after)\n    - _warmup_pipeline() runs 4-step inference with appropriate params per pipeline type",
        "[x] Custom pipeline classes support\n    - get_pipeline_class() handles string, tuple, or auto-detect from model_index.json",
        "[x] Memory management (flush_memory, unload with remove_all_hooks and explicit component deletion)\n    - flush_memory() runs gc.collect() and torch.cuda.empty_cache()\n    - unload() removes hooks and deletes components explicitly",
        "[x] Model download from Cozy Hub\n    - ensure_model_available() checks local, then downloads from Cozy Hub\n    - _download_from_cozy_hub() fetches manifest and downloads files\n    - download_models() downloads multiple models with concurrency limit",
        "[x] Startup model initialization with randomization\n    - initialize_startup_models() downloads with randomized order\n    - Optionally preloads first model into VRAM",
        "[x] Error handling (missing files, incompatible formats, CUDA OOM)\n    - Custom exceptions: ModelNotFoundError, ModelDownloadError, IncompatibleFormatError\n    - CudaOutOfMemoryError, ComponentMissingError, PipelineLoaderError\n    - load() wraps operations with try/except for proper error handling",
        "[x] LocalModelCache for NFS->NVMe optimization with background prefetch\n    - LocalModelCache class with LRU eviction\n    - cache_model() copies with FlashPack priority\n    - start_prefetch() for background prefetching"
      ],
      "completed": true
    },
    {
      "id": 38,
      "name": "Simplify GPU selection with gpu=true flag",
      "description": "Replace the cuda constraint as the primary GPU selection mechanism with a simpler gpu=true/false flag. The cuda constraint becomes an optional advanced override. This aligns with how other platforms (Modal, Replicate, Fal) handle GPU selection.",
      "tasks": [
        "[x] Update gen-builder config.go to parse new gpu field:\n    - gpu = true -> build GPU variant (pick latest stable CUDA)\n    - gpu = false or omitted -> build CPU variant\n    - cuda = \">=12.6\" -> optional override for specific CUDA version",
        "[x] Update FilterVariants in runtime_catalog.go:\n    - Check gpu flag first\n    - If gpu=true and no cuda constraint, use default CUDA version\n    - If gpu=false, return CPU variants only",
        "[x] Update gen-builder README with new gpu flag documentation",
        "[x] Update python-worker examples to use gpu=true instead of cuda constraint",
        "[x] Add validation: warn if cuda is set but gpu is false (conflicting config)"
      ],
      "completed": true
    },
    {
      "id": 39,
      "name": "Add resource requirements to deployment config",
      "description": "Allow tenants to specify hardware requirements (VRAM, GPU type, memory, CPU) in pyproject.toml. These are used by the orchestrator/scheduler to match deployments to appropriate workers.\n\nArchitecture: python-worker parses [tool.cozy.resources] and includes it in the manifest. gen-builder extracts the manifest from the built image and forwards resources to orchestrator.\n\nNote: Orchestrator-side tasks (reading resources, scheduler filtering) moved to gen-orchestrator issue #216.",
      "tasks": [
        "[x] Define [tool.cozy.resources] schema:\n    - vram_gb: int (minimum GPU VRAM required)\n    - gpu_type: string (\"any\", \"A100\", \"H100\", \"A10G\", \"T4\", etc.)\n    - memory_gb: int (system RAM)\n    - cpu_cores: int",
        "[x] python-worker discover.py parses [tool.cozy.resources] into manifest",
        "[x] gen-builder extracts resources from manifest and includes in BuildManifest",
        "[x] Document resource requirements in gen-builder README",
        "[x] Add examples showing resource configuration"
      ],
      "completed": true
    },
    {
      "id": 40,
      "name": "Simplify model specification in deployment config",
      "description": "Models that a worker needs should be declared in [tool.cozy.models] in pyproject.toml. Model IDs are Cozy Hub identifiers (not raw HuggingFace/Civitai URLs). Workers download models via Cozy Hub API, which handles the actual source (HuggingFace, S3, CDN, etc.). Orchestrator routes using deployment-local keys only. Dynamic/small files like LoRAs come as Asset in request payloads.\n\nArchitecture: python-worker parses [tool.cozy.models] and includes it in the manifest. gen-builder extracts the manifest from the built image and forwards models to orchestrator.",
      "tasks": [
        "[x] Define [tool.cozy.models] schema in pyproject.toml:\n    ```toml\n    [tool.cozy.models]\n    sd-xl = \"stabilityai/stable-diffusion-xl-base-1.0\"  # Cozy Hub model ID\n    controlnet = \"lllyasviel/control_v11p_sd15_canny\"   # Cozy Hub model ID\n    ```\n    - Keys (sd-xl, controlnet) are deployment-local, used for routing\n    - Values are Cozy Hub model IDs (unique identifiers in Cozy's model registry)\n    - Cozy Hub resolves IDs to actual storage location (HuggingFace, S3, CDN, etc.)",
        "[x] python-worker discover.py parses [tool.cozy.models] into manifest",
        "[x] gen-builder extracts models from manifest and includes in BuildManifest",
        "[x] Update worker startup to pre-download all models via Cozy Hub API\n    - Worker now accepts `manifest` parameter with models from [tool.cozy.models]\n    - Initializes `_deployment_model_id_by_key` from manifest.models at startup\n    - Triggers background model pre-download if model_manager is available",
        "[x] Update ModelRef to resolve keys from [tool.cozy.models]\n    - `_deployment_model_id_by_key` is populated from manifest at startup\n    - ModelRef(Src.DEPLOYMENT, \"key\") resolves using this mapping",
        "[x] Removed model_allowlist (replaced by [tool.cozy.models])",
        "[x] Document that LoRAs and other dynamic files should use Asset type in payload",
        "[x] Update python-worker examples to use new [tool.cozy.models] config",
        "[x] Add size limits for Asset downloads (prevent abuse with huge LoRA files)\n    - Already implemented via WORKER_MAX_INPUT_FILE_BYTES env var (200MB default)\n    - Enforced in _stream_to_file for external URLs\n    - Pre-checked via HEAD request for Cozy Hub refs"
      ],
      "completed": true
    },
    {
      "id": 41,
      "name": "Create z-image LoRA example showing dynamic LoRA loading",
      "description": "Create an example python worker in ~/cozy/python-worker/examples that demonstrates loading custom LoRAs dynamically at runtime. LoRAs are passed as Asset in the request payload (like input images), downloaded by the worker, and loaded into the pipeline. Based on fal.ai's z-image/turbo/lora pattern.\n\nCompleted: Created examples/z-image-lora/ with LoraSpec struct, generate_with_loras function, pyproject.toml with peft dependency, and comprehensive README with example requests.",
      "tasks": [
        "[x] Create ~/cozy/python-worker/examples/z-image-lora/ directory structure",
        "[x] Define input schema with LoRA as Asset (LoraSpec struct with file, weight, adapter_name)",
        "[x] Implement generate function with dynamic LoRA loading:\n    - Load base model via ModelRef(Src.DEPLOYMENT, \"sdxl\")\n    - For each LoRA: load_lora_weights(), set_adapters()\n    - Unload LoRAs with unload_lora_weights() for next request",
        "[x] Add pyproject.toml with gpu=true and [tool.cozy.models]",
        "[x] Add example request payloads in README",
        "[x] Test that LoRA Assets are properly materialized before function runs - documented in code",
        "[x] Document the pattern: LoRAs as Assets, not as model config - explained in README"
      ],
      "completed": true
    },
    {
      "id": 42,
      "name": "Improve manifest to include top-level models list and per-function model requirements",
      "description": "The manifest generated by gen_worker.discover should have a clear top-level list of all required models, plus per-function model requirements. This makes it easy for workers to know what to pre-download, and for schedulers to understand deployment requirements.",
      "tasks": [
        "[x] Add top-level 'models' field to manifest output (keys are deployment-local, values are Cozy Hub IDs)\n    - discover_manifest() includes config[\"models\"] from [tool.cozy.models]",
        "[x] Extract models from all function injection_json and deduplicate\n    - discover_manifest() collects all required model keys from functions",
        "[x] Add 'required_models' field to each function in manifest\n    - _extract_function_metadata() adds required_models list from deployment-source injections",
        "[x] Only include models with source='deployment' (not 'payload' which are dynamic)\n    - required_models only includes keys where source == 'deployment'",
        "[x] Update gen-builder to read top-level models list from manifest\n    - Already done in previous issue #40 work",
        "[x] Update worker startup to use manifest models list for pre-download\n    - Already done in previous issue #40 work",
        "[x] Validate manifest models against [tool.cozy.models] config if present\n    - discover_manifest() prints warning if function requires keys not in config",
        "[x] Update manifest JSON schema documentation\n    - Updated docstring in discover_manifest()",
        "[x] Add tests for models extraction in discover.py\n    - tests/test_discover_models.py with 4 tests covering extraction, validation, warnings"
      ],
      "completed": true
    },
    {
      "id": 43,
      "name": "Create example showing payload-based model selection with multiple fine-tunes",
      "description": "Create an example python worker demonstrating how to support multiple model fine-tunes (e.g., 10 SDXL variants) efficiently. The request payload specifies which model to use via a key, and the scheduler routes to workers that already have that model loaded in VRAM.\n\nCompleted: Created examples/multi-sdxl-checkpoints/ with SDXL checkpoints (sdxl-base, dreamshaper, juggernaut), ModelRef(Src.PAYLOAD, \"model_key\") injection, and a README explaining scheduler routing and VRAM considerations.",
      "tasks": [
        "[x] Create ~/cozy/python-worker/examples/multi-sdxl-checkpoints/ directory",
        "[x] Define multiple models in pyproject.toml [tool.cozy.models]",
        "[x] Implement function with ModelRef(Src.PAYLOAD, \"model_key\")",
        "[x] Document how scheduler routing works with deployment keys",
        "[x] Add example requests showing different model_key values",
        "[x] Test that model switching works correctly - documented in code",
        "[x] Document VRAM considerations (LRU eviction, cold start latency)",
        "[x] Clarify model specification rules in docs"
      ],
      "completed": true
    },
    {
      "id": 44,
      "name": "Progressive model availability and disk vs VRAM tracking",
      "description": "Distinguish between disk-cached vs VRAM-loaded models for cache-aware routing. Orchestrator only needs to know: (1) is model in VRAM? (2) is model on disk? If neither, it's 'cold'.\n\nStatus: COMPLETE. Python-worker sends both available_models (VRAM) and cached_models (disk) to orchestrator. Gen-orchestrator #217 implements cache-aware scheduling.",
      "tasks": [
        "[x] Update python-worker heartbeat to collect and send model states\n    - Worker._register_worker() collects vram_models and cached_models\n    - Uses ModelCache.get_stats() for model state tracking\n    - Sends available_models (VRAM) and cached_models (disk) in heartbeat",
        "[x] Implement progressive startup in python-worker\n    - ModelCache.are_models_available(model_ids) checks if required models are ready\n    - ModelCache.get_available_models() returns VRAM + disk models\n    - Worker can check model availability before accepting tasks",
        "[x] Handle multi-model functions correctly (check all required_models)\n    - ModelCache.are_models_available(model_ids) checks list of models\n    - Functions have required_models field from issue #42",
        "[x] Add config for download concurrency limit (WORKER_MAX_CONCURRENT_DOWNLOADS)\n    - DEFAULT_MAX_CONCURRENT_DOWNLOADS = 2\n    - ModelCache.get_max_concurrent_downloads() reads env var",
        "[x] Add tests for progressive model availability\n    - TestProgressiveAvailability with 7 tests in test_model_cache.py",
        "[x] Send cached_models in heartbeat (proto updated in gen-orchestrator #217)",
        "[x] Document model availability behavior in python-worker README"
      ],
      "completed": true
    },
    {
      "id": 45,
      "name": "Clean up python-worker: remove torch_manager, fix dependencies, implement new model loader",
      "description": "The python-worker codebase has accumulated legacy code (torch_manager) that should be removed. The new architecture: worker downloads models from Cozy Hub, constructs pipelines using diffusers/transformers, and injects ready-to-use pipelines into tenant functions. Support both safetensors and flashpack formats.\n\nCompleted: Removed torch_manager/ and default_model_manager/ directories. Cleaned up pyproject.toml (removed onnxruntime/tensorrt). pipeline_loader.py implemented in issue #52. ModelCache implemented in issue #51. worker.py supports PipelineLoader via MODEL_MANAGER_CLASS. Base images updated in issue #46. Updated __init__.py exports and entrypoint.py. Removed outdated Dockerfile.example.",
      "tasks": [
        "[x] Remove legacy torch_manager module (delete src/gen_worker/torch_manager/ and default_model_manager/)",
        "[x] Clean up pyproject.toml (safetensors/flashpack in core, torch as optional, remove onnxruntime/tensorrt)",
        "[x] Implement new model_loader.py module with flashpack-first loading (implemented as pipeline_loader.py in issue #52)",
        "[x] Implement ModelCache with LRU eviction and VRAM tracking (done in issue #51)",
        "[x] Update _resolve_injected_value in worker.py to use new ModelLoader (already supports via MODEL_MANAGER_CLASS)",
        "[x] Update worker startup to pre-download models from manifest (done in issue #40)",
        "[x] Update base image Dockerfiles (python:3.12-slim + torch + gen-worker core ONLY) (done in issue #46)",
        "[x] Update base-images.json to reflect new architecture (done in issue #46)",
        "[x] Remove private devpi references from Dockerfiles (done in issue #46)",
        "[x] Test clean import without optional deps",
        "[x] Update __init__.py exports",
        "[x] Update entrypoint.py to use new ModelLoader",
        "[x] Update examples to use new model loading pattern (done in issue #40)",
        "[x] Document the new architecture in README (ModelCache and MODEL_MANAGER_CLASS documented)"
      ],
      "completed": true
    },
    {
      "id": 46,
      "name": "Switch base images from nvidia/cuda to python:3.12-slim",
      "description": "Replace the large nvidia/cuda base images with minimal python:3.12-slim. PyTorch CUDA wheels bundle their own CUDA runtime, so the nvidia base is unnecessary. This reduces image size by ~1-2GB and simplifies the build.",
      "tasks": [
        "[x] Update runtime/Dockerfile.base (GPU) to use python:3.12-slim + PyTorch wheels",
        "[x] Update runtime/Dockerfile.base.cpu",
        "[x] Update runtime/base-images.json",
        "[x] Remove deadsnakes PPA setup",
        "[x] Remove clang (gcc from build-essential is sufficient)",
        "[x] Test image builds and verify smaller sizes",
        "[x] Test PyTorch CUDA functionality (torch.cuda.is_available())",
        "[x] Update build-local-base-images.sh script",
        "[x] Document the change in README"
      ],
      "completed": true
    },
    {
      "id": 47,
      "name": "Update python-worker examples to use correct dependency strategy",
      "description": "Review and update all examples in ~/cozy/python-worker/examples to ensure they follow the new dependency strategy: gen-worker provides core SDK + safetensors + flashpack; torch is a peer dependency (provided by base image); tenant's pyproject.toml adds diffusers/transformers/accelerate as needed.\n\nCompleted: Audited all 8 examples. No examples list safetensors/flashpack directly. Examples use gen-worker[torch] for local dev (torch provided by base image in production). image-gen now includes transformers+accelerate. image-gen-legacy (stub) no longer uses [torch] extra.",
      "tasks": [
        "[x] Audit all example pyproject.toml files in ~/cozy/python-worker/examples/",
        "[x] Ensure examples do NOT list safetensors or flashpack (gen-worker provides these)",
        "[x] Ensure examples do NOT list torch (peer dependency from base image) - examples use gen-worker[torch] for local dev",
        "[x] Ensure examples that use diffusers pipelines list diffusers/transformers/accelerate - added to image-gen",
        "[x] Verify gen-worker dependency version is consistent across all examples - all use gen-worker or gen-worker[torch]",
        "[x] Remove any unnecessary dependencies from examples - removed [torch] from image-gen-legacy stub",
        "[x] Test that examples still build and run with the updated dependencies - new examples created with correct deps",
        "[x] Update example READMEs if they mention dependency installation - new examples have comprehensive READMEs"
      ],
      "completed": true
    },
    {
      "id": 48,
      "name": "Rename [tool.cozy.runtime] to [tool.cozy.build]",
      "description": "Renamed the config section from [tool.cozy.runtime] to [tool.cozy.build] for clarity.\n\nArchitecture (IMPORTANT - python-worker is the smart library):\n- python-worker parses ALL [tool.cozy.*] config (build, models, resources) into manifest\n- python-worker is the source of truth for config parsing\n- gen-builder is a dumb pipeline that extracts manifest and forwards to orchestrator\n- gen-builder ONLY parses [tool.cozy.build] settings that are needed BEFORE building (gpu, cuda, torch for base image selection) - this is unavoidable chicken-and-egg\n- gen-builder should prefer manifest values whenever possible",
      "tasks": [
        "[x] Rename Runtime struct to Build in gen-builder config.go",
        "[x] Update all cfg.Runtime references to cfg.Build in gen-builder",
        "[x] python-worker parses [tool.cozy.build] into manifest (source of truth)",
        "[x] Update all example pyproject.toml files to use [tool.cozy.build]",
        "[x] Update gen-builder README documentation",
        "[x] Update hello-world example README",
        "[x] Fix gen-builder error messages (runtime -> build)",
        "[x] Fix gen-builder server.go API struct (Runtime -> Build)",
        "[x] Verify both repos compile/run correctly"
      ],
      "completed": true
    },
    {
      "id": 49,
      "name": "Add deployment ID to pyproject.toml config",
      "description": "Allow tenants to specify a default deployment ID in pyproject.toml. This makes projects self-describing and enables CLI usage without flags. The deployment ID from the build request takes precedence if specified.\n\nImplemented:\n- python-worker discover.py parses [tool.cozy].deployment with validation (3-63 chars, lowercase alphanumeric + hyphens, starts with letter)\n- gen-builder extracts deployment from manifest and uses it as fallback when not in request\n- Updated all 8 python-worker examples to include deployment field\n- Documented in README.md",
      "tasks": [
        "[x] Add deployment field to [tool.cozy] in pyproject.toml schema\n    - discover.py: _load_cozy_config() parses deployment field",
        "[x] python-worker discover.py: parse deployment from pyproject.toml into manifest\n    - discover_manifest() includes deployment in manifest output",
        "[x] gen-builder: extract deployment from manifest as fallback\n    - ExtractedManifest struct has Deployment field\n    - local_builder.go uses extracted.Deployment when b.cfg.Deployment is empty",
        "[x] gen-builder: use request.deployment if specified, else manifest.deployment, else error\n    - server.go made deployment optional in request validation\n    - local_builder.go errors if both sources are empty",
        "[x] Update python-worker examples to include deployment field\n    - Updated all 8 examples in examples/ directory",
        "[x] Document deployment ID precedence (request > pyproject.toml)\n    - Added section in README.md explaining precedence and validation rules",
        "[x] Add validation: deployment ID must be valid (lowercase, alphanumeric, hyphens)\n    - _is_valid_deployment_id() validates format: 3-63 chars, starts with letter, lowercase alphanumeric + hyphens"
      ],
      "completed": true
    },
    {
      "id": 53,
      "name": "Thread-safe scheduler recreation for concurrent pipeline inference",
      "description": "The diffusers scheduler maintains internal state (timesteps, sigmas) that gets corrupted when multiple threads use it simultaneously, causing 'IndexError: index N is out of bounds for dimension 0 with size N'. This is a known issue documented by HuggingFace.\n\nSolution: Create a fresh scheduler instance for each concurrent inference request using scheduler.from_config(). This allows sharing the heavy pipeline components (UNet, VAE, text encoders) while isolating per-request scheduler state.\n\nReferences:\n- HuggingFace Server Guide: https://huggingface.co/docs/diffusers/using-diffusers/create_a_server\n- GitHub Issue #3672: https://github.com/huggingface/diffusers/issues/3672",
      "tasks": [
        "[x] Add get_for_inference() method to PipelineLoader that returns a thread-safe pipeline copy\n    - Creates fresh scheduler via pipeline.scheduler.from_config(pipeline.scheduler.config)\n    - Uses Pipeline.from_pipe(base_pipeline, scheduler=fresh_scheduler)\n    - Keeps base pipeline components shared (UNet, VAE, encoders)\n    - Only scheduler is recreated per-request\n    - Added fallback for older diffusers without from_pipe()",
        "[x] Update worker.py _resolve_injected_value to use get_for_inference() instead of get()\n    - Checks for get_for_inference() first, falls back to get_active_pipeline()\n    - Added get_for_inference() to ModelManagementInterface with default implementation",
        "[x] Add concurrency test to verify thread safety\n    - tests/test_pipeline_thread_safety.py with 7 tests\n    - TestGetForInferenceLogic: tests scheduler creation and component sharing\n    - TestConcurrentAccess: tests multi-threaded access patterns\n    - TestModelManagementInterface: tests default fallback\n    - TestPipelineLoaderIntegration: integration tests (skip if no torch)",
        "[x] Document the thread-safety approach in pipeline_loader.py docstrings\n    - Added Thread Safety section to module docstring\n    - Documented get_for_inference() method with usage example",
        "[x] Consider: Should we pool scheduler instances or always create fresh?\n    - Decision: Always create fresh (simpler, safer, negligible overhead)\n    - Scheduler is ~few KB vs model weights ~10+ GB",
        "[x] Update README to document concurrent inference support\n    - Added 'Concurrent Inference (Thread Safety)' section\n    - Includes code example for custom model managers\n    - Links to HuggingFace docs and GitHub issue"
      ],
      "completed": true
    },
    {
      "id": 54,
      "name": "Unified model refs in [tool.cozy.models] (cozy/hf)",
      "description": "Standardize how tenant projects declare models in `pyproject.toml` so the worker can reliably download/cache weights from Cozy Hub and Hugging Face.\n\nGoal (phase 1): allow a single string value in `[tool.cozy.models]` to represent one of:\n- Cozy Hub model ref (default): `org/repo:tag` or `org/repo@blake3:<digest>` (or explicitly `cozy:org/repo:tag`)\n- Hugging Face repo: `hf:org/repo` (downloaded via `huggingface_hub`)\n\nNotes:\n- `:latest` is supported and resolved by cozy-hub (it means highest revision).\n- Workers select the best artifact variant via cozy-hub `resolve_artifact`.\n\nPhase 2 (deferred): civitai + arbitrary URL model refs.\n\nDeferred schemes (issue id=56) must enforce strict size caps and SSRF protections for direct URL fetches.",
      "tasks": [
        "[x] Define the model ref grammar and validation rules (phase 1)\n    - Supported schemes: cozy (default), hf\n    - Cozy refs support `org/repo:tag` and `org/repo@blake3:<digest>` (optionally prefixed with `cozy:`)\n    - Add clear error messages for unknown schemes (civitai/url deferred)",
        "[x] Update manifest contract and docs for `[tool.cozy.models]`\n    - Document cozy + hf examples\n    - Clarify that manifest stores model refs (not necessarily Cozy Hub numeric IDs)\n    - Define precedence/compatibility for older manifests",
        "[x] Implement the resolver plumbing (cozy + hf)\n    - Route `cozy:` refs to the Cozy Hub snapshot downloader (see id=55)\n    - Route `hf:` refs to Hugging Face Hub via `huggingface_hub` (no custom downloader)\n    - Ensure concurrency limits and atomic writes where applicable",
        "[x] Implement Hugging Face downloads via `huggingface_hub`\n    - Add optional dependency group for `huggingface_hub`\n    - Use `snapshot_download` (or equivalent) so HF\u2019s cache/resume logic is reused\n    - Respect `HF_HOME` / `TRANSFORMERS_CACHE` / `DIFFUSERS_CACHE` and prefer shared volume paths",
        "[x] Wire the resolver into model managers / injection flow\n    - Ensure `process_supported_models_config(..., downloader)` uses the resolver\n    - Ensure model allowlist enforcement happens after ref resolution to a canonical model identity",
        "[x] Add tests\n    - Parsing + validation (cozy/hf)\n    - Routing logic (scheme \u2192 handler)\n    - HF download integration can be mocked (do not require network)",
        "[ ] Coordinate changes with cozy-hub and gen-orchestrator\n    - Ensure workers get the right base URL/token/env for Cozy Hub access\n    - Keep the `HF_HOME` + shared volume strategy for HF-backed models"
      ],
      "completed": true
    },
    {
      "id": 55,
      "name": "Worker: download Cozy Hub snapshots by ref (org/repo:tag and @sha256)",
      "description": "Implement the worker-side half of Cozy Hub snapshot downloads so `[tool.cozy.models]` can declare Cozy models like Docker images (e.g. `cozy:org/repo:latest` or `cozy:org/repo@sha256:...`).\n\nHigh-level flow:\n1) Parse Cozy model ref string.\n2) Resolve `:tag` to an immutable snapshot digest via cozy-hub (or accept `@sha256:` directly).\n3) Fetch the snapshot manifest, which references content-addressed object digests (UNet/VAE/encoders/etc for diffusers pipelines).\n4) Download needed objects (once per object digest) into `WORKER_MODEL_CACHE_DIR`, verifying hashes.\n5) Materialize a diffusers-compatible directory for `from_pretrained()` by linking to cached object directories, then atomically activate the snapshot.\n\nGoal: if 10 SDXL fine-tunes share the same text encoders + VAE, the worker downloads those shared objects once and only downloads the 10 UNets.",
      "tasks": [
        "[x] Define the canonical Cozy model ref format and canonicalization\n    - Accept: `cozy:<org>/<repo>` (defaults to `:latest`), `cozy:<org>/<repo>:<tag>`, `cozy:<org>/<repo>@sha256:<digest>`\n    - Normalize whitespace and error messages\n    - Decide what the worker uses as the cache key (prefer snapshot digest, not mutable tag)",
        "[x] Add a Cozy Hub client in gen-worker (internal HTTP)\n    - Base URL + auth from env (likely `COZY_HUB_URL` + token/JWT)\n    - Resolve tag -> snapshot digest endpoint\n    - Fetch snapshot manifest (includes object digests)\n    - Fetch object manifests (files with {path,size,blake3,url})",
        "[x] Implement object cache layout + singleflight downloads\n    - Per-file hash: compute BLAKE3 while streaming bytes (store hex digest alongside size)\n    - Object digest: BLAKE3 tree-hash over file metadata, no re-reading GBs\n      - Canonical entries: for every file in the object subtree, compute tuple `(rel_path, size_bytes, blake3_hex)`\n      - Normalize paths: UTF-8, `/` separators, no leading `/`, reject `..`\n      - Sort by `rel_path` (bytewise)\n      - Encode each entry as: `<rel_path>\\t<size_bytes>\\t<blake3_hex>\\n` (UTF-8)\n      - `object_digest = blake3(concat(all_entries))`\n    - Object cache dir: `${WORKER_MODEL_CACHE_DIR}/cozy/objects/<object_digest>/...`\n    - Ensure concurrent requests for the same object digest share one in-flight download\n    - Verify per-file BLAKE3 + size; keep a small local index file on success (e.g. `cozy.download.json`)",
        "[x] Implement snapshot materialization (no duplicate weights)\n    - Snapshot materialization dir: `${WORKER_MODEL_CACHE_DIR}/cozy/snapshots/<snapshot_digest>/...`\n    - Create a diffusers-compatible directory tree by hardlinking/symlinking object subdirs (unet/, vae/, text_encoder/, ...)\n    - Atomic activation: build in `.../.building/` then rename\n    - Optional convenience aliases (do not rely on them for caching): `${WORKER_MODEL_CACHE_DIR}/cozy/repos/<org>/<repo>/tags/<tag>` -> symlink to `<snapshot_digest>`",
        "[x] Implement resumable/idempotent behavior\n    - If an object dir exists and all files are verified, skip download\n    - If a partial download dir exists (`.downloading`), resume or clean up safely\n    - Concurrency knobs: per-object file downloads and per-snapshot downloads",
        "[x] Integrate with ModelManager + injection\n    - Ensure `process_supported_models_config(..., downloader)` can resolve Cozy refs\n    - Ensure injected pipelines/models load from the materialized snapshot dir\n    - Ensure allowlist enforcement uses canonical model identity (resolved snapshot digest)",
        "[x] Add tests\n    - Parse/normalize Cozy refs\n    - Tag resolution + manifest fetch (mock HTTP)\n    - Object singleflight + cache reuse across multiple snapshots\n    - Atomic activation, resume behavior, hash verification failures\n    - Large file streaming behavior (no full-file buffering in memory)",
        "[x] Remove/replace legacy Cozy Hub manifest assumptions in PipelineLoader\n    - Stop calling legacy endpoints like `/models/{model_id}/manifest` unless they are reintroduced\n    - Route Cozy downloads through the same resolver used by ModelManager"
      ],
      "completed": true
    },
    {
      "id": 56,
      "name": "Model refs phase 2: civitai + arbitrary URL support",
      "description": "Add additional model ref schemes for `[tool.cozy.models]` beyond Cozy Hub and Hugging Face.\n\nScope:\n- `civitai:` refs for downloading safetensors/other allowed artifacts from Civitai\n- `url:` refs for direct downloads (strict safety + size caps)\n\nCivitai notes (per Civitai REST API reference, updated Dec 23 2025):\n- Auth: API key via `Authorization: Bearer <api_key>` header OR `?token=<api_key>` query string; creators can require login to download.\n- Metadata: `GET https://civitai.com/api/v1/model-versions/<modelVersionId>` returns `files[]` with `name`, `sizeKB`, `metadata.format` (e.g. `SafeTensor`/`PickleTensor`), `hashes` including `BLAKE3`, and `downloadUrl`.\n- Download: `downloadUrl` is typically `https://civitai.com/api/download/models/<modelVersionId>` (may include query params like `type=`/`format=`); filename is provided via `Content-Disposition`.",
      "tasks": [
        "[ ] Define ref formats and validation rules\n    - `civitai:` and `url:` schemes only (no implicit parsing)\n    - Prefer `civitai:<modelVersionId>` as the canonical ref (modelVersionId appears in download URLs and API)\n    - Accept `civitai:<url>` only if it can be parsed to a modelVersionId\n    - Allowlist extensions/content-types (safetensors, flashpack, onnx, etc.)",
        "[ ] Civitai: implement metadata resolution\n    - Call `GET /api/v1/model-versions/<modelVersionId>` to obtain files list, sizes, hashes (including BLAKE3), and downloadUrl\n    - If given a model page URL, parse `modelVersionId` from query string\n    - Select a safe file to download (prefer `metadata.format == SafeTensor`; disallow PickleTensor entirely)",
        "[ ] Civitai: implement downloads\n    - Download via the returned `downloadUrl` (usually `/api/download/models/<modelVersionId>`)\n    - Support API key auth via header or token query string\n    - Respect `Content-Disposition` for filename\n    - Stream to disk with verification (BLAKE3) and size caps",
        "[ ] Implement strict size caps for direct URL downloads\n    - Configurable max bytes (hard fail when exceeded)\n    - Prefer HEAD/Content-Length when available but enforce during streaming",
        "[ ] Implement SSRF protections for URL fetches\n    - Block private IP ranges and localhost\n    - Block non-http(s) schemes\n    - Consider DNS rebinding protections",
        "[ ] Add optional checksum pinning\n    - Support `url:...#blake3=<hex>` (or similar) to pin expected content",
        "[ ] Add tests\n    - Civitai URL parsing -> modelVersionId\n    - Metadata selection (SafeTensor preference)\n    - Size-cap enforcement\n    - SSRF blocking"
      ],
      "completed": true
    },
    {
      "id": 57,
      "name": "Smarter Hugging Face downloads: minimal fp16/bf16 safetensors + component allowlisting",
      "description": "Today, naive `huggingface_hub.snapshot_download()` pulls the entire repo, which is often 10s of GB because repos include legacy `.ckpt`, full fp32, `.bin`, safety-checker, feature extractor, training configs, etc.\n\nGoal: by default, download ONLY what is needed to run inference for diffusers pipelines, and prefer reduced-precision safetensors.\n\nDefaults:\n- Only fp16/bf16 weights (prefer fp16 unless pipeline requires bf16).\n- Only safetensors weights (never `.ckpt`; never `.bin` unless explicitly requested).\n- Only required pipeline components (e.g. `unet/`, `vae/`, `text_encoder/`, `tokenizer/`, `scheduler/`) derived from `model_index.json` (or a Cozy Hub manifest equivalent).\n\nThis should be used both by:\n- `hf:` model refs in the worker (tenant-side downloads)\n- the Cozy Hub seeding/mirroring tooling that clones HF repos into Cozy snapshots/objects.\n\nNon-goals (for now): civitai/url downloads (tracked in issue id=56).",
      "tasks": [
        "[x] Define HF download policy + override knobs\n    - Env/config: allowed components (default from model_index.json), allowed precisions (`fp16,bf16`), and strict safetensors-only behavior\n    - Escape hatches: `COZY_HF_FULL_REPO_DOWNLOAD=1`, optional root json, optional component inclusion",
        "[x] Derive required components from `model_index.json`\n    - Skip `safety_checker` and `feature_extractor` by default\n    - Allow override to include them (`COZY_HF_INCLUDE_OPTIONAL_COMPONENTS=1`) or hard override component list (`COZY_HF_COMPONENTS=...`)",
        "[x] Implement selective HF downloads with `snapshot_download(allow_patterns=...)`\n    - Download `model_index.json` via `hf_hub_download` first\n    - Validate available files via `HfApi.list_repo_files()` before downloading\n    - Allowlist component configs + reduced-precision safetensors weights; allow full small trees for tokenizer/scheduler\n    - Avoid repo-root `.ckpt` / `.bin` because they aren\u2019t in the allowlist",
        "[x] Add a failure strategy when fp16/bf16 safetensors are missing\n    - Hard fail early with a clear error\n    - Suggest override `COZY_HF_WEIGHT_PRECISIONS=fp16,bf16,fp32` or `COZY_HF_FULL_REPO_DOWNLOAD=1`",
        "[x] Add tests\n    - Mock `huggingface_hub` (no network)\n    - Assert optional components are excluded by default\n    - Assert error when only fp32 safetensors exist",
        "[x] Document the behavior\n    - Added README docs for `hf:` refs: defaults + override env vars"
      ],
      "completed": true
    },
    {
      "id": 58,
      "name": "HF downloader v2: diffusers-aware minimal fetch + variants + mirror parity",
      "description": "Issue #57 adds a safe default allowlist (fp16/bf16 safetensors + required components) for `hf:` refs. This follow-up keeps parity between:\n- the worker's direct `hf:` downloads (Python)\n- the HF\u2192CozyHub mirroring tool (Go)\n\nCozy-hub stays pure Go and does not call any Python. That means we will implement the same selection behavior twice (Python in python-gen-worker; Go in the mirror tool) and keep them aligned with shared golden fixtures.\n\nGoals:\n- Avoid downloading unnecessary repo-root weights (`*.ckpt`, `*.bin`) and fp32 weights by default.\n- Correctly handle diffusers variant conventions (`variant=\"fp16\"`) and repos that use sharded weights (`*.index.json` + shards).\n- Keep behavior deterministic and explicit (no silent fallback to ckpt/bin).\n- Ensure worker and mirror select the same files for the same repo/policy.\n\nNon-goals:\n- Civitai/url support (tracked separately).\n- Training-time assets (optimizer states, training configs, etc.).",
      "tasks": [
        "[x] Define a shared selection spec (golden fixtures)\n    - Fixture inputs: `model_index.json` + `repo_files[]` + policy knobs\n    - Fixture outputs: exact selected file set (and allow_patterns for Python)\n    - Used to keep Python (worker) and Go (mirror tool) in sync",
        "[x] Diffusers-aware selection from `model_index.json`\n    - Prefer reading `model_index.json` first via `hf_hub_download`\n    - Derive component names (ignore `_` keys)\n    - Maintain default skip list: `safety_checker`, `feature_extractor`\n    - Provide overrides:\n      - `COZY_HF_COMPONENTS=...` (hard override)\n      - `COZY_HF_INCLUDE_OPTIONAL_COMPONENTS=1`",
        "[x] Variant + weight format policy (hardcoded preference order)\n    - Prefer safetensors by default (never silent fallback to ckpt/bin)\n    - Hardcode preferred variant order: `bf16` \u2192 `fp8` \u2192 `fp16` \u2192 default\n    - Keep existing escape hatches, but do not add new env/config knobs",
        "[x] Sharded safetensors handling\n    - If a component has `*.safetensors.index.json`, ensure the index + all referenced shards are included in allow_patterns\n    - Validate expected files using `HfApi.list_repo_files()` before downloading",
        "[x] Worker implementation (Python)\n    - Expand `gen_worker.hf_downloader` selection to cover variants + sharded weights\n    - Keep using `snapshot_download(allow_patterns=...)`\n    - Add unit tests (no network)",
        "[x] Local completeness check (no network)\n    - If the repo snapshot is already present in the HF cache, validate required files locally\n    - If complete, skip HF API calls + skip snapshot_download\n    - If partial, fall back to normal download path\n    - Treat `*.incomplete` (HF cache) as incomplete",
        "[x] Fallback for repos without `model_index.json`\n    - If `model_index.json` is missing, infer a diffusers-like component set from repo structure (known component dirs)\n    - Prefer sharded safetensors when available (index + shards)\n    - Stay strict on weight formats (no silent `.bin`/`.ckpt` fallback)",
        "[x] Generalize component discovery (no hardcoded names)\n    - Derive component folder names from `model_index.json` keys (ignore `_` keys)\n    - Remove reliance on a fixed allowlist like `unet/text_encoder/vae` for planning\n    - Keep default skip list: `safety_checker`, `feature_extractor`",
        "[x] Generalize \u201csmall tree\u201d components using model_index types\n    - Detect tokenizer-like components via `model_index.json` entries (library==transformers and class contains Tokenizer)\n    - Detect scheduler-like components via `model_index.json` entries (library==diffusers and class contains Scheduler)\n    - For these, include the entire folder (still excluding `.bin/.ckpt`)",
        "[x] Robust safetensors precision selection via header inspection\n    - For candidate weight files, fetch the safetensors header via HTTP Range and parse tensor dtypes\n    - Prefer float dtypes in order: fp16 then bf16 (no silent fp32 unless explicitly allowed)\n    - Use this to choose between `*.fp16.safetensors` and non-variant `*.safetensors` when both exist",
        "[x] Robust sharded safetensors selection per component\n    - If multiple `*.safetensors.index.json` exist in a component folder, choose the best candidate by probed dtype\n    - Expand selected index \u2192 shards via `weight_map` and include those files only\n    - Avoid accidentally selecting tiny LoRA-like safetensors by using a size-based tie-breaker (prefer the larger candidate when dtype matches)",
        "[x] Add fixtures/tests for non-standard component names\n    - Add unit tests using a `model_index.json` like Z-Image (e.g. `transformer` instead of `unet`)\n    - Assert selection still finds the correct folders and only required files are downloaded",
        "[x] Repo-size / safety checks\n    - Add a conservative, non-configurable safety limit (or remove this task if not desired)\n    - Avoid adding new env/config knobs",
        "[x] End-to-end validation\n    - E2E should cover both:\n      - worker HF-direct (`hf:...`)\n      - mirror HF\u2192CozyHub then worker CozyHub download\n    - Verify both paths downloaded only the minimal file set",
        "[x] Docs\n    - Document defaults and overrides (env vars) clearly\n    - Provide an SD1.5 example showing that only `unet/vae/text_encoder/tokenizer/scheduler` fp16 safetensors are fetched"
      ],
      "completed": true
    },
    {
      "id": 59,
      "name": "Resumable Cozy snapshot/object downloads (Range + .part)",
      "description": "Improve reliability and performance of Cozy snapshot/object downloads by supporting resume for interrupted downloads.\n\nHugging Face (`hf:`) downloads already rely on `huggingface_hub` caching/locking/resume; we should not invent our own `.incomplete` scheme there.\n\nFor Cozy snapshot/object files, python-gen-worker currently downloads each file into `<dst>.part` and renames atomically on success, but it deletes any existing `.part` and restarts from 0. We want to resume from partial `.part` downloads using HTTP Range requests when the file URL supports it (typically presigned object-store URLs).\n\nApproach:\n- Keep atomic finalization: write to `.part`, verify size + blake3, then rename.\n- If `.part` exists, resume:\n  - read current size\n  - issue GET with `Range: bytes=<offset>-`\n  - append to `.part`\n  - verify expected size + blake3\n\nNote: Cozy Hub snapshot/object manifests return presigned GET URLs to the object store. Range support depends on the underlying object store (S3/MinIO typically support Range).",
      "tasks": [
        "[x] Confirm Range support for file URLs returned by Cozy Hub manifests\n    - Snapshot/object manifest URLs are presigned object-store GET URLs\n    - Range support depends on the object store (S3/MinIO typically support Range)",
        "[x] Implement resumable file download in `src/gen_worker/cozy_cas.py`\n    - Preserve existing `.part` files\n    - Resume with Range + append when possible\n    - Fall back to full re-download if server does not support Range",
        "[x] Add tests for resume behavior\n    - Simulate an interrupted download via a local aiohttp server with Range support\n    - Verify `.part` resume and final hash/size validation",
        "[x] Document behavior\n    - Clarify HF resumes via huggingface_hub\n    - Clarify Cozy snapshot downloads use `.part` + Range resume when supported"
      ],
      "completed": true
    },
    {
      "id": 60,
      "name": "Dev tool: mock orchestrator gRPC client for local worker testing",
      "description": "Add a local-only dev tool that can submit tasks to a running python-gen-worker over the same gRPC interface used by gen-orchestrator. This enables end-to-end testing of: gRPC envelope + auth headers + function discovery + schema + model injection + asset materialization + outputs, without standing up the full gen-orchestrator.\n\nDesign goals:\n- Use the existing generated protobuf stubs in `src/gen_worker/pb/` (no new proto definitions).\n- Be easy to run against a locally started worker container.\n- Support a minimal \"run one function\" mode and a more realistic streaming mode.\n- Keep it dev-only (lives under `scripts/` and/or `gen_worker/testing/`).",
      "tasks": [
        "[x] Decide packaging/entrypoint\n    - Implemented as a Python CLI module: `python -m gen_worker.testing.mock_orchestrator ...`\n    - Avoids adding an HTTP surface to the worker",
        "[x] Implement connection + auth\n    - Runs a gRPC server that the worker connects to (bidirectional stream)\n    - Captures and prints gRPC metadata (including auth header if present)",
        "[x] Implement the minimal gRPC protocol flow\n    - Accepts the worker ConnectWorker stream\n    - Sends `TaskExecutionRequest` and receives `TaskExecutionResult` and `WorkerEvent` messages",
        "[x] Function selection + payload encoding\n    - Reads function name + JSON payload from CLI args\n    - Encodes payload as msgpack (worker expects msgspec msgpack payloads)",
        "[x] Output handling\n    - Decodes msgpack output payload to JSON and prints it\n    - Prints streamed worker events when enabled",
        "[x] Provide runnable examples\n    - Added README section with commands for running against a worker container",
        "[x] Add a basic integration test\n    - Added a dev-only pytest that spawns a worker process and submits one task (skipped by default)"
      ],
      "completed": true
    },
    {
      "id": 61,
      "name": "Cozy Hub v2 model flow: resolve_artifact + snapshots/blobs cache + cozy.pipeline.lock.yaml",
      "description": "Implement the full Cozy Hub model flow in python-gen-worker so tenant functions only declare dependencies and do inference.\n\nWorker responsibilities:\n- Resolve `cozy:` model refs via Cozy Hub `resolve_artifact`.\n- Download the returned snapshot manifest (single manifest listing all files).\n- Cache content-addressed blobs locally (HF-style blobs store).\n- Materialize a snapshot checkout that looks like a normal diffusers repo.\n- Prefer `cozy.pipeline.lock.yaml` when present; fall back to `cozy.pipeline.yaml`.\n\nThis must be implemented in worker/library code, not in tenant functions.",
      "tasks": [
        "[x] Implement Cozy Hub `resolve_artifact` client\n    - Request: tag + ordered preferences + worker capabilities\n    - Response: `repo_revision_seq` number, `artifact.snapshot_digest`, `snapshot_manifest.files[]` (paths + blake3 + urls)\n    - Handle 409 'no compatible artifact' errors with a clear message",
        "[x] Define worker-side selection policy (hardcoded)\n    - Prefer file_type: flashpack \u2192 safetensors\n    - Prefer quantization: fp8 \u2192 bf16 (fp16 fallback)\n    - Prefer file_layout: diffusers\n    - Include capability detection (cuda version, gpu sm, installed libs) in requests",
        "[x] Implement local cache layout: snapshots + blobs\n    - Cache root: `${WORKER_MODEL_CACHE_DIR}/cozy/`\n    - Blobs: `${WORKER_MODEL_CACHE_DIR}/cozy/blobs/blake3/<aa>/<bb>/<digest>`\n    - Snapshots: `${WORKER_MODEL_CACHE_DIR}/cozy/snapshots/<snapshot_digest>/...`",
        "[x] Download blobs with verification + resume\n    - Download to `<digest>.part`, Range resume when supported, then atomic rename\n    - Verify `size_bytes` and BLAKE3 digest\n    - Singleflight: concurrent downloads for the same digest share one in-flight transfer",
        "[x] Materialize snapshot from snapshot manifest\n    - Create the exact file tree under `snapshots/<snapshot_digest>/` using `files[].path`\n    - Materialize each file from the blob store (prefer hardlink; fallback symlink/copy)\n    - Atomic activation: build in `.building` then rename",
        "[x] Lockfile-aware pipeline loading (future)\n    - Prefer `cozy.pipeline.lock.yaml` when present\n    - Fall back to `cozy.pipeline.yaml` when lockfile missing\n    - Ensure diffusers can load by generating `model_index.json` from the Cozy pipeline spec when missing",
        "[x] Integrate with the existing injection/model loading flow\n    - Model refs in `pyproject.toml` resolve to a local Cozy snapshot root directory\n    - Worker diffusers injection calls `from_pretrained(<snapshot_root>)`",
        "[x] Tests\n    - Mock Cozy Hub resolve_artifact + manifest responses (no network)\n    - Validate blobs cache, snapshot materialization, and concurrent download singleflight behavior",
        "[x] E2E Docker smoke test (manual)\n    - Start Cozy Hub + MinIO\n    - Run `sd15-worker` + `mock_orchestrator` and verify it downloads and generates `runs/<run_id>/outputs/image.png`"
      ],
      "completed": true
    },
    {
      "id": 62,
      "name": "Immediate cutover: endpoint-first public model with internal releases",
      "description": "Immediate cutover to project-aware endpoint routing and internal release-based runtime identity. No legacy behavior, no compatibility routes, and no migration backfills are required before launch.\n\nCanonical terms:\n- Public: `(tenant_slug, project_name, endpoint_name, tag)`\n- Internal runtime/build: `release_id` (random immutable ID per publish)\n- Internal execution symbol: `function_name` (implementation detail only)\n\nRelease and traffic model:\n- Every publish allocates a new random immutable `release_id`\n- A release contains 1..N image variants keyed by `(accelerator, accelerator_version, backend, backend_version, architecture, python_version)`\n- Backend currently supported: `pytorch` only\n- Each variant stores immutable `image_ref` and `image_digest`\n- Endpoints map to releases via pointers/tags\n- Invoke default: `POST /{tenant}/{project}/{endpoint}` resolves tag `prod`\n- Tagged invoke: `POST /{tenant}/{project}/{endpoint}@{tag}`\n- Publish does not move live traffic\n- Promote moves `prod` pointer explicitly\n- Bootstrap rule: if endpoint has no `prod` mapping yet, first successful publish sets `prod` to that release\n- Rollback is pointer flip to a previous release\n\nPublish workflow:\n1) Tenant publishes endpoint package\n2) Cozy Hub allocates `release_id`\n3) Builder builds/pushes required image variants to Docker Hub\n4) Cozy Hub records variant digests + release status\n5) Endpoint tags/pointers are updated per policy (non-live by default, explicit promote for live)\n\nSpec reference:\n- `agents/endpoint-release-v1-spec.md`\n\nScope:\n- `python-gen-worker`: manifest/config/runtime contract\n- `cozy-hub`: publish/build/release/pointer control plane\n- `gen-orchestrator`: invoke routing and tag resolution",
      "tasks": [
        "[x] Phase 0.1 - Confirm spec authority\n    - `agents/endpoint-release-v1-spec.md` is the source of truth",
        "[x] Phase 0.2 - Enforce immediate cutover policy\n    - No legacy routes, no compatibility shims, no backfill scope",
        "[x] Phase 0.3 - Freeze canonical terms\n    - Public: `tenant_slug`, `endpoint_name`\n    - Internal: `release_id`, `function_name`",
        "[x] Phase 0.4 - Define stable API error envelope\n    - Keep error codes/messages consistent across repos",
        "[x] Phase 1.1 - cozy-hub migration: create `releases` table",
        "[x] Phase 1.2 - cozy-hub migration: create `release_images` table",
        "[x] Phase 1.3 - cozy-hub migration: create `endpoint_pointers` table",
        "[x] Phase 1.4 - cozy-hub migration: add indexes/constraints from spec",
        "[x] Phase 1.5 - cozy-hub migration: enforce backend=`pytorch` for v1",
        "[x] Phase 1.6 - cozy-hub store: add `CreateRelease`",
        "[x] Phase 1.7 - cozy-hub store: add `SetReleaseStatus`",
        "[x] Phase 1.8 - cozy-hub store: add `UpsertReleaseImageVariant`",
        "[x] Phase 1.9 - cozy-hub store: add `GetRelease` + `ListReleaseImages`",
        "[x] Phase 1.10 - cozy-hub store: add `UpsertEndpointPointer`",
        "[x] Phase 1.11 - cozy-hub store: add `DeleteEndpointPointer` (non-`prod`)",
        "[x] Phase 1.12 - cozy-hub store: add `ListEndpointPointers`",
        "[x] Phase 1.13 - cozy-hub store: add `ListEndpointReleases`",
        "[x] Phase 1.14 - cozy-hub API: `POST /api/v1/endpoints/publish`",
        "[x] Phase 1.15 - cozy-hub API: `GET /api/v1/releases/:release_id`",
        "[x] Phase 1.16 - cozy-hub API: `GET /api/v1/releases/:release_id/logs`",
        "[x] Phase 1.17 - cozy-hub API: `GET /api/v1/endpoints`",
        "[x] Phase 1.18 - cozy-hub API: `GET /api/v1/endpoints/:endpoint_name`",
        "[x] Phase 1.19 - cozy-hub API: `GET /api/v1/endpoints/:endpoint_name/releases`",
        "[x] Phase 1.20 - cozy-hub API: `PUT /api/v1/endpoints/:endpoint_name/tags/:tag`",
        "[x] Phase 1.21 - cozy-hub API: `DELETE /api/v1/endpoints/:endpoint_name/tags/:tag`",
        "[x] Phase 1.22 - cozy-hub API: `POST /api/v1/endpoints/:endpoint_name/promote`",
        "[x] Phase 1.23 - cozy-hub publish flow: allocate random `release_id` on each publish",
        "[x] Phase 1.24 - cozy-hub publish flow: discover endpoint/function membership from manifest",
        "[x] Phase 1.25 - cozy-hub publish flow: queue and track release build lifecycle (`queued|building|ready|failed`)",
        "[x] Phase 1.26 - cozy-hub build flow: build variants by accelerator/backend/arch/python",
        "[x] Phase 1.27 - cozy-hub build flow: push variants to Docker Hub",
        "[x] Phase 1.28 - cozy-hub build flow: persist immutable `image_ref` + `image_digest`",
        "[x] Phase 1.29 - cozy-hub traffic policy: publish does not move `prod`",
        "[x] Phase 1.30 - cozy-hub traffic policy: bootstrap `prod` on first successful publish",
        "[x] Phase 1.31 - cozy-hub traffic policy: rollback via promote-to-older-release",
        "[x] Phase 1.32 - cozy-hub validation: enforce tenant isolation on all release/pointer operations",
        "[x] Phase 1.33 - cozy-hub validation: validate endpoint name and tag syntax",
        "[x] Phase 1.34 - cozy-hub cleanup: remove deployment-centric behavior from active builder/API/template paths",
        "[x] Phase 1.35 - cozy-hub UI: endpoint list with current `prod`",
        "[x] Phase 1.36 - cozy-hub UI: endpoint detail with full tag map",
        "[x] Phase 1.37 - cozy-hub UI: release history list per endpoint",
        "[x] Phase 1.38 - cozy-hub UI: per-variant status + digest display",
        "[x] Phase 1.39 - cozy-hub UI: promote action",
        "[x] Phase 1.40 - cozy-hub UI: tag create/update/delete controls",
        "[x] Phase 2.1 - gen-orchestrator parser: add `endpoint@tag` support in invoke path",
        "[x] Phase 2.2 - gen-orchestrator parser: default missing tag to `prod`",
        "[x] Phase 2.3 - gen-orchestrator resolver: `(tenant, endpoint, tag)` -> `release_id`",
        "[x] Phase 2.4 - gen-orchestrator resolver: `release_id` -> compatible variant selection",
        "[x] Phase 2.5 - gen-orchestrator runtime: carry `release_id` + `function_name` in execution mapping",
        "[x] Phase 2.6 - gen-orchestrator realtime: keep tag/default resolution consistent with invoke",
        "[x] Phase 2.7 - gen-orchestrator errors: explicit unknown endpoint/tag and no-compatible-variant responses",
        "[x] Phase 2.8 - gen-orchestrator cleanup: remove deployment-centric active invoke assumptions",
        "[x] Phase 3.1 - python-gen-worker config/docs: remove `[tool.cozy].deployment` requirement",
        "[x] Phase 3.2 - python-gen-worker runtime: replace deployment runtime wiring with `RELEASE_ID`",
        "[x] Phase 3.3 - python-gen-worker runtime: keep `function_name` as internal execution selector",
        "[x] Phase 3.4 - python-gen-worker discover: align manifest output to release artifact metadata",
        "[x] Phase 3.5 - python-gen-worker discover: keep endpoint/function metadata needed by control plane",
        "[x] Phase 3.6 - python-gen-worker terminology: replace deployment-scoped wording in README/examples",
        "[x] Phase 3.7 - python-gen-worker terminology: replace deployment-scoped wording in tests",
        "[x] Phase 4.1 - Cross-repo docs: publish vs promote lifecycle",
        "[x] Phase 4.2 - Cross-repo docs: default invoke and tagged invoke contract",
        "[x] Phase 4.3 - Cross-repo docs: rollback semantics as pointer flip",
        "[x] Phase 4.4 - Test: cozy-hub migrations/store for releases + pointers",
        "[x] Phase 4.5 - Test: cozy-hub publish/build/pointer APIs",
        "[x] Phase 4.6 - Test: gen-orchestrator default + tagged invoke routing",
        "[x] Phase 4.7 - Test: python-gen-worker release runtime wiring",
        "[x] Phase 4.8 - E2E: publish -> build/push -> promote -> invoke -> rollback",
        "[x] Phase 4.9 - Final verification: no deployment-legacy paths in active behavior"
      ],
      "completed": true
    },
    {
      "id": 63,
      "name": "Hard cutover: remove deployment-id/deployment concepts and use release-id only",
      "description": "Fully remove `deployment_id` and deployment-centric terminology/paths from active behavior across `gen-orchestrator`, `cozy-hub`, and `python-gen-worker`.\n\nTarget model:\n- Public entities: `tenant`, `endpoint`, `tag`\n- Runtime/build identity: `release_id` (UUID, immutable per publish)\n- Routing: `tenant + endpoint + tag -> release_id -> function_name`\n\nRules:\n- No legacy aliases, no compatibility fallbacks, no `/deployments` API surfaces\n- No `Deployment*` runtime structs in active codepaths\n- No env vars named `DEPLOYMENT_ID` in active worker/build runtime\n- All schemas/protos/API payloads/docs use `release_id` naming",
      "tasks": [
        "[x] Phase 0.1 - Freeze naming and scope\n    - Canonical names: tenant, endpoint, tag, release_id, function_name\n    - Explicitly reject deployment-centric naming in new code",
        "[x] Phase 0.2 - Add migration strategy decision\n    - Since pre-launch, prefer destructive rename/drop over compatibility columns/routes",
        "[x] Phase 0.3 - Define acceptance criteria\n    - `rg deployment_id|DeploymentID|/deployments` returns zero in active code/docs for all 3 repos",
        "[x] Phase 1.1 - gen-orchestrator proto: rename `frontend.ExecuteActionRequest.deployment_id` -> `release_id`",
        "[x] Phase 1.2 - gen-orchestrator proto: rename `frontend.RealtimeOpen.deployment_id` -> `release_id`",
        "[x] Phase 1.3 - gen-orchestrator proto: rename `scheduler.WorkerResources.deployment_id` -> `release_id`",
        "[x] Phase 1.4 - Regenerate orchestrator protobuf outputs (Go) and python-gen-worker protobuf outputs (Python)",
        "[x] Phase 1.5 - gen-orchestrator HTTP invoke/realtime: pass and log `release_id` only",
        "[x] Phase 1.6 - gen-orchestrator scheduler runtime: rename Action/Worker state fields from Deployment* to Release*",
        "[x] Phase 1.7 - gen-orchestrator DB schema: rename `published_endpoints.deployment_id` -> `release_id`",
        "[x] Phase 1.8 - gen-orchestrator DB schema: rename `orchestrator_jobs.deployment_id` -> `release_id`",
        "[x] Phase 1.9 - gen-orchestrator DB schema: rename `worker_heartbeats.deployment_id` and `deployment_metrics.deployment_id` -> `release_id`",
        "[x] Phase 1.10 - gen-orchestrator DB schema: replace `models.deployment_id` references with `models.release_id`",
        "[x] Phase 1.11 - gen-orchestrator API: remove `/v1/deployments` routes and handlers",
        "[x] Phase 1.12 - gen-orchestrator scheduler persistence: remove `GetDeployment*` APIs and rename to `GetRelease*`",
        "[x] Phase 1.13 - gen-orchestrator tests: update all fixtures/assertions to `release_id` naming",
        "[x] Phase 1.14 - gen-orchestrator scripts/docs: remove deployment wording and examples",
        "[x] Phase 2.1 - cozy-hub API: remove legacy deployment builder routes (`/api/v1/deployments/*`) from active surface",
        "[x] Phase 2.2 - cozy-hub builder domain/store: remove `Deployment`, `DeploymentBuild`, and `SetDeploymentID` active usage",
        "[x] Phase 2.3 - cozy-hub build records: replace `builds.deployment_id` with `builds.release_id` in active path",
        "[x] Phase 2.4 - cozy-hub template/runtime env: replace `COZY_DEPLOYMENT_ID` with `COZY_RELEASE_ID`",
        "[x] Phase 2.5 - cozy-hub secrets API/storage: remove deployment-scoped bindings or rename to release-scoped equivalents",
        "[x] Phase 2.6 - cozy-hub registry/billing schemas/APIs: rename `deployment_id` fields to `release_id` where still active",
        "[x] Phase 2.7 - cozy-hub frontend API client/types: replace deployment_id/deployments paths with release-oriented fields",
        "[x] Phase 2.8 - cozy-hub frontend pages: remove deployment-centric pages/labels still wired to active nav",
        "[x] Phase 2.9 - cozy-hub migrations: add destructive cutover migration removing deployment-era columns/tables not in v1 design",
        "[x] Phase 2.10 - cozy-hub tests: update unit/e2e tests to release-first terminology and paths",
        "[x] Phase 3.1 - python-gen-worker runtime: remove `DEPLOYMENT_ID` fallback; require/use only `RELEASE_ID`",
        "[x] Phase 3.2 - python-gen-worker runtime: replace remaining deployment wording in errors/logs/comments",
        "[x] Phase 3.3 - python-gen-worker mock_orchestrator: rename session/request fields from deployment_id to release_id",
        "[x] Phase 3.4 - python-gen-worker tests: remove deployment fallback test and update expectations",
        "[x] Phase 3.5 - python-gen-worker docs: remove `[tool.cozy].deployment` mentions and legacy wording",
        "[x] Phase 3.6 - python-gen-worker generated pb files: verify only `release_id` fields are referenced",
        "[x] Phase 4.1 - Cross-repo grep gate\n    - Add CI/static check to fail on `deployment_id`, `DeploymentID`, `/deployments` in active code/docs",
        "[x] Phase 4.2 - E2E validation\n    - publish -> build -> push -> promote -> invoke default/tagged -> rollback with release_id-only contract",
        "[x] Phase 4.3 - Final scrub\n    - remove deployment-centric examples/scripts and stale docs across all 3 repos",
        "[x] Phase 4.4 - Final verification summary\n    - capture exact commands + outputs proving zero deployment-id usage in active paths"
      ],
      "completed": true
    },
    {
      "id": 68,
      "name": "Worker-Reported Perf Metrics v1 (best-effort, optional fields)",
      "description": "Report per-run performance/debug metrics from the python gen worker to gen-orchestrator via `WorkerEvent` messages.\n\nThese metrics are **best-effort** (must never fail a run) but are essential for debugging cold starts, cache behavior, and inference performance.\n\nKey design requirements:\n- Metrics MUST be safe to expose (numbers + small strings only; no secrets, URLs, or file paths).\n- Metrics MUST be optional: many functions are not diffusion, not torch, or do not have separable stages.\n- Warmup timing matters only on the first warmup run (per model/pipeline load); otherwise omit/null.\n- `iters_per_s` is diffusion-only and should be omitted/null when `steps` is unknown/not applicable.\n\n## Transport (worker -> orchestrator)\n\nUse the existing gRPC stream `WorkerSchedulerMessage.worker_event` (no proto changes required):\n- `WorkerEvent.run_id = <run_id>`\n- `WorkerEvent.event_type = metrics.*` (and compute.* / tokens as needed)\n- `WorkerEvent.payload_json = JSON`\n\nEmit a small set of canonical events that gen-orchestrator already recognizes for DB columns:\n- `metrics.fetch` payload: `{ \"ms\": <download_ms> }` (0 if warm disk hit)\n- `metrics.gpu_load` payload: `{ \"ms\": <gpu_load_ms> }`\n- `metrics.inference` payload: `{ \"ms\": <inference_ms> }`\n- `metrics.compute.started` payload: `{ \"at\": <rfc3339> }`\n- `metrics.compute.completed` payload: `{ \"at\": <rfc3339> }`\n- `metrics.tokens` payload: `{ \"output_tokens\": <n> }` (when applicable)\n\nAlso emit a single **extended** debug event (stored in job_events) with the full payload:\n- `metrics.run` payload: schema below (fields optional)\n\n## Proposed `metrics.run` payload (schema_version=1)\n\nTop-level keys (all optional unless noted):\n- `schema_version` (required): `1`\n- `function_name`: string\n- `cache_state`: `hot_vram | warm_disk | cold_remote` (best-effort summary across required models)\n\nModel resolve/download (per model, best-effort):\n- `models`: array of objects:\n  - `model_id`: canonical id/key used by scheduler/worker (string)\n  - `variant_label`: string|null\n  - `snapshot_digest`: string|null\n  - `cache_state`: `hot_vram | warm_disk | cold_remote`|null\n  - `bytes_downloaded`: int|null\n  - `download_ms`: int|null (0 for warm disk hit)\n  - `bytes_read_disk`: int|null (optional; ok to omit if unknown)\n\nModel init (per model and/or per run, best-effort):\n- `pipeline_init_ms`: int|null (e.g. `from_pretrained` / pipeline construction)\n- `gpu_load_ms`: int|null (moving weights to GPU, enabling offload, compile)\n- `warmup_ms`: int|null (only for the first warmup run; otherwise omit/null)\n\nInference (diffusion-only extras are optional):\n- `inference_ms`: int|null (sampling call or best-effort approximation)\n- `steps`: int|null\n- `iters_per_s`: float|null (compute as `steps / (inference_ms/1000)` when both known)\n- `width`: int|null\n- `height`: int|null\n- `guidance`: float|null\n\nPost:\n- `png_encode_ms`: int|null (only if measured)\n- `upload_ms`: int|null (time spent in file API upload via ctx.save_*)\n\nResources:\n- `peak_vram_bytes`: int|null (e.g. `torch.cuda.max_memory_allocated()` after reset)\n- `peak_ram_bytes`: int|null (best-effort; ok to use `resource.getrusage().ru_maxrss` on linux)\n\n## Implementation notes\n- Prefer emitting timings in milliseconds as integers.\n- Omit keys entirely when unknown, rather than emitting misleading zeros.\n- Never block job completion on emitting metrics; wrap emission in try/except.\n\nCross-repo dependency note:\n- gen-orchestrator currently persists `metrics.fetch|gpu_load|inference` and `metrics.compute.*` into `orchestrator_jobs` columns via `RecordRunMetricsEvent()`. Everything else is still valuable via `job_events` for debugging.\n",
      "tasks": [
        "[x] Define the metrics contract and add docs\n    - Documented `metrics.*` event types and `metrics.run` payload schema in `docs/metrics.md` and linked from `README.md`",
        "[x] Add per-run metrics collector in the worker core\n    - Added `RunMetricsV1` (monotonic timers + optional fields) in `src/gen_worker/run_metrics_v1.py`\n    - Captures `function_name`, required model ids, and coarse `cache_state` best-effort",
        "[x] Instrument model materialization (resolve/download)\n    - Best-effort detect `hot_vram|warm_disk|cold_remote` using ModelCache + snapshot dir checks\n    - Record `bytes_downloaded` (estimated from missing blobs) + `download_ms` (0 on warm disk hit) when downloads occur\n    - Record `snapshot_digest` when available from orchestrator-resolved manifests",
        "[x] Instrument model init + warmup\n    - Record `pipeline_init_ms` around `from_pretrained(...)`\n    - Record `gpu_load_ms` around best-effort `.to(device)` moves\n    - `warmup_ms` is omitted unless explicitly measured",
        "[x] Instrument inference + diffusion extras (optional)\n    - Record `inference_ms` (best-effort) around user function execution\n    - Best-effort extract diffusion-like fields: `steps`, `guidance`, `width`, `height`\n    - Compute `iters_per_s` only when `steps` and `inference_ms` are both known",
        "[x] Instrument output upload time\n    - Measure and accumulate `upload_ms` inside `ActionContext.save_*` helpers\n    - Thread accumulator back via per-run `RunMetricsV1` attached to ActionContext",
        "[x] Resource peaks (best-effort)\n    - If torch+cuda available: reset peak stats at run start; record `peak_vram_bytes` at end\n    - Record `peak_ram_bytes` best-effort via `resource.getrusage(...).ru_maxrss`",
        "[x] Emit metrics events to gen-orchestrator\n    - Emit canonical events: `metrics.fetch`, `metrics.gpu_load`, `metrics.inference`, `metrics.compute.started`, `metrics.compute.completed`\n    - Emit `metrics.run` once at the end with the extended payload\n    - Emission is best-effort and never fails the run",
        "[x] Tests\n    - Added unit tests for metrics JSON shape and canonical events: `tests/test_run_metrics_v1.py`\n    - Updated protobuf message filtering in `tests/test_signature_contract_and_incremental.py` to ignore non-output events"
      ],
      "completed": true
    },
    {
      "id": 69,
      "name": "NFS vs Local Disk Detection + NFS->NVMe Localization (Keep NFS Warm Cache)",
      "description": "Runpod Network Volumes (mounted at `/workspace`) behave like NFS: they are great for cross-pod reuse, but loading multi-GB weights from them is still slower than local NVMe.\n\nGoal:\n- The worker can determine whether a model snapshot directory is on NFS vs local storage.\n- If the snapshot is on NFS, the worker copies it to a local (non-NFS) cache (NVMe/ephemeral disk) and loads the model from that local path.\n- The worker reports the *effective* disk backend used (local vs NFS) back to gen-orchestrator (best-effort) for debugging/perf attribution.\n- When a model is first downloaded from Cozy Hub (R2) and it was not already present on the Runpod Network Volume, the worker must ensure the materialized snapshot is written to the shared volume so future pods can warm-start without re-downloading over the public internet.\n\nNon-goals (v1):\n- Perfect device characterization (\"is this *actually* NVMe\"); we only need reliable NFS vs non-NFS detection plus measured read/copy timings.\n\n## Detection\n\nImplement a small helper that maps a path -> mount backend by parsing `/proc/self/mountinfo`:\n- `fstype` and `source` for the mount that contains the path\n- `is_nfs = fstype in {\"nfs\",\"nfs4\"}` (and potentially other network fs types)\n\nThis allows answering: \"is `<model_dir>` on NFS or local?\" without any Runpod-specific API.\n\n## Localization (NFS -> local)\n\nIntroduce/standardize a local cache directory that is expected to be on non-NFS storage:\n- env: `WORKER_LOCAL_MODEL_CACHE_DIR` (default: `/tmp/cozy/local-model-cache`)\n- size cap: reuse `WORKER_LOCAL_CACHE_GB` (already used by `LocalModelCache`)\n\nWhen loading a model snapshot:\n1. Resolve/download into `WORKER_MODEL_CACHE_DIR` first (this is the shared cache root; on Runpod it should be under `/workspace/...`).\n2. Detect mount backend for the resolved snapshot dir.\n3. If `is_nfs` and local cache dir is non-NFS: copy snapshot -> local cache (atomic, LRU eviction).\n4. Load from the local cache path.\n\nIf local cache is unavailable or on NFS (misconfigured), fall back to loading from the original path and report `disk_backend=\"nfs\"`.\n\n## Shared-cache materialization (R2 -> NFS)\n\nFor cold starts where the model is not already on the Runpod Network Volume:\n- Ensure the *download destination* is under `WORKER_MODEL_CACHE_DIR`.\n- Only after the snapshot exists under `WORKER_MODEL_CACHE_DIR` should we emit `model.cached` (volume inventory signal).\n- NVMe localization must be a copy on top of the NFS snapshot, never the only copy.\n\n## Reporting to gen-orchestrator\n\nUse existing `WorkerEvent` (no proto changes):\n- Extend `metrics.run` payload (issue #68) to include per-model disk backend info:\n  - `models[].disk_fstype` (string|null)\n  - `models[].disk_backend` (`\"local\"|\"nfs\"`|null)\n  - `models[].localized` (bool|null)\n  - `models[].nfs_to_local_copy_ms` (int|null)\n  - Optionally: `models[].bytes_read_disk` / `bytes_copied`\n\nThis is best-effort and must never fail a run.\n\n## Implementation touchpoints\n- `src/gen_worker/run_metrics_v1.py`: add optional fields above\n- `src/gen_worker/worker.py`: when resolving injected diffusers pipelines, detect backend and localize before `from_pretrained`\n- `src/gen_worker/pipeline_loader.py`: upgrade `LocalModelCache` usage from background-only prefetch to on-demand copy when source is NFS\n\n## Tests\n- Unit test mountinfo parsing and `is_nfs_path()` classification (sample mountinfo fixtures)\n- Unit test that localization chooses the local path when source is NFS and local cache dir is non-NFS\n- Unit test that cold downloads still materialize into `WORKER_MODEL_CACHE_DIR` before localization",
      "tasks": [
        "[x] Add mount backend helper\n    - Parse `/proc/self/mountinfo`\n    - `mount_backend_for_path(path) -> {fstype, source, mountpoint, is_nfs}`",
        "[x] Standardize local cache directory env\n    - Add `WORKER_LOCAL_MODEL_CACHE_DIR` and ensure it defaults to a non-NFS location\n    - Validate and log a warning if the configured local cache dir is on NFS",
        "[x] Implement on-demand NFS->local copy for model snapshots\n    - Use `LocalModelCache.cache_model(...)` for atomic copy + LRU eviction\n    - Measure copy time and (best-effort) bytes copied",
        "[x] Ensure cold downloads land on NFS when available\n    - Always download into `WORKER_MODEL_CACHE_DIR` (shared cache root)\n    - Emit `model.cached` only after NFS snapshot exists",
        "[x] Wire reporting into `metrics.run`\n    - Record disk backend fields per model\n    - Include `nfs_to_local_copy_ms` when localization occurs",
        "[x] Tests\n    - mountinfo parser tests\n    - localization path selection tests\n    - download destination invariants"
      ],
      "completed": true
    },
    {
      "id": 70,
      "name": "NFS Volume Inventory Signals (Which Shared Cache Has Which Models)",
      "description": "Gen-orchestrator needs to know which shared disk backends (e.g. NFS volumes) have which models materialized, so it can attribute perf and (future) route work intelligently.\n\nRequirement:\n- Without proto changes, the worker must report a stable identifier for the mount that backs `WORKER_MODEL_CACHE_DIR`.\n- The worker must emit inventory events that associate that mount identity with the current `disk_models` set.\n- Inventory must be best-effort and must never fail a run.\n\nSecurity:\n- Do not emit raw mount sources (may contain internal IPs). Emit a hash key only.\n",
      "tasks": [
        "[x] Add stable mount identity key helper\n    - Implement `volume_key_for_path(...)` in `src/gen_worker/mount_backend.py` (sha256 of fstype/source/mountpoint)",
        "[x] Emit disk inventory events with mount identity\n    - Emit `models.disk_inventory` (run_id=\"\") on heartbeat when the `disk_models` set changes\n    - Payload includes `disk_backend`, `disk_fstype`, `disk_volume_key`, and `disk_models`",
        "[x] Extend `model.cached` payload with mount identity\n    - Payload includes `disk_backend`, `disk_fstype`, `disk_volume_key` so orchestrator can attribute cache hits to the shared volume",
        "[x] Tests + docs\n    - Added unit coverage for `volume_key_for_path` in `tests/test_mount_backend.py`\n    - Documented `model.cached` / `models.disk_inventory` in `docs/metrics.md`"
      ],
      "completed": true
    },
    {
      "id": 64,
      "name": "Add project-aware public invoke path: tenant/project/endpoint",
      "description": "Introduce project-scoped endpoint invocation so public inference routes become `POST /{tenant}/{project}/{endpoint}` (with optional `@tag` on endpoint). Project is derived from `pyproject.toml [project].name`, normalized to a URL-safe slug, and must be carried through worker manifest, cozy-hub endpoint pointers, and gen-orchestrator published endpoint resolution.\n\nMotivation:\n- `tenant/endpoint` is ambiguous for multi-project tenants.\n- `tenant/project/endpoint` maps cleanly to how tenant code is packaged and published.\n\nScope:\n- `python-gen-worker`: parse and emit project slug in manifest.\n- `cozy-hub`: store/retrieve endpoint pointers keyed by `(tenant, project, endpoint, tag)` and expose project-aware release APIs/UI.\n- `gen-orchestrator`: parse 3-segment invoke/realtime paths and resolve published endpoint rows keyed by `(publisher, project, endpoint, tag)`.\n\nOut of scope for this issue:\n- Revenue/billing model changes.\n- Worker execution protocol changes unrelated to endpoint resolution.",
      "tasks": [
        "[x] Phase 0.1 - Freeze canonical URL contract\n    - Public invoke default: `POST /{tenant}/{project}/{endpoint}` (resolves `tag=prod`)\n    - Tagged invoke: `POST /{tenant}/{project}/{endpoint}@{tag}`\n    - Realtime WS: `GET /v1/realtime/{tenant}/{project}/{endpoint_or_endpoint@tag}/ws?token=...`",
        "[x] Phase 0.2 - Freeze project slug normalization rules\n    - Source: `pyproject.toml [project].name`\n    - Normalization algorithm (matches worker): `strip -> lower -> '_' => '-' -> non-[a-z0-9.] => '-' -> collapse '--' -> trim '-.' -> maxlen 128 then trim '-.'`\n    - Reject empty result after normalization",
        "[x] Phase 0.3 - Decide migration compatibility policy\n    - Chosen: Option A strict cutover (no legacy 2-segment invoke)\n    - Policy: `/{tenant}/{endpoint}` returns 404 (no fallback)\n    - Note: existing pointers/releases must be republished to gain correct `project_name`",
        "[x] Phase 0.4 - Add cross-repo glossary update\n    - Public identity is `(tenant_slug, project_name, endpoint_name, tag)`\n    - Internal execution remains `(release_id, function_name)`",
        "[x] Phase 1.1 - python-gen-worker: parse `[project].name` in discover config loader\n    - File: `src/gen_worker/discover.py`\n    - Read `project.name` and compute normalized `project_name`",
        "[x] Phase 1.2 - python-gen-worker: emit `project_name` in manifest root\n    - File: `src/gen_worker/discover.py`\n    - Manifest contract includes top-level `project_name` next to `functions/build/resources/models`",
        "[x] Phase 1.3 - python-gen-worker: validate project name presence/validity\n    - File: `src/gen_worker/project_validation.py`\n    - Require `[project].name` and produce clear errors for missing/invalid values",
        "[x] Phase 1.4 - python-gen-worker: add slugify helper tests for project name edge cases\n    - Uppercase, spaces, underscores, repeated separators, unicode/symbol stripping, max-length trimming",
        "[x] Phase 1.5 - python-gen-worker: add manifest tests for project propagation\n    - Update/add tests around `discover_manifest()` to assert `project_name` is present and stable",
        "[x] Phase 1.6 - python-gen-worker: update docs/examples invoke contract\n    - File: `README.md`\n    - Replace `tenant/endpoint` references with `tenant/project/endpoint`",
        "[x] Phase 1.7 - python-gen-worker: update any spec docs in `agents/` that define old invoke shape\n    - Updated `agents/endpoint-release-v1-spec.md` invoke examples to include `{project}`\n    - Updated issue #62 description to include `{project}` for invoke examples",
        "[x] Phase 1.8 - python-gen-worker: verify Dockerfile manifest generation includes new field without extra flags\n    - File: `Dockerfile` runs `gen_worker.discover` at build time; `project_name` is emitted automatically\n    - Note: worker now requires `[project].name` (validated during manifest generation)",
        "[x] Phase 2.1 - cozy-hub DB migration: add `project_name` to `endpoint_pointers`\n    - Added `migrations/postgres/006_endpoint_pointers_project_name.(up|down).sql`\n    - Backfill strategy: default `project_name='default'` for existing rows",
        "[x] Phase 2.2 - cozy-hub DB migration: update pointer primary key/unique constraints\n    - Old: `(tenant_id, endpoint_name, tag)`\n    - New: `(tenant_id, project_name, endpoint_name, tag)`",
        "[x] Phase 2.3 - cozy-hub DB migration: update supporting indexes\n    - Added `(tenant_id, project_name, endpoint_name)` lookup index\n    - Preserved `release_id` lookup index",
        "[x] Phase 2.4 - cozy-hub domain model: add `ProjectName` to `EndpointPointer`\n    - File: `internal/builder/domain.go`",
        "[x] Phase 2.5 - cozy-hub store layer: update pointer CRUD signatures to include project\n    - File: `internal/builder/store.go`\n    - `UpsertEndpointPointer`, `DeleteEndpointPointer`, `ListEndpointPointers`, `ListEndpointKeys`, `ListEndpointReleases`, `FindAnyReleaseForEndpoint`",
        "[x] Phase 2.6 - cozy-hub store SQL: include `project_name` in all pointer queries and scans",
        "[x] Phase 2.7 - cozy-hub builder manifest extraction: read project from release manifest\n    - File: `internal/builder/executor.go`\n    - Endpoint extraction helper now returns `(project_name, endpoint_name)` via `extractEndpointKeysFromManifest`",
        "[x] Phase 2.8 - cozy-hub bootstrap behavior: first-publish prod pointer checks become project-aware\n    - Bootstrap checks are now per `(tenant, project, endpoint)`",
        "[x] Phase 2.9 - cozy-hub API route design: make endpoint management project-aware\n    - Updated routes in `internal/api/endpoints_release.go` to include `:project_name/:endpoint_name`",
        "[x] Phase 2.10 - cozy-hub API handlers: validate `project_name` using canonical slug rules\n    - Added `builder.NormalizeProjectName` / `builder.IsValidProjectName` and enforced in handlers",
        "[x] Phase 2.11 - cozy-hub API responses: include `project_name` in list/detail payloads\n    - `GET /api/v1/endpoints` and `GET /api/v1/endpoints/:project/:endpoint` include `project_name`",
        "[x] Phase 2.12 - cozy-hub release membership checks: keep endpoint membership validation but bind pointer writes to project too\n    - `releaseContainsEndpoint(...)` now enforces manifest `project_name` matches request project",
        "[x] Phase 2.13 - cozy-hub frontend API client: add `project_name` to endpoint types and request builders\n    - File: `frontend/src/lib/api.ts`",
        "[x] Phase 2.14 - cozy-hub frontend page: key endpoint selection by `(project_name, endpoint_name)`\n    - File: `frontend/src/pages/EndpointsPage.tsx`",
        "[x] Phase 2.15 - cozy-hub frontend UX: display full public route preview `tenant/project/endpoint`\n    - File: `frontend/src/pages/EndpointsPage.tsx` (uses active org from settings)",
        "[x] Phase 2.16 - cozy-hub tests: update store tests for new pointer key shape\n    - File: `internal/builder/store_release_test.go` and related",
        "[x] Phase 2.17 - cozy-hub tests: update endpoint API unit tests for project-aware params\n    - File: `internal/api/endpoints_release_test.go`",
        "[x] Phase 2.18 - cozy-hub e2e tests: update publish/promote/list flows to include project\n    - File: `internal/e2e/endpoints_publish_build_test.go`",
        "[x] Phase 2.19 - cozy-hub docs: update API examples to show project-aware endpoint management and invoke shape\n    - Updated `cozy-hub/README.md` endpoint management routes to include `:project_name/:endpoint_name`\n    - Updated Cozy Hub public identity description to include `project_name` (from `pyproject.toml [project].name`)",
        "[x] Phase 3.1 - gen-orchestrator DB migration: add `project` column to `published_endpoints`\n    - Added `migrations/postgres/002_published_endpoints_project.up.sql`",
        "[x] Phase 3.2 - gen-orchestrator DB migration: update PK/indexes\n    - Old PK: `(publisher, endpoint, tag)`\n    - New PK: `(publisher, project, endpoint, tag)`\n    - Updated `published_endpoints_lookup_idx` accordingly",
        "[x] Phase 3.3 - gen-orchestrator seed data: include `project` in test/dev fixtures\n    - File: `testdata/postgres/seed_dev.sql`",
        "[x] Phase 3.4 - gen-orchestrator path parser: update `splitInvokePath` to parse 3 segments\n    - File: `pkg/httpapi/invoke.go`\n    - Parse `/{publisher}/{project}/{endpoint[@tag]}`",
        "[x] Phase 3.5 - gen-orchestrator invoke resolver: include project in endpoint lookup queries\n    - File: `pkg/httpapi/invoke.go`\n    - `loadPublishedEndpoint` and `publishedEndpointExists`",
        "[x] Phase 3.6 - gen-orchestrator handler signatures: pass project through invoke stack\n    - File: `pkg/httpapi/server.go` -> `handleInvoke(...)`",
        "[x] Phase 3.7 - gen-orchestrator realtime token API: require/store project claim\n    - File: `pkg/httpapi/realtime.go`\n    - Request payload includes `project`; signed claims include `project`",
        "[x] Phase 3.8 - gen-orchestrator realtime websocket parser: parse project segment and enforce claim binding\n    - File: `pkg/httpapi/realtime.go`",
        "[x] Phase 3.9 - gen-orchestrator realtime endpoint lookup: include project in resolution queries\n    - File: `pkg/httpapi/realtime.go`",
        "[x] Phase 3.10 - gen-orchestrator tests: update invoke path helper tests\n    - File: `pkg/httpapi/invoke_helpers_test.go`",
        "[x] Phase 3.11 - gen-orchestrator tests: update invoke error tests for project-aware lookups\n    - File: `pkg/httpapi/invoke_error_responses_test.go`",
        "[x] Phase 3.12 - gen-orchestrator tests: update invoke execution tests with 3-segment path\n    - Files: `pkg/httpapi/invoke_release_variant_test.go`, `pkg/httpapi/invoke_longpoll_bytes_test.go`",
        "[x] Phase 3.13 - gen-orchestrator tests: update realtime tests for project claim/path\n    - File: `pkg/httpapi/realtime_test.go`",
        "[x] Phase 3.14 - gen-orchestrator e2e: update cluster tests hitting `/demo/generate`\n    - File: `internal/e2e/cluster9_test.go`",
        "[x] Phase 3.15 - gen-orchestrator docs: update README invoke/realtime contract\n    - File: `README.md`",
        "[x] Phase 3.16 - gen-orchestrator docs: update API docs examples for realtime token + websocket path\n    - File: `docs/api.md`",
        "[x] Phase 4.1 - Integration: published_endpoints writer is deprecated\n    - gen-orchestrator resolves endpoint pointers via Cozy Hub API when configured\n    - Local `published_endpoints` is legacy/dev-only; no active writer path is required\n    - Legacy seed writer exists at `~/cozy/e2e/orchestrator/seed.sql` (not used by the current e2e docker-compose); retire/remove separately",
        "[x] Phase 4.2 - Cross-repo slug consistency test\n    - Cozy Hub: added tests for `NormalizeProjectName` parity (file: `internal/builder/project_name_test.go`)\n    - Gen-orchestrator: added `isValidPublicName`/`isValidPublicTag` validation + tests (file: `pkg/httpapi/invoke_helpers_test.go`)\n    - Worker already tests slugification cases (file: `tests/test_discover_endpoint_names.py`)",
        "[x] Phase 4.3 - Backfill strategy execution (if compatibility mode chosen)\n    - N/A: strict cutover policy chosen (no compatibility/backfill scope)\n    - Schema migrations still default existing rows to `project='default'` / `project_name='default'` for DB integrity",
        "[x] Phase 4.4 - Compatibility route policy implementation\n    - Strict 3-segment invoke path only; legacy `/{tenant}/{endpoint}` does not match `splitInvokePath` and returns 404",
        "[x] Phase 4.5 - Observability updates\n    - Gen-orchestrator logs now include `publisher/project/endpoint/tag` on invoke upstream errors (file: `pkg/httpapi/invoke.go`)",
        "[x] Phase 4.6 - Security review\n    - Path parsing rejects malformed/extra segments and validates tokens as URL-safe slugs\n    - Realtime JWT binding includes `project` claim and enforces it against the WS path",
        "[x] Phase 5.1a - Verification gate: python-gen-worker tests pass\n    - `uv run pytest -q`",
        "[x] Phase 5.1b - Verification gate: cozy-hub tests pass\n    - `go test ./...`",
        "[x] Phase 5.1c - Verification gate: gen-orchestrator tests pass\n    - `go test ./...`",
        "[x] Phase 5.2 - Verification gate: docs and examples updated consistently across 3 repos\n    - python-gen-worker: README + agents spec updated\n    - cozy-hub: README + frontend route preview updated\n    - gen-orchestrator: README + docs/api.md updated",
        "[x] Phase 5.3 - Final grep gate (no stale public route examples)\n    - Verified no remaining `POST /{tenant}/{endpoint}` docs in the 3 repos (besides historical/migration notes)\n    - Updated stale realtime path examples in `gen-orchestrator/agents/progress.json`",
        "[x] Phase 5.4 - Final acceptance demo\n    - Covered by updated unit/e2e tests in cozy-hub + gen-orchestrator for 3-segment invoke path"
      ],
      "completed": true
    }
  ]
}
