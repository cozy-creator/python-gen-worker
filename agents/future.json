{
  "description": "Future work items for python-worker (not planned for immediate implementation)",
  "issues": [
    {
      "id": 200,
      "name": "Multi-backend support (PyTorch, TensorRT, ONNX Runtime)",
      "description": "Allow tenants to specify which ML backend(s) they need in [tool.cozy.build]. Currently PyTorch is assumed. Support TensorRT and ONNX Runtime as alternatives or additions. This affects base image selection and what gets installed.",
      "config_schema": {
        "example": "[tool.cozy.build]\ngpu = true\ncuda = \">=12.6\"\nbackends = [\"pytorch\", \"tensorrt\"]  # or just \"onnxruntime\"\n\n# Optional version constraints per backend\ntorch = \">=2.9\"\ntensorrt = \">=10.0\"\nonnxruntime = \">=1.17\"",
        "fields": {
          "backends": "List of backends to install. Options: 'pytorch', 'tensorrt', 'onnxruntime'. Default: ['pytorch']. Can be a single string or list.",
          "torch": "PyTorch version constraint (semver). Only used if 'pytorch' in backends.",
          "tensorrt": "TensorRT version constraint. Only used if 'tensorrt' in backends.",
          "onnxruntime": "ONNX Runtime version constraint. Only used if 'onnxruntime' in backends."
        },
        "notes": "If backends is omitted, defaults to ['pytorch'] for backward compatibility. Tenants who don't want PyTorch at all can specify backends = ['onnxruntime'] or backends = ['tensorrt']."
      },
      "backend_details": {
        "pytorch": {
          "packages": ["torch", "torchvision", "torchaudio"],
          "size": "~2.2 GB download, ~5-6 GB installed",
          "use_cases": "Training, general inference, diffusers pipelines, most ML workloads"
        },
        "tensorrt": {
          "packages": ["tensorrt-cu12"],
          "size": "~1.1 GB download, ~1.5 GB installed",
          "use_cases": "Optimized NVIDIA inference, 2-10x faster than PyTorch for supported models"
        },
        "onnxruntime": {
          "packages": ["onnxruntime-gpu"],
          "size": "~900 MB download, ~1.2 GB installed",
          "use_cases": "Cross-platform inference, universal model format, good for exported models"
        }
      },
      "tasks": [
        "[ ] Update discover.py to parse 'backends' field from [tool.cozy.build]",
        "[ ] Update discover.py to parse backend-specific version constraints (torch, tensorrt, onnxruntime)",
        "[ ] Include backends in manifest output for gen-builder consumption",
        "[ ] Update backends.py Protocol definitions if needed for runtime injection",
        "[ ] Add validation: warn if backend version constraint specified but backend not in list",
        "[ ] Update examples to show non-PyTorch configurations",
        "[ ] Document backend selection in README"
      ],
      "notes": "This pairs with gen-builder issue #101 (on-demand base image builds). python-worker parses the config and includes it in the manifest; gen-builder uses it to select or build the appropriate base image.",
      "priority": "medium"
    },
    {
      "id": 201,
      "name": "Backend Optimizations (torch.compile, TensorRT Engines, ONNX, Candle)",
      "description": "Define a forward-compatible way for tenants to request performance optimizations and/or alternative execution backends.\n\nScope (future):\n- PyTorch optimization toggles (torch.compile, SDPA/FlashAttention, xformers, CUDA graphs where applicable)\n- Export/compile flows (PyTorch -> ONNX, PyTorch -> TensorRT engine)\n- Non-PyTorch runtime option: HuggingFace Candle (when/if a usable Python runtime story exists)\n\nConstraints:\n- Optimizations must be optional and safe-by-default. If an optimization fails, fall back to the baseline backend.\n- Compilation artifacts should be cacheable and shareable (per release + variant), ideally persisted under the shared model cache root (e.g. Runpod `/workspace`).\n- No secrets/paths in telemetry; only timing + small identifiers.",
      "config_schema": {
        "example": "[tool.cozy.build]\n# One of: pytorch, onnxruntime, tensorrt, candle\nbackend = \"pytorch\"\n\n[tool.cozy.build.optimize]\n# PyTorch-only\ntorch_compile = true\ntorch_compile_mode = \"max-autotune\"   # or \"default\", \"reduce-overhead\"\ntorch_compile_dynamic = false\n\n# Attention/memory kernels (backend-dependent)\nflash_attn = true\nxformers = false\n\n# Export/compile (future)\nexport_onnx = false\nbuild_tensorrt_engine = false\n\n# Cache compiled artifacts (per release+variant)\nartifact_cache = \"shared\"   # shared|local|off",
        "fields": {
          "backend": "Execution backend. Default: 'pytorch'. Future: 'onnxruntime', 'tensorrt', 'candle'.",
          "optimize": "Optimization options. Backend-specific; ignored when not applicable.",
          "optimize.torch_compile": "Enable torch.compile (PyTorch only). Must gracefully fall back if compile fails.",
          "optimize.torch_compile_mode": "torch.compile mode hint (e.g. default/max-autotune/reduce-overhead).",
          "optimize.torch_compile_dynamic": "Whether to allow dynamic shapes (may reduce compilation cache hit rate).",
          "optimize.flash_attn": "Prefer FlashAttention / SDPA fastpath when available (backend-dependent).",
          "optimize.xformers": "Prefer xformers attention kernels when available (backend-dependent).",
          "optimize.export_onnx": "Request ONNX export as part of build/warmup (future).",
          "optimize.build_tensorrt_engine": "Request building a TensorRT engine as part of build/warmup (future).",
          "optimize.artifact_cache": "Where compiled artifacts should be stored: shared (NFS), local (NVMe), or off."
        },
        "notes": "This is a request/intent API. The platform may ignore options it cannot satisfy for the chosen accelerator, architecture, or model type."
      },
      "tasks": [
        "[ ] Extend cozy build config parsing to capture optimization intents (manifest-only, no behavior change yet)",
        "[ ] Define canonical artifact cache keys (release_id + variant + model_id + backend + optimize flags)",
        "[ ] Decide compilation lifecycle: build-time vs warmup-time vs first-request",
        "[ ] Decide artifact storage location(s): shared cache root vs local cache root vs object storage",
        "[ ] Add best-effort telemetry fields to metrics.run for compile/export timings (no secrets/paths)",
        "[ ] Add docs + examples showing how to request torch.compile and alternative backends"
      ],
      "notes": "This is intentionally separated from basic 'install these backends' (issue #200). This issue is about the execution/compilation/optimization story and artifact caching.",
      "priority": "medium"
    }
  ]
}
